[
    {
        "pdf_file": "paper1.pdf",
        "text": "Automated Design of Deep Learning Methods for\nBiomedical Image Segmentation\nFabian Isensee1,2\u2020, Paul F. Jaeger1\u2020, Simon A. A. Kohl3\u2021, Jens Petersen1,4, and Klaus H.\nMaier-Hein1,5*\n1Division of Medical Image Computing, German Cancer Research Center, Heidelberg\n2Faculty of Biosciences, University of Heidelberg, Heidelberg, Germany\n3DeepMind, London, United Kingdom\n4Faculty of Physics & Astronomy, University of Heidelberg, Heidelberg, Germany\n5Pattern Analysis and Learning Group, Heidelberg University Hospital, Department of Radiation\nOncology, Heidelberg, Germany\n*k.maier-hein@dkfz.de\nAbstract\nBiomedical imaging is a driver of scienti\ufb01c discovery and core component of\nmedical care, currently stimulated by the \ufb01eld of deep learning. While semantic\nsegmentation algorithms enable 3D image analysis and quanti\ufb01cation in many\napplications, the design of respective specialised solutions is non-trivial and highly\ndependent on dataset properties and hardware conditions. We propose nnU-Net,\na deep learning framework that condenses the current domain knowledge and\nautonomously takes the key decisions required to transfer a basic architecture to dif-\nferent datasets and segmentation tasks. Without manual tuning, nnU-Net surpasses\nmost specialised deep learning pipelines in 19 public international competitions and\nsets a new state of the art in the majority of the 49 tasks. The results demonstrate\na vast hidden potential in the systematic adaptation of deep learning methods to\ndifferent datasets. We make nnU-Net publicly available as an open-source tool\nthat can effectively be used out-of-the-box, rendering state of the art segmentation\naccessible to non-experts and catalyzing scienti\ufb01c progress as a framework for\nautomated method design.\n1\nIntroduction\nSemantic segmentation transforms raw biomedical image data into meaningful, spatially structured\ninformation and thus plays an essential role for scienti\ufb01c discovery in the \ufb01eld [9, 14]. At the same\ntime, semantic segmentation is an essential ingredient to numerous clinical applications [1, 27],\nincluding applications of arti\ufb01cial intelligence in diagnostic support systems [7, 3], therapy planning\n\u2020 Equal contribution. \u2021 Work started while doing a PhD at the German Cancer Research Center.\nPreprint. Under review.\narXiv:1904.08128v2  [cs.CV]  2 Apr 2020\nsupport [28], intra-operative assistance [14] or tumor growth monitoring [19]. The high interest in\nautomatic segmentation methods manifests in a thriving research landscape, accounting for 70% of\ninternational image analysis competitions in the biomedical sector [23].\nDespite the recent success of deep learning-based segmentation methods, their applicability\nto speci\ufb01c image analysis problems of end-users is often limited. The task-speci\ufb01c design and\ncon\ufb01guration of a method requires high levels of expertise and experience, with small errors leading\nto strong performance drops [22]. Especially in 3D biomedical imaging, where dataset properties like\nimaging modality, image size, (anisotropic) voxel spacing or class ratio vary drastically, the pipeline\ndesign can be cumbersome, because experience on what constitutes a successful con\ufb01guration\nmay not translate to the dataset at hand. The numerous expert decisions involved in designing\nand training a neural network range from the exact network architecture to the training schedule\nand methods for data augmentation or post-processing. Each sub-component is controlled by\nessential hyperparameters like learning rate, batch size, or class sampling [22]. An additional layer\nof complexity on the overall setup is posed by the hardware available for training and inference\n[21]. Algorithmic optimization of the codependent design choices in this high dimensional space\nof hyperparameters is technically demanding and ampli\ufb01es both the number of required training\ncases as well as compute resources by orders of magnitude [8]. As a consequence, the end-user is\ncommonly left with an iterative trial and error process during method design that is mostly driven\nby their individual experience, only scarcely documented and hard to replicate, inevitably evoking\nsuboptimal segmentation pipelines and methodological \ufb01ndings that do not generalize to other\ndatasets [22, 2].\nTo further complicate things, there is an unmanageable number of research papers that pro-\npose architecture variations and extensions for performance improvement. This bulk of studies is\nincomprehensible to the non-expert and dif\ufb01cult to evaluate even for experts [22]. Approximately\n12000 studies cite the 2015 U-Net architecture on biomedical image segmentation [31], many of\nwhich propose extensions and advances. We put forward the hypothesis that a basic U-Net is still\nhard to beat if the corresponding pipeline is designed adequately.\nTo this end, we propose nnU-Net (\u201cno new net\u201d), which makes successful 3D biomedical\nimage segmentation accessible for biomedical research applications. nnU-Net automatically adapts\nto arbitrary datasets and enables out-of-the-box segmentation on account of two key contributions:\n1. We formulate the pipeline optimization problem in terms of a data \ufb01ngerprint (representing\nthe key properties of a dataset) and a pipeline \ufb01ngerprint (representing the key design choices\nof a segmentation algorithm).\n2. We make their relation explicit by condensing domain knowledge into a set of heuristic\nrules that robustly generate a high quality pipeline \ufb01ngerprint from a corresponding data\n\ufb01ngerprint while considering associated hardware constraints.\nIn contrast to algorithmic approaches for method con\ufb01guration that are formulated as a task-speci\ufb01c\noptimization problem, nnU-Net readily executes systematic rules to generate deep learning methods\nfor previously unseen datasets without need for further optimization.\nIn the following, we demonstrate the superiority of this concept by presenting a new state\nof the art in numerous international challenges through application of our algorithm without manual\n2\nintervention.\nThe strong results underline the signi\ufb01cance of nnU-Net for users who require\nalgorithms for semantic segmentation on their custom datasets: as an open source tool, nnU-Net can\nsimply be downloaded and trained out-of-the box to generate state of the art segmentations without\nrequiring manual adaptation or expert knowledge. We further demonstrate shortcomings in the\ndesign process of current biomedical segmentation methods. Speci\ufb01cally, we take an in-depth look at\nthe 2019 Kidney and Kidney Tumor Segmentation (KiTS) semantic image segmentation challenge\nand demonstrate how important task-speci\ufb01c design and con\ufb01guration of a method are in comparison\nto choosing one of the many architectural extensions and advances previously proposed on top of the\nU-Net. By automating this design and con\ufb01guration process, nnU-Net fosters the ambition and the\nability of researchers to validate novel ideas on larger numbers of datasets, while at the same time\nserving as an ideal reference method when demonstrating methodological improvements.\n2\nResults\nnnU-Net is a deep learning framework that enables 3D semantic segmentation in many biomedical\nimaging applications, without requiring the design of respective specialised solutions. Exemplary\nsegmentation results generated by nnU-Net for a variety of datasets are shown in Figure 1.\nnnU-Net automatically adapts to any new dataset\nFigure 2a shows the current practice of adapt-\ning segmentation pipelines to a new dataset. This process is expert-driven and involves manual\ntrial-and-error experiments that are typically speci\ufb01c to the task at hand [22]. As shown in Figure 2b,\nnnU-Net addresses the adaptation process systematically. Therefore, we de\ufb01ne a dataset \ufb01ngerprint\nas a standardized dataset representation comprising key properties such as image sizes, voxel spacing\ninformation or class ratios, and a pipeline \ufb01ngerprint as the entirety of choices being made during\nmethod design. nnU-Net is designed to generate a successful pipeline \ufb01ngerprint for a given dataset\n\ufb01ngerprint. In nnU-Net, the pipeline \ufb01ngerprint is divided into three groups: blueprint, inferred and\nempirical parameters. The blueprint parameters represent fundamental design choices (such as using\na plain U-Net-like architecture template) as well as hyperparameters for which a robust default value\ncan simply be picked (for example loss function, training schedule and data augmentation). The\ninferred parameters encode the necessary adaptations to a new dataset and include modi\ufb01cations\nto the exact network topology, patch size, batch size and image preprocessing. The link between a\ndata \ufb01ngerprint and the inferred parameters is established via execution of a set of heuristic rules,\nwithout the need for expensive re-optimization when applied to unseen datasets. Note that many of\nthese design choices are co-dependent: The target image spacing, for instance, affects image size,\nwhich in return determines the size of patches the model should see during training, which affects\nthe network topology and has to be counterbalanced by the size of training mini-batches in order to\nnot exceed GPU memory limitations. nnU-Net strips the user of the burden to manually account for\nthese co-dependencies. The empirical parameters are autonomously identi\ufb01ed via cross-validation\non the training cases. Per default, nnU-Net generates three different U-Net con\ufb01gurations: a 2D\nU-Net, a 3D U-Net that operates at full image resolution and a 3D U-Net cascade where the \ufb01rst\nU-Net operates on downsampled images and the second is trained to re\ufb01ne the segmentation maps\ncreated by the former at full resolution. After cross-validation nnU-Net empirically chooses the best\nperforming con\ufb01guration or ensemble. Finally, nnU-Net empirically opts for \u201cnon-largest component\nsuppression\u201d as a postprocessing step if performance gains are measured. The output of nnU-Net\u2019s\nautomated adaptation and training process are fully trained U-Net models that can be deployed to\nmake predictions on unseen images. We provide an in-depth description of the methodology behind\nnnU-Net in the online methods. The overarching design principles, i.e. our best-practice recommen-\n3\nFigure 1: nnU-Net handles a broad variety of datasets and target image properties. All examples\noriginate from the test sets of different international segmentation challenges that nnU-Net was applied\non. Target structures for each dataset are shown in 2D projected onto the raw data (left) and in 3D\ntogether with a volume rendering of the raw data (right). All visualizations are created with the\nMITK Workbench [29]. a: heart (green), aorta (red), trachea (blue) and esophagus (yellow) in CT\nimages (D18). b: synaptic clefts (green) in electron microscopy scans (D19). c: liver (yellow),\nspleen (orange), left/right kidney (blue/green) in T1 in-phase MRI (D16). d: thirteen abdominal\norgans in CT images (D11). e: liver (yellow) and liver tumors (green) in CT images (D14). f: right\nventricle (yellow), left ventricular cavity (blue) myocardium of left ventricle (green) in cine MRI\n(D13). g: prostate (yellow) in T2 MRI (D12). h: lung nodules (yellow) in CT images (D6). i: kidneys\n(yellow) and kidney tumors (green) in CT images (D17). j: edema (yellow), enhancing tumor (purple),\nnecrosis (green) in MRI (T1, T1 with contrast agent, T2, FLAIR) (D1). k: left ventricle (yellow) in\nMRI (D2). l: hepatic vessels (yellow) and liver tumors (green) in CT (D8). See Figure 5 for dataset\nreferences.\n4\nFigure 2: Manual and proposed automated con\ufb01guration of deep learning methods. a) Current\npractice of con\ufb01guring a deep learning method for biomedical segmentation: An iterative trial and\nerror process of manually choosing a set of hyperparameters and architecture con\ufb01gurations, training\nthe model, and monitoring performance of the model on a validation set. b) Proposed automated\ncon\ufb01guration by nnU-Net: Dataset properties are summarized in a \u201cdataset \ufb01ngerprint\u201d. A set\nof heuristic rules operates on this \ufb01ngerprint to infer the data-dependent hyperparameters of the\npipeline. These are completed by blueprint parameters, the data-independent design choices to form\n\u201cpipeline \ufb01ngerprints\u201d. Three architectures are trained based on these pipeline \ufb01ngerprints in a 5-fold\ncross-validation. Finally, nnU-Net automatically selects the optimal ensemble of these architectures\nand performs postprocessing if required.\ndations for method adaptation to new datasets, are summarized in Supplementary Information B.\nAll segmentation pipelines generated by nnU-Net in the context of this manuscript are provided in\nSupplementary Information F.\nnnU-Net handles a wide variety of target structures and image properties\nWe demonstrate the\nvalue of nnU-Net as an out-of-the-box segmentation tool by applying it to 10 international biomedical\nimage segmentation challenges comprising 19 different datasets and 49 segmentation tasks across a\nvariety of organs, organ substructures, tumors, lesions and cellular structures in magnetic resonance\nimaging (MRI), computed tomography scans (CT) as well as electron microscopy (EM) images.\nChallenges are international competitions that can be seen as the equivalent to clinical trials for\nalgorithm benchmarking. Typically, they are hosted by individual researchers, institutes, or societies,\naiming to assess the performance of multiple algorithms in a standardized environment [23]. In\nall segmentation tasks, nnU-Net was trained from scratch using only the provided challenge data.\nWhile the methodology behind nnU-Net was developed on the 10 training sets provided by the\nMedical Segmentation Decathlon [32], the remaining datasets and tasks were used for independent\ntesting, i.e. nnU-Net was simply applied without further optimization. Qualitatively, we observe that\n5\nnnU-Net is able to handle a large disparity in dataset properties and diversity in target structures,\ni.e. generated pipeline con\ufb01gurations are in line with what human experts consider a reasonable or\nsensible setting (see Supplementary Information C.1and C.2). Examples for segmentation results\ngenerated by nnU-Net are presented in Figure 1.\nnnU-Net outperforms specialized pipelines in a range of diverse tasks\nMost international chal-\nlenges use the Soerensen-Dice coef\ufb01cient as a measure of overlap to quantify segmentation quality\n[13, 4, 25, 3]. Here, perfect agreement results in a Dice coef\ufb01cient of 1, whereas no agreement\nresults in a score of 0. Other metrics used by some of the challenges include the Normalized Surface\nDice (higher is better) [7] and the Hausdorff Distance (lower is better), both quantifying the distance\nbetween the borders of two segmentations. Figure 3 provides an overview of the quantitative results\nachieved by nnU-Net and the competing challenge teams across all 49 segmentation tasks. Despite\nits generic nature, nnU-Net outperforms most existing semantic segmentation solutions, even though\nthe latter were speci\ufb01cally optimized towards the respective task. Overall, nnU-Net sets a new state\nof the art in 29 out of 49 target structures and otherwise shows performances on par with or close to\nthe top leaderboard entries.\nDetails in pipeline con\ufb01guration have more impact on performance than architectural varia-\ntions\nTo highlight how important the task-speci\ufb01c design and con\ufb01guration of a method are in\ncomparison to choosing one of the many architectural extensions and advances previously proposed\non top of the U-Net, we put our results into context of current research by analyzing the participating\nalgorithms in the recent Kidney and Kidney Tumor Segmentation (KiTS) 2019 challenge hosted by\nthe Medical Image Computing and Computer Assisted Intervention (MICCAI) society [13]. The\nMICCAI society has consistently been hosting at least 50% of all annual biomedical image analysis\nchallenges [23]. With more than 100 competitors, the KiTS challenge was the largest competition at\nMICCAI 2019. Our analysis of the KiTS leaderboard1 (see Figure 4a) reveals several insights on the\ncurrent landscape of deep learning based segmentation method design: First, the top-15 methods were\noffspring of the (3D) U-Net architecture from 2016, con\ufb01rming its impact on the \ufb01eld of biomedical\nimage segmentation. Second, the \ufb01gure demonstrates that contributions using the same type of\nnetwork result in performances spread across the entire leaderboard. Third, when looking closer into\nthe top-15, none of the commonly used architectural modi\ufb01cations (e.g. residual connections [26, 10],\ndense connections [18, 15], attention mechanisms [30] or dilated convolutions [5, 24]) represent a\nnecessary condition for good performance on the KiTS task. By example this shows that many of\nthe previously introduced algorithm modi\ufb01cations may not generally be superior to a properly tuned\nbaseline method.\nFigure 4b underlines the importance of hyperparameter tuning by analyzing algorithms using the same\narchitecture variant as the challenge-winning contribution, a 3D U-Net with residual connections.\nWhile one of these methods won the challenge, other contributions based on the same principle cover\nthe entire range of evaluation scores and rankings. Key con\ufb01guration parameters were selected from\nrespective pipeline \ufb01ngerprints and are shown for all non-cascaded residual U-Nets, illustrating the\nco-dependent design choices that each team made during pipeline design. The drastically varying\ncon\ufb01gurations submitted by contestants indicate the underlying complexity of the high-dimensional\noptimization problem that is implicitly posed by designing a deep learning method for biomedical 3D\nimage segmentation.\n1http://results.kits-challenge.org/miccai2019/\n6\nFigure 3: nnU-Net outperforms most specialized deep learning pipelines. Quantitative results\nfrom all international challenges that nnU-Net competed in. For each segmentation task, results\nachieved by nnU-Net are highlighted in red, competing teams are shown in blue. For each segmenta-\ntion task the respective rank is displayed in the bottom right corner as nnU-Net\u2019s rank / total number\nof submissions. Axis scales: [DC] Dice coef\ufb01cient, [OH] other score (higher is better), [OL] other\nscore (lower is better). All leaderboards were accessed on December 12th 2019.\n7\nFigure 4: Pipeline \ufb01ngerprints from KITS 2019 [13] leaderboard entries. a) Coarse categoriza-\ntion of leaderboard entries by architecture variation. All top 15 contributions are encoder-decoder\narchitectures with skip-connections, 3D convolutions and output stride 1 (\u201c3D U-Net-like\u201d, purple).\nNo clear pattern arises from further sub-groupings into different architectural variations. Also,\nnone of the analyzed architectures guarantees good performance, indicating a large dependency of\nperformance beyond architecture type. b) Finer-grained key parameters selected from the pipeline\n\ufb01ngerprints of all non-cascade 3D-U-Net-like architectures with residual connections (displayed on\nz-score normalized scale). The contributions vary drastically in their rankings as well as their \ufb01nger-\nprints. Still, there is no evident relation between single parameters and performance. Abbreviations:\nCE = Cross entropy loss function, Dice = Soft Dice loss function, WBCE = Weighted binary cross\nentropy loss function.\nnnU-Net experimentally con\ufb01rms the importance of good hyperparameters over architectural vari-\nations on the KiTS dataset by setting a new state of the art on the open leaderboard (which also\nincludes the original challenge submissions analysed here) with a plain 3D U-Net architecture (see\nFigure 3). Our results from further international challenge participations con\ufb01rm this observation\nacross a variety of datasets.\nDifferent datasets require different pipeline con\ufb01gurations\nWe extract the data \ufb01ngerprints of\n19 biomedical segmentation datasets. As displayed in Figure 5, this documents an exceptional dataset\ndiversity in biomedical imaging, and reveals the fundamental reason behind the lack of out-of-the-box\n8\nFigure 5: Data \ufb01ngerprints across different challenge datasets. The data \ufb01ngerprints show the\nkey properties (displayed on z-score normalized scale) for the 19 datasets used in the nnU-Net experi-\nments (see Supplementary Material A for detailed dataset descriptions). Datasets vary tremendously\nin their properties, requiring intense method adaptation to the individual dataset and underlining\nthe need for evaluation on larger numbers of datasets when drawing general methodological conclu-\nsions. Abbreviations: EM = Electron Microscopy, CT = Computed Tomography, MRI = Magnetic\nResonance Imaging.\nsegmentation algorithms: The complexity of method design is ampli\ufb01ed by the fact that suitable\npipeline settings either directly or indirectly depend on the data \ufb01ngerprint under potentially complex\nrelations. As a consequence, pipeline settings that are identi\ufb01ed as optimal for one dataset (such\nas KiTS, see above) may not generalize to others, resulting in a need for (currently manual) re-\noptimization on each individual dataset. An example for con\ufb01guration parameters depending on\ndataset properties is the image size which affects the size of patches that the model sees during training,\nwhich in turn affects the required network topology (i.e. number of downsampling steps, size of\nconvolution \ufb01lters, etc.). The network topology itself again in\ufb02uences several other hyperparameters\nin the pipeline.\nMultiple tasks enable robust design decisions\nnnU-Net is a framework that enables benchmark-\ning of new modi\ufb01cations or extensions of methods across multiple datasets without having to manually\nrecon\ufb01gure the entire pipeline for each dataset. To demonstrate this, and also to support some of the\ncore design choices made in nnU-Net, we systematically tested the performance of common pipeline\nvariations in the nnU-Net blueprint parameters on 10 different datasets (Figure 6): the application\nof two alternative loss functions (Cross-entropy and TopK10 [35]), the introduction of residual\nconnections in the encoder [11], using three convolutions per resolution instead of two (resulting\nin a deeper network architecture), two modi\ufb01cations of the optimizer (a reduced momentum term\nand an alternative optimizer (Adam [20])), batch norm [17] instead of instance norm [33] and the\nomission of data augmentation. Ranking stability was estimated by bootstrapping as suggested by the\nchallengeR tool [34].\nThe volatility of the ranking between datasets demonstrates how single hyperparameter choices can\naffect segmentation performance depending on the dataset. The results clearly show that caution is\nrequired when drawing methodological conclusions from evaluations that are based on an insuf\ufb01cient\nnumber of datasets. While \ufb01ve out of the nine variants achieved rank 1 in at least one of the\n9\nFigure 6: Evaluation of design decisions across multiple tasks. (a-j) Evaluation of exemplary\nmodel variations on ten datasets of the medical segmentation decathlon (D1-D10, see Figure 5 for\ndataset references). The analysis is done for every dataset by aggregating validation splits of the\n\ufb01ve-fold cross-validation into one large validation set. 1000 virtual validation sets are generated\nvia bootstrapping (drawn with replacement). Algorithms are ranked on each virtual validation set,\nresulting in a distribution over rankings. The results indicate that evaluation of methodological\nvariations on too few datasets is prone to result in a misleading level of generality, since most\nperformance changes are not consistent over datasets. (k) The aggregation of rankings across datasets\nyields insights into what design decisions robustly generalize.\ndatasets, neither of them exhibits consistent improvements across the ten tasks. The original nnU-Net\ncon\ufb01guration shows the best generalization and ranks \ufb01rst when aggregating results of all datasets.\nIn current research practice, evaluation is rarely performed on more than two datasets and even then\nthe datasets come with largely overlapping properties (such as both being abdominal CT scans). As\nwe showed here, such evaluation is unsuitable for drawing general methodological conclusions. We\nrelate the lack of suf\ufb01ciently broad evaluations to the manual tuning effort required when adapting\nexisting pipelines to individual datasets. nnU-Net alleviates this shortcoming in two ways: As a\nframework that can be extended to enable effective evaluation of new concepts across multiple tasks,\nand as a plug-and-play, standardized and state-of-the-art baseline to compare against.\n10\nnnU-Net is freely available and can be used out-of-the-box\nnnU-Net is freely available as an\nopen-source tool. It can be installed via Python Package Index (PyPI). The source code is publicly\navailable on Github (https://github.com/MIC-DKFZ/nnUNet). A comprehensive documentation\nis available together with the source code. Pretrained models for all presented datasets are available\nfor download at https://zenodo.org/record/3734294.\n3\nDiscussion\nWe presented nnU-Net, a deep learning framework for biomedical image analysis that automates\nmodel design for 3D semantic segmentation tasks. The method sets a new state of the art in the\nmajority of tasks it was evaluated on, outperforming all respective specialized processing pipelines.\nThe strong performance of nnU-Net is not achieved by a new network architecture, loss function or\ntraining scheme (hence the name nnU-Net - \u201cno new net\u201d), but by replacing the complex process of\nmanual pipeline optimization with a systematic approach based on explicit and interpretable heuristic\nrules. Requiring zero user-intervention, nnU-Net is the \ufb01rst segmentation tool that can be applied\nout-of-the-box to a very large range of biomedical imaging datasets and is thus the ideal tool for\nusers who require access to semantic segmentation methods and do not have the expertise, time, or\ncompute resources required to manually adapt existing solutions to their problem.\nOur analysis on the KITS leaderboard as well as nnU-Net\u2019s performance across 19 datasets\ncon\ufb01rms our initial hypothesis that common architectural modi\ufb01cations proposed by the \ufb01eld during\nthe last 5 years may not necessarily be required to achieve state-of-the-art segmentation performance.\nInstead, we observed that contributions using the same type of network result in performances\nspread across the entire leaderboard. This observation is in line with Litjens et al., who, in their\nreview from 2017, found that \"many researchers use the exact same architectures [...] but have\nwidely varying results\" [22]. There are several possible reasons for why performance improvements\nbased on architectural extensions proposed by the literature may not hold beyond the dataset they\nwere proposed on: many of them are evaluated on a limited amount of datasets, often as low as a\nsingle one. In practice this largely limits their success on unseen datasets with varying properties,\nbecause the quality of the hyperparameter con\ufb01guration often overshadows the effect of the evaluated\narchitectural modi\ufb01cation. This \ufb01nding is in line with an observation by Litjens et al., who concluded\nthat \"the exact architecture is not the most important determinant in getting a good solution\" [22].\nMoreover, as shown above, it can be dif\ufb01cult to tune existing baselines to a given dataset. This\nobstacle can unknowingly, but nonetheless unduly, make a new approach look better than the baseline,\nresulting in biased literature.\nIn this work, we demonstrated that nnU-Net is able to alleviate this bottleneck of current\nresearch in biomedical image segmentation in two ways: On the one hand, nnU-Net serves as a\nframework for methodological modi\ufb01cations enabling simple evaluation on an arbitrary number of\ndatasets. On the other hand, nnU-Net represents the \ufb01rst standardized method that does not require\nmanual task-speci\ufb01c adaptation and as such can readily serve as a strong baseline on any new 3D\nsegmentation task.\nThe research performed in \u201cAutoML\u201d [16, 6] or \u201cNeural architecture search\u201d [8] has simi-\nlarities to our approach in that this line of research seeks to strip the ML user or researcher of the\nburden to manually \ufb01nd good hyperparameters. In contrast to nnU-Net however, AutoML aims to\n11\nlearn hyperparameters directly from the data. This comes with practical dif\ufb01culties such as enormous\nrequirements with respect to compute and data resources. Additionally, AutoML methods need to\noptimize the hyperparameters for each new task. The same disadvantages apply to \u201cGrid Search\u201d [2],\nwhere extensive trial and error sweeps in the hyperparameter landscape are performed to empirically\n\ufb01nd good con\ufb01gurations for a speci\ufb01c task. In contrast, nnU-Net transforms domain knowledge into\ninductive biases, thus shortcuts the high dimensional optimization of hyperparameters and minimizes\nrequired computational and data resources. As elaborated above, these heuristics are developed\non the basis of 10 different datasets of the Medical Segmentation Decathlon. The diversity within\nthese 10 datasets has proven suf\ufb01cient to achieve robustness to the variability encountered in all the\nremaining challenge participations. This is quite remarkable given the underlying complexity of\nmethod design and strongly con\ufb01rms the suitability of condensing the process in a few generally\napplicable rules that are simply executed when given a new dataset \ufb01ngerprint and do not require any\nfurther task-speci\ufb01c actions. The formal de\ufb01nition and also publishing of these explicit rules is a\nstep towards systematicity and interpretability in the task of hyperparameter selection, which has\npreviously been considered a \u201chighly empirical exercise\u201d, for which \u201cno clear recipe can be given.\u201d\n[22].\nDespite its strong performance across 49 diverse tasks, there might be segmentation tasks\nfor which nnU-Net\u2019s automatic adaptation is suboptimal. For example, nnU-Net was developed\nwith a focus on the Dice coef\ufb01cient as performance metric. Some tasks, however, might require\nhighly domain speci\ufb01c target metrics for performance evaluation, which could in\ufb02uence method\ndesign.\nAlso, yet unconsidered dataset properties could exist which may cause suboptimal\nsegmentation performance. One example is the synaptic cleft segmentation task of the CREMI\nchallenge (https://cremi.org). While nnU-Net\u2019s performance is highly competitive (rank 6/39),\nmanual adaptation of the loss function as well as electron microscopy-speci\ufb01c preprocessing may be\nnecessary to surpass state-of-the-art performance [12]. In principle, there are two ways of handling\ncases that are not yet optimally covered by nnU-Net: For potentially re-occurring cases, nnU-Net\u2019s\nheuristics could be extended accordingly; for highly domain speci\ufb01c cases, nnU-Net should be seen\nas a good starting point for necessary modi\ufb01cations.\nIn summary, nnU-Net sets a new state of the art in various semantic segmentation chal-\nlenges and displays strong generalization characteristics without need for any manual intervention,\nsuch as the tuning of hyper-parameters. As pointed out by Litjens et al. and quantitatively con\ufb01rmed\nhere, hyper-parameter optimization constitutes a major dif\ufb01culty for past and current research\nin biomedical image segmentation. nnU-Net automates the otherwise often unsystematic and\ncumbersome procedure and may thus help alleviate this burden. We propose to leverage nnU-Net as\nan out-of-the box tool for state-of-the-art segmentation, a framework for large-scale evaluation of\nnovel ideas without manual effort, and as a standardized baseline method to compare ideas against\nwithout the need for task-speci\ufb01c optimization.\n4\nAcknowledgements\nThis work was co-funded by the National Center for Tumor Diseases (NCT) in Heidelberg and\nthe Helmholtz Imaging Platform (HIP) of the German Cancer Consortium (DKTK). We thank our\ncolleagues at DKFZ who were involved in the various challenge contributions, especially Andre\nKlein, David Zimmerer, Jakob Wasserthal, Gregor Koehler, Tobias Norajitra and Sebastian Wirkert\n12\nwho contributed to the Decathlon submission. We also thank the MITK team who supported us in\nproducing all medical dataset visualizations. We are also thankful to all the challenge organizers,\nwho provided an important basis for our work. We want to especially mention Nicholas Heller, who\nenabled the collection of all the details from the KiTS challenge through excellent challenge design,\nand Emre Kavur from the CHAOS team, who generated comprehensive leaderboard information for\nus. We thank Manuel Wiesenfarth for his helpful advice concerning the ranking of methods and the\nvisualization of rankings. Last but not least, we thank Olaf Ronneberger and Lena Maier-Hein for\ntheir important feedback on this manuscript.\nReferences\n[1] H. J. Aerts, E. R. Velazquez, R. T. Leijenaar, C. Parmar, P. Grossmann, S. Carvalho, J. Bussink,\nR. Monshouwer, B. Haibe-Kains, D. Rietveld, et al. Decoding tumour phenotype by noninvasive\nimaging using a quantitative radiomics approach. Nature communications, 5(1):1\u20139, 2014.\n[2] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. Journal of machine\nlearning research, 13(Feb):281\u2013305, 2012.\n[3] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A. Heng, I. Cetin, K. Lekadir,\nO. Camara, M. A. G. Ballester, et al. Deep learning techniques for automatic mri cardiac multi-\nstructures segmentation and diagnosis: Is the problem solved? IEEE TMI, 37(11):2514\u20132525,\n2018.\n[4] P. Bilic, P. F. Christ, E. Vorontsov, G. Chlebus, H. Chen, Q. Dou, C.-W. Fu, X. Han, P.-\nA. Heng, J. Hesser, et al. The liver tumor segmentation benchmark (lits). arXiv preprint\narXiv:1901.04056ada, 2019.\n[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image\nsegmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE\ntransactions on pattern analysis and machine intelligence, 40(4):834\u2013848, 2017.\n[6] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning augmenta-\ntion strategies from data. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 113\u2013123, 2019.\n[7] J. De Fauw, J. R. Ledsam, B. Romera-Paredes, S. Nikolov, N. Tomasev, S. Blackwell, H. Askham,\nX. Glorot, B. O\u2019Donoghue, D. Visentin, et al. Clinically applicable deep learning for diagnosis\nand referral in retinal disease. Nature medicine, 24(9):1342\u20131350, 2018.\n[8] T. Elsken, J. H. Metzen, and F. Hutter. Neural architecture search: A survey. Journal of Machine\nLearning Research, 20(55):1\u201321, 2019.\n[9] T. Falk, D. Mai, R. Bensch, \u00d6. \u00c7i\u00e7ek, A. Abdulkadir, Y. Marrakchi, A. B\u00f6hm, J. Deubner,\nZ. J\u00e4ckel, K. Seiwald, et al. U-net: deep learning for cell counting, detection, and morphometry.\nNature methods, 16(1):67\u201370, 2019.\n[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778,\n2016.\n[11] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European\nconference on computer vision, pages 630\u2013645. Springer, 2016.\n13\n[12] L. Heinrich, J. Funke, C. Pape, J. Nunez-Iglesias, and S. Saalfeld. Synaptic cleft segmentation\nin non-isotropic volume electron microscopy of the complete drosophila brain. In International\nConference on Medical Image Computing and Computer-Assisted Intervention, pages 317\u2013325.\nSpringer, 2018.\n[13] N. Heller, F. Isensee, K. H. Maier-Hein, X. Hou, C. Xie, F. Li, Y. Nan, G. Mu, Z. Lin, M. Han,\net al. The state of the art in kidney and kidney tumor segmentation in contrast-enhanced ct\nimaging: Results of the kits19 challenge. arXiv preprint arXiv:1912.01054, 2019.\n[14] T. C. Hollon, B. Pandian, A. R. Adapa, E. Urias, A. V. Save, S. S. S. Khalsa, D. G. Eichberg,\nR. S. D\u2019Amico, Z. U. Farooq, S. Lewis, et al. Near real-time intraoperative brain tumor diagnosis\nusing stimulated raman histology and deep neural networks. Nature Medicine, pages 1\u20137, 2020.\n[15] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 4700\u20134708, 2017.\n[16] F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general\nalgorithm con\ufb01guration. In International conference on learning and intelligent optimization,\npages 507\u2013523. Springer, 2011.\n[17] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n[18] S. J\u00e9gou, M. Drozdzal, D. Vazquez, A. Romero, and Y. Bengio. The one hundred layers tiramisu:\nFully convolutional densenets for semantic segmentation. In Proceedings of the IEEE conference\non computer vision and pattern recognition workshops, pages 11\u201319, 2017.\n[19] P. Kickingereder, F. Isensee, I. Tursunova, J. Petersen, U. Neuberger, D. Bonekamp, G. Brugnara,\nM. Schell, T. Kessler, M. Foltyn, et al. Automated quantitative tumour response assessment of\nmri in neuro-oncology with arti\ufb01cial neural networks: a multicentre, retrospective study. The\nLancet Oncology, 20(5):728\u2013740, 2019.\n[20] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Y. Bengio and\nY. LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San\nDiego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\n[21] Y. LeCun. 1.1 deep learning hardware: Past, present, and future. In 2019 IEEE International\nSolid-State Circuits Conference-(ISSCC), pages 12\u201319. IEEE, 2019.\n[22] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian, J. A. Van\nDer Laak, B. Van Ginneken, and C. I. S\u00e1nchez. A survey on deep learning in medical image\nanalysis. Medical image analysis, 42:60\u201388, 2017.\n[23] L. Maier-Hein, M. Eisenmann, A. Reinke, S. Onogur, M. Stankovic, P. Scholz, T. Arbel,\nH. Bogunovic, A. P. Bradley, A. Carass, et al. Why rankings of biomedical image analysis\ncompetitions should be interpreted with care. Nature communications, 9(1):5217, 2018.\n[24] R. McKinley, R. Meier, and R. Wiest. Ensembles of densely-connected cnns with label-\nuncertainty for brain tumor segmentation. In International MICCAI Brainlesion Workshop, pages\n456\u2013465. Springer, 2018.\n14\n[25] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, Y. Burren, N. Porz,\nJ. Slotboom, R. Wiest, et al. The multimodal brain tumor image segmentation benchmark (brats).\nIEEE transactions on medical imaging, 34(10):1993\u20132024, 2014.\n[26] F. Milletari, N. Navab, and S.-A. Ahmadi. V-net: Fully convolutional neural networks for\nvolumetric medical image segmentation. In International Conference on 3D Vision (3DV), pages\n565\u2013571. IEEE, 2016.\n[27] U. Nestle, S. Kremp, A. Schaefer-Schuler, C. Sebastian-Welsch, D. Hellwig, C. R\u00fcbe, and C.-M.\nKirsch. Comparison of different methods for delineation of 18f-fdg pet\u2013positive tissue for target\nvolume de\ufb01nition in radiotherapy of patients with non\u2013small cell lung cancer. Journal of Nuclear\nMedicine, 46(8):1342\u20131348, 2005.\n[28] S. Nikolov, S. Blackwell, R. Mendes, J. De Fauw, C. Meyer, C. Hughes, H. Askham, B. Romera-\nParedes, A. Karthikesalingam, C. Chu, et al. Deep learning to achieve clinically applicable\nsegmentation of head and neck anatomy for radiotherapy. arXiv preprint arXiv:1809.04430,\n2018.\n[29] M. Nolden, S. Zelzer, A. Seitel, D. Wald, M. M\u00fcller, A. M. Franz, D. Maleike, M. Fangerau,\nM. Baumhauer, L. Maier-Hein, et al. The medical imaging interaction toolkit: challenges and\nadvances. International journal of computer assisted radiology and surgery, 8(4):607\u2013620, 2013.\n[30] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori, S. McDonagh,\nN. Y. Hammerla, B. Kainz, et al. Attention u-net: learning where to look for the pancreas. arXiv\npreprint arXiv:1804.03999, 2018.\n[31] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image\nsegmentation. In MICCAI, pages 234\u2013241. Springer, 2015.\n[32] A. L. Simpson, M. Antonelli, S. Bakas, M. Bilello, K. Farahani, B. van Ginneken, A. Kopp-\nSchneider, B. A. Landman, G. Litjens, B. Menze, et al. A large annotated medical image dataset\nfor the development and evaluation of segmentation algorithmsdelldatagrowth. arXiv preprint\narXiv:1902.09063, 2019.\n[33] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normalization: The missing ingredient for\nfast stylization. arXiv preprint arXiv:1607.08022, 2016.\n[34] M. Wiesenfarth, A. Reinke, B. A. Landman, M. J. Cardoso, L. Maier-Hein, and A. Kopp-\nSchneider. Methods and open-source toolkit for analyzing and visualizing challenge results.\narXiv preprint arXiv:1910.05121, 2019.\n[35] Z. Wu, C. Shen, and A. v. d. Hengel. Bridging category-level and instance-level semantic image\nsegmentation. arXiv preprint arXiv:1605.06885, 2016.\n15\nMethods\nA quick overview of the nnU-Net design principles can be found in the Supplemental Material B.\nThis section provides detailed information on how these principles are implemented.\nDataset \ufb01ngerprints\nAs a \ufb01rst processing step, nnU-Net crops the provided training cases to their\nnonzero region. While this had no effect on most datasets in our experiments, it reduced the image\nsize of brain datasets such as D1 (Brain Tumor) and D15 (MSLes) substantially and thus improved\ncomputational ef\ufb01ciency. Based on the cropped training data, nnU-Net creates a dataset \ufb01ngerprint\nthat captures all relevant parameters and properties: image sizes (i.e. number of voxels per spatial\ndimension) before and after cropping, image spacings (i.e. the physical size of the voxels), modalities\n(read from metadata) and number of classes for all images as well as the total number of training\ncases. Furthermore, the \ufb01ngerprint includes the mean, standard deviation as well as the 0.5 and 99.5\npercentiles of the intensity values in the foreground regions, i.e. the voxels belonging to any of the\nclass labels, computed over all training cases.\nPipeline \ufb01ngerprints\nnnU-Net automizes the design of deep learning methods for biomedical im-\nage segmentation by generating a so-called pipeline \ufb01ngerprint that contains all relevant information.\nImportantly, nnU-Net reduces the design choices to the really essential ones and automatically infers\nthese choices using a set of heuristic rules. These rules condense the domain knowledge and operate\non the above-described data \ufb01ngerprint and the project-speci\ufb01c hardware constraints. These inferred\nparameters are complemented by blueprint parameters, which are data-independent, and empirical\nparameters, which are optimized during training.\nBlueprint parameters\nArchitecture template: All U-Net architectures con\ufb01gured by nnU-Net\noriginate from the same template. This template closely follows the original U-Net [16] and its\n3D counterpart [3]. According to our hypothesis that a well-con\ufb01gured plain U-Net is still hard to\nbeat, none of our U-Net con\ufb01gurations make use of recently proposed architectural variations such\nas residual connections [6, 7], dense connections [10, 12], attention mechanisms [14], squeeze and\nexcitation [9] or dilated convolutions [2]. Minor changes with respect to the original architecture were\nmade: To enable large patch sizes, the batch size of the networks in nnU-Net is small. In fact, most\n3D U-Net con\ufb01gurations were trained with a batch size of only 2 (see Supplementary Material Figure\nE.1a). Batch normalization [11], which is often used to speed up or stabilize the training, does not\nperform well with small batch sizes [20, 17]. We therefore use instance normalization [19] for all U-\nNet models. Furthermore, we replace ReLU with leaky ReLUs [13] (negative slope 0.01). Networks\nare trained with deep supervision: additional auxiliary losses are added in the decoder to all but the\ntwo lowest resolutions, allowing gradients to be injected deeper into the network and facilitating the\ntraining of all layers in the network. All U-Nets employ the very common con\ufb01guration of two blocks\nper resolution step in both encoder and decoder, with each block consisting of a convolution, followed\nby instance normalization and a leaky ReLU nonlinearity. Downsampling is implemented as strided\nconvolution (motivated by representational bottleneck, see [18]) and upsampling as convolution\ntransposed. As a tradeoff between performance and memory consumption, the initial number of\nfeature maps is set to 32 and doubled (halved) with each downsampling (upsampling) operation. To\nlimit the \ufb01nal model size, the number of feature maps is additionally capped at 320 and 512 for 3D\nand 2D U-Nets, respectively.\nTraining schedule: Based on experience and as a trade-off between runtime and reward, all networks\nare trained for 1000 epochs with one epoch being de\ufb01ned as iteration over 250 minibatches. Stochastic\n16\ngradient descent with nesterov momentum (\u00b5 = 0.99) and an initial learning rate of 0.01 is used for\nlearning network weights. The learning rate is decayed throughout the training following the \u2018poly\u2019\nlearning rate policy [2]: (1 \u2212epoch/epochmax)0.9. The loss function is the sum of cross-entropy\nand Dice loss [4]. For each deep supervision output, a corresponding downsampled ground truth\nsegmentation mask is used for loss computation. The training objective is the sum of the losses at\nall resolutions: L = w1 \u00b7 L1 + w2 \u00b7 L2 + ... . Hereby, the weights halve with each decrease in\nresolution, resulting in w2 = 1/2 \u00b7 w1; w3 = 1/4 \u00b7 w1, etc. and are normalized to sum to 1. Samples\nfor the mini batches are chosen from random training cases. Oversampling is implemented to ensure\nrobust handling of class imbalances: 66.7% of samples are from random locations within the selected\ntraining case while 33.3% of patches are guaranteed to contain one of the foreground classes that\nare present in the selected training sample (randomly selected). The number of foreground patches\nis rounded with a forced minimum of 1 (resulting in 1 random and 1 foreground patch with batch\nsize 2). A variety of data augmentation techniques are applied on the \ufb02y during training: rotations,\nscaling, Gaussian noise, Gaussian blur, brightness, contrast, simulation of low resolution, gamma and\nmirroring. Details are provided in Supplementary Information D.\nInference: Images are predicted with a sliding window approach, where the window size equals the\npatch size used during training. Adjacent predictions overlap by half the size of a patch. The accuracy\nof segmentation decreases towards the borders of the window. To suppress stitching artifacts and\nreduce the in\ufb02uence of positions close to the borders, a Gaussian importance weighting is applied,\nincreasing the weight of the center voxels in the softmax aggregation. Test time augmentation by\nmirroring along all axes is applied.\nInferred Parameters\nIntensity normalization: There are two different image intensity normal-\nization schemes supported by nnU-Net. The default setting for all modalities except CT images is\nz-scoring. For this option, during training and inference, each image is normalized independently\nby subtracting its mean, followed by division with its standard deviation. If cropping resulted in\nan average size decrease of 25% or more, a mask for central non-zero voxels is created and the\nnormalization is applied within that mask only, ignoring the surrounding zero voxels. For computed\ntomography (CT) images, nnU-Net employs a different scheme, as intensity values are quantitative\nand re\ufb02ect physical properties of the tissue. It can therefore be bene\ufb01cial to retain this information by\nusing a global normalization scheme that is applied to all images. To this end, nnU-Net uses the 0.5\nand 99.5 percentiles of the foreground voxels for clipping as well as the global foreground mean a\nstandard deviation for normalization on all images.\nResampling: In some datasets, particularly in the medical domain, the voxel spacing (the physical\nspace the voxels represent) is heterogeneous. Convolutional neural networks operate on voxel grids\nand ignore this information. To cope with this heterogeneity, nnU-Net resamples all images to the\nsame target spacing (see paragraph below) using either third order spline, linear or nearest neighbor\ninterpolation. The default setting for image data is third order spline interpolation. For anisotropic\nimages (maximum axis spacing / minimum axis spacing > 3), in-plane resampling is done with third\norder spline whereas out of plane interpolation is done with nearest neighbor. Treating the out of\nplane axis differently in anisotropic cases suppresses resampling artifacts, as large contour changes\nbetween slices are much more common. Segmentation maps are resampled by converting them to\none hot encodings. Each channel is then interpolated with linear interpolation and the segmentation\nmask is retrieved by an argmax operation. Again, anisotropic cases are interpolated using \u201cnearest\nneighbor\u201d on the low resolution axis.\n17\nTarget spacing: The selected target spacing is a crucial parameter. Larger spacings result in smaller\nimages and thus a loss of details whereas smaller spacings result in larger images preventing the\nnetwork from accumulating suf\ufb01cient contextual information since the patch size is limited by the\ngiven GPU memory budget. Although this tradeoff is in part addressed by the 3D U-Net cascade\n(see below), a sensible target spacing for low and full resolution is still required. For the 3D full\nresolution U-Net, nnU-Net uses the median value of the spacings found in the training cases computed\nindependently for each axis as default target spacing. For anisotropic datasets, this default can result in\nsevere interpolation artifacts or in a substantial loss of information due to large variances in resolution\nacross the training data. Therefore, the target spacing of the lowest resolution axis is selected to be\nthe 10th percentile of the spacings found in the training cases if both voxel and spacing anisotropy\n(i.e. the ratio of lowest spacing axis to highest spacing axis) are larger than 3. For the 2D U-Net,\nnnU-Net generally operates on the two axes with the highest resolution. If all three axes are isotropic,\nthe two trailing axes are utilized for slice extraction. The target spacing is the median spacing of the\ntraining cases (computed independently for each axis). For slice-based processing, no resampling\nalong the out-of-plane axis is required.\nAdaptation of network topology, patch size and batch size: Finding an appropriate U-Net architecture\ncon\ufb01guration is crucial for good segmentation performance. nnU-Net prioritizes large patch sizes\nwhile remaining within a prede\ufb01ned GPU memory budget. Larger patch sizes allow for more\ncontextual information to be aggregated and thus typically increase segmentation performance.\nThey come, however, at the cost of a decreased batch size which results in noisier gradients during\nbackpropagation. To improve the stability of the training, we require a minimum batch size of 2 and\nchoose a large momentum term for network training (see blueprint parameters). Image spacing is also\nconsidered in the adaptation process: Downsampling operations may operate only on speci\ufb01c axes\nand convolutional kernels in the 3D U-Nets can operate on certain image planes only (pseudo-2D).\nThe network topology for all U-Net con\ufb01gurations is chosen on basis of the median image size after\nresampling as well as the target spacing the images were resampled to. A \ufb02ow chart for the adaptation\nprocess is presented in the Supplements in Figure E.1. The adaptation of the architecture template,\nwhich is described in more detail in the following, is computationally inexpensive. Due to the GPU\nmemory consumption estimate being based on feature map sizes, no GPU is required to run the\nadaptation process.\nInitialization: The patch size is initialized as the median image shape after resampling. If the patch\nsize is not divisible by 2nd for each axis, where nd is the number of downsampling operations, it is\npadded accordingly.\nArchitecture topology: The architecture is con\ufb01gured by determining the number of downsampling\noperations along each axis depending on the patch size and voxel spacing. Downsampling is\nperformed until further downsampling would reduce the feature map size to smaller than 4 voxels or\nthe feature map spacings become anisotropic. The downsampling strategy is determined by the voxel\nspacing: high resolution axes are downsampled separately until their resolution is within factor 2 of\nthe lower resolution axis. Subsequently, all axes are downsampled simultaneously. Downsampling is\nterminated for each axis individually, once the respective feature map constraint is triggered. The\ndefault kernel size for convolutions is 3 \u00d7 3 \u00d7 3 and 3 \u00d7 3 for 3D U-Net and 2D U-Net, respectively.\nIf there is an initial resolution discrepancy between axes (de\ufb01ned as a spacing ratio larger than 2), the\nkernel size for the out-of-plane axis is set to 1 until the resolutions are within a factor of 2. Note that\nthe convolutional kernel size then remains at 3 for all axes.\n18\nAdaptation to GPU memory budget: The largest possible patch size during con\ufb01guration is limited\nby the amount of GPU memory. Since the patch size is initialized to the median image shape after\nresampling, it is initially too large to \ufb01t into the GPU for most datasets. nnU-Net estimates the\nmemory consumption of a given architecture based on the size of the feature maps in the network,\ncomparing it to reference values of known memory consumption. The patch size is then reduced in\nan iterative process while updating the architecture con\ufb01guration accordingly in each step until the\nrequired budget is reached (see Figure E.1 in the Supplements). The reduction of the patch size is\nalways applied to the largest axis relative to the median image shape of the data. The reduction in\none step amounts to 2nd voxels of that axis, where nd is the number of downsampling operations.\nBatch size: As a \ufb01nal step, the batch size is con\ufb01gured. If a reduction of patch size was performed\nthe batch size is set to 2. Otherwise, the remaining GPU memory headroom is utilized to increase the\nbatch size until the GPU is fully utilized. To prevent over\ufb01tting, the batch size is capped such that the\ntotal number of voxels in the minibatch do not exceed 5% of the total number of voxels of all training\ncases. Examples for generated U-Net architectures are presented in Supplementary Information C.1\nand C.2.\nCon\ufb01guration of 3D U-Net cascade: Running a segmentation model on downsampled data increases\nthe size of patches in relation to the image and thus enables the network to accumulate more contextual\ninformation. This comes at the cost of a reduction in details in the generated segmentations and\nmay also cause errors if the segmentation target is very small or characterized by its texture. In a\nhypothetical scenario with unlimited GPU memory, it is thus generally favored to train models at\nfull resolution with a patch size that covers the entire image. The 3D U-Net cascade approximates\nthis approach by \ufb01rst running a 3D U-Net on downsampled images and then training a second, full\nresolution 3D U-Net to re\ufb01ne the segmentation maps of the former. This way, the \u201cglobal\u201d, low\nresolution network utilizes maximal contextual information to generate its segmentation output,\nwhich then serves as an additional input channel that guides the second, \u201clocal\u201d U-Net. The cascade\nis triggered only for datasets where the patch size of the 3d full resolution U-Net covers less than\n12.5% of the median image shape. If this is the case, the target spacing for the downsampled data\nand the architecture of the associated 3D low resolution U-Net are con\ufb01gured jointly in an iterative\nprocess. The target spacing is initialized as the target spacing of the full resolution data. In order\nfor the patch size to cover a large proportion of the input image, the target spacing is then increased\nstepwise by 1% while updating the architecture con\ufb01guration accordingly in each step until the patch\nsize of the resulting network topology surpasses 25% of the current median image shape. If the\ncurrent spacing is anisotropic (factor 2 difference between lowest and highest resolution axis), only\nthe spacing of the higher resolution axes is increased. The con\ufb01guration of the second 3D U-Net of\nthe cascade is identical to the standalone 3D U-Net for which the con\ufb01guration process is described\nabove (except that the upsampled segmentation maps of the \ufb01rst U-Net are concatenated to its input).\nFigure E.1b in the Supplements provides an overview of this optimization process.\nEmpirical parameters\nEnsembling and selection of U-Net con\ufb01guration(s): nnU-Net automat-\nically determines which (ensemble of) con\ufb01guration(s) to use for inference based on the average\nforeground Dice coef\ufb01cient computed via cross-validation on the training data. The selected model(s)\ncan be either a single U-Net (2D, 3D full resolution, 3D low resolution or the full resolution U-Net of\nthe cascade) or an ensemble of any two of these con\ufb01gurations. Models are ensembled by averaging\nsoftmax probabilities.\n19\nPostprocessing: Connected component-based postprocessing is commonly used in medical image\nsegmentation [1, 8]. Especially in organ segmentation it often helps to remove spurious false positive\ndetections by removing all but the largest connected component. nnU-Net follows this assumption\nand automatically benchmarks the effect of suppressing smaller components on the cross-validation\nresults. First, all foreground classes are treated as one component. If suppression of all but the largest\nregion improves the average foreground Dice coef\ufb01cient and does not reduce the Dice coef\ufb01cient\nfor any of the classes, this procedure is selected as the \ufb01rst postprocessing step. Finally, nnU-Net\nbuilds on the outcome of this step and decides whether the same procedure should be performed for\nindividual classes.\nImplementation details\nnnU-Net is implemented in Python utilizing the PyTorch [15] framework.\nThe Batchgenerators library [5] is used for data augmentation. For reduction of computational\nburden and GPU memory footprint, mixed precision training is implemented with Nvidia Apex/Amp\n(https://github.com/NVIDIA/apex). For use as a framework, the source code is available\non GitHub (https://github.com/MIC-DKFZ/nnUNet). Users who seek to use nnU-Net as a\nstandardized benchmark or to run inference with our pretrained models can install nnU-Net via PyPI.\nFor a full description of how to use nnU-Net, please refer to the online documentation available on\nthe GitHub page.\nReporting summary\nFurther information on research design is available in the Nature Research Reporting Summary linked\nto this article.\nCode availability\nThe nnU-Net repository is available at: https://github.com/mic-dkfz/nnunet. Pre-traiend\nmodels for all datasets utilized in this study are available for download at https://zenodo.org/\nrecord/3734294.\nData availability\nAll 19 datasets used in this study are publicly available. References for web access as well as key\ndata properties can be found in the Supplementary Material A and F.\nReferences\n[1] P. Bilic, P. F. Christ, E. Vorontsov, G. Chlebus, H. Chen, Q. Dou, C.-W. Fu, X. Han, P.-\nA. Heng, J. Hesser, et al. The liver tumor segmentation benchmark (lits). arXiv preprint\narXiv:1901.04056ada, 2019.\n[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic\nimage segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.\nIEEE transactions on pattern analysis and machine intelligence, 40(4):834\u2013848, 2017.\n[3] \u00d6. \u00c7i\u00e7ek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger. 3d u-net: learning\ndense volumetric segmentation from sparse annotation. In International conference on medical\nimage computing and computer-assisted intervention, pages 424\u2013432. Springer, 2016.\n20\n[4] M. Drozdzal, E. Vorontsov, G. Chartrand, S. Kadoury, and C. Pal. The importance of skip\nconnections in biomedical image segmentation. In Deep Learning and Data Labeling for\nMedical Applications, pages 179\u2013187. Springer, 2016.\n[5] I. Fabian, J. Paul, W. Jakob, Z. David, P. Jens, K. Simon, S. Justus, K. Andre, R. Tobias,\nW. Sebastian, N. Peter, D. Stefan, K. Gregor, and M.-H. Klaus. batchgenerators - a python\nframework for data augmentation, Jan. 2020.\n[6] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013\n778, 2016.\n[7] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European\nconference on computer vision, pages 630\u2013645. Springer, 2016.\n[8] N. Heller, F. Isensee, K. H. Maier-Hein, X. Hou, C. Xie, F. Li, Y. Nan, G. Mu, Z. Lin, M. Han,\net al. The state of the art in kidney and kidney tumor segmentation in contrast-enhanced ct\nimaging: Results of the kits19 challenge. arXiv preprint arXiv:1912.01054, 2019.\n[9] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 7132\u20137141, 2018.\n[10] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 4700\u20134708, 2017.\n[11] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n[12] S. J\u00e9gou, M. Drozdzal, D. Vazquez, A. Romero, and Y. Bengio. The one hundred layers\ntiramisu: Fully convolutional densenets for semantic segmentation. In Proceedings of the IEEE\nconference on computer vision and pattern recognition workshops, pages 11\u201319, 2017.\n[13] A. L. Maas, A. Y. Hannun, and A. Y. Ng. Recti\ufb01er nonlinearities improve neural network\nacoustic models. In Proc. icml, volume 30, page 3, 2013.\n[14] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori, S. McDonagh,\nN. Y. Hammerla, B. Kainz, et al. Attention u-net: learning where to look for the pancreas. arXiv\npreprint arXiv:1804.03999, 2018.\n[15] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning\nlibrary. In Advances in Neural Information Processing Systems, pages 8024\u20138035, 2019.\n[16] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image\nsegmentation. In MICCAI, pages 234\u2013241. Springer, 2015.\n[17] S. Singh and S. Krishnan. Filter response normalization layer: Eliminating batch dependence in\nthe training of deep neural networks. arXiv preprint arXiv:1911.09737, 2019.\n[18] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception archi-\ntecture for computer vision. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 2818\u20132826, 2016.\n21\n[19] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normalization: The missing ingredient for\nfast stylization. arXiv preprint arXiv:1607.08022, 2016.\n[20] Y. Wu and K. He. Group normalization. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 3\u201319, 2018.\n22\nSupplementary Information\nThis document contains supplementary information for the manuscript \u2019Automated Design\nof Deep Learning Methods for Biomedical Image Segmentation\u2019.\nA\nDataset details\nTable A provides an overview of the datasets used in this manuscript including respective references\nfor data access. The numeric values presented here are computed based on the training cases for each\nof these datasets. They are the basis of the dataset \ufb01ngerprints presented in Figure 5.\nID\nDataset Name\nAssociated\nChallenges\nModalities\nMedian Shape\n(Spacing [mm])\nN\nClasses\nRarest\nClass Ratio\nN Training\nCases\nSegmentation Tasks\nD1\nBrain Tumour\n[15], [14]\nMRI (T1, T1c,\nT2, FLAIR)\n138x169x138\n(1, 1, 1)\n3\n7.310\u22123\n484\nedema, active tumor,\nnecrosis\nD2\nHeart\n[15]\nMRI\n115x320x232\n(1.37, 1.25, 1.25)\n1\n4.010\u22123\n20\nleft ventricle\nD3\nLiver\n[15], [2]\nCT\n432x512x512\n(1, 0.77, 0.77)\n2\n2.610\u22122\n131\nliver, liver tumors\nD4\nHippocampus\n[15]\nMRI\n36x50x35\n(1, 1, 1)\n2\n2.710\u22122\n260\nanterior and posterior\nhippocampus\nD5\nProstate\n[15]\nMRI\n(T2, ADC)\n20x320x319\n(3.6, 0.62, 0.62)\n2\n5.410\u22123\n32\nperipheral and\ntransition zone\nD6\nLung\n[15]\nCT\n252x512x512\n(1.24, 0.79, 0.79)\n1\n3.910\u22124\n63\nlung nodules\nD7\nPancreas\n[15]\nCT\n93x512x512\n(2.5, 0.80, 0.80)\n2\n2.010\u22123\n282\npancreas, pancreas\ncancer\nD8\nHepaticVessel\n[15]\nCT\n49x512x512\n(5, 0.80, 0.80)\n2\n1.110\u22123\n303\nhepatic vessels,\ntumors\nD9\nSpleen\n[15]\nCT\n90x512x512\n(5, 0.79, 0.79)\n1\n4.710\u22123\n41\nspleen\nD10\nColon\n[15]\nCT\n95x512x512\n(5, 0.78, 0.78)\n1\n5.610\u22124\n126\ncolon cancer\nD11\nAbdOrgSeg\n[12]\nCT\n128x512x512\n(3, 0.76, 0.76)\n13\n4.410\u22123\n30\n13 abdominal\norgans\nD12\nPromise\n[13]\nMRI\n24x320x320\n(3.6, 0.61, 0.61)\n1\n2.010\u22122\n50\nprostate\nD13\nACDC\n[1]\ncine MRI\n9x256x216\n(10, 1.56, 1.56)\n3\n1.210\u22122\n200\n(100x2) *\nleft ventricle, right\nventricle,\nmyocardium\nD14\nLiTS **\n[2]\nCT\n432x512x512\n(1, 0.77, 0.77)\n2\n2.610\u22122\n131\nliver, liver tumors\nD15\nMSLesion\n[4]\nMRI (FLAIR,\nMPRAGE, PD,\nT2)\n137x180x137\n(1, 1, 1)\n1\n1.710\u22123\n42\n(21x2) *\nmultiple sclerosis\nlesions\nD16\nCHAOS\n[11]\nMRI\n30x204x256\n(9, 1.66, 1.66)\n4\n3.310\u22122\n60\n(20 + 20x2) *\nliver, spleen, left and\nright kidney\nD17\nKiTS\n[7]\nCT\n107x512x512\n(3, 0.78, 0.78)\n2\n7.510\u22123\n206\nkidney, kidney\ntumor\nD18\nSegTHOR\n[16]\nCT\n178x512x512\n(2.5, 0.98, 0.98)\n4\n4.610\u22124\n40\nheart, aorta,\nesophagus, trachea\nD19\nCREMI\n[6]\nElectron\nMicroscopy\n125x1250x1250\n(40, 4, 4)\n1\n5.210\u22123\n3\nsynaptic clefts\n* multiple annotated examples per training case\n** almost identical to Decathlon Liver; Decathlon changed the training cases and test set slightly\nTable A.1: Overview over the challenge datasets used in this manuscript.\n23\nB\nnnU-Net Design Principles\nHere we present a brief overview of the design principles of nnU-Net on a conceptual level. Please\nrefer to the online methods for a more detailed information on how these guidelines are implemented.\nB.1\nBlueprint Parameters\n\u2022 Architecture Design decisions:\n\u2013 U-Net like architectures enable state of the art segmentation when the pipeline is\nwell-con\ufb01gured. According to our experience, sophisticated architectural variations are\nnot required to achieve state of the art performance.\n\u2013 Our architectures only use plain convolutions, instance normalization and Leaky non-\nlinearities. The order of operations in each computational block is conv - instance norm\n- leaky ReLU.\n\u2013 We use two computational blocks per resolution stage in both encoder and decoder.\n\u2013 Downsampling is done with strided convolutions (the convolution of the \ufb01rst block of\nthe new resolution has stride >1), upsampling is done with convolutions transposed.\nWe should note that we did not observe substantial disparities in segmentation accuracy\nbetween this approach and alternatives (e.g. max pooling, bi/trilinear upsampling).\n\u2022 Selecting the best U-Net con\ufb01guration: It is dif\ufb01cult to estimate which U-Net con\ufb01guration\nperforms best on what dataset. To address this, nnU-Net designs three separate con\ufb01gurations\nand automatically chooses the best one based on cross-validation (see inferred parameters).\nPredicting which con\ufb01gurations should be trained on which dataset is a future research\ndirection.\n\u2013 2D U-Net: Runs on full resolution data. Expected to work well on anisotropic data,\nsuch as D5 (Prostate MRI) and D13 (ACDC, cine MRI) (for dataset references see\nTable A).\n\u2013 3D full resolution U-Net: Runs on full resolution data. Patch size is limited by\navailability of GPU memory. Is overall the best performing con\ufb01guration (see results\nin F). For large data, however, the patch size may be too small to aggregate suf\ufb01cient\ncontextual information.\n\u2013 3D U-Net cascade: Speci\ufb01cally targeted towards large data. First, coarse segmentation\nmaps are learned by a 3D U-Net that operates on low resolution data. These segmen-\ntation maps are then re\ufb01ned by a second 3D U-Net that operates on full resolution\ndata.\n\u2022 Training Scheme\n\u2013 All trainings run for a \ufb01xed length of 1000 epochs, where each epoch is de\ufb01ned as 250\ntraining iterations (using the batch size con\ufb01gured by nnU-Net). Shorter trainings than\nthis default empirically result in diminished segmentation performance.\n\u2013 As for the opimizer, stochastic gradient descent with a high initial learning rate (0.01)\nand a large nesterov momentum (0.99) empirically provided the best results. The\nlearning rate is reduced during the training using the \u2019polyLR\u2019 schedule as described in\n[5], which is an almost linear decrease to 0.\n\u2013 Data augmentation is essential to achieve state of the art performance. It is important\nto run the augmentations on the \ufb02y and with associated probabilities to obtain a never\nending stream of unique examples (see Section D for details).\n24\n\u2013 Data in the biomedical domain suffers from class imbalance. Rare classes could end\nup being ignored because they are underrepresented during training. Oversampling\nforeground regions addresses this issue reliably. It should, however, not be overdone so\nthat the network also sees all the data variability of the background.\n\u2013 The Dice loss function is well suited to address the class imbalance, but comes with\nits own drawbacks. Dice loss optimizes the evaluation metric directly, but due to the\npatch based training, in practice merely approximates it. Furthermore, oversampling\nof classes skews the class distribution seen during training. Empirically, combining\nthe Dice loss with a cross-entropy loss improved training stability and segmentation\naccuracy. Therefore, the two loss terms are simply averaged.\n\u2022 Inference\n\u2013 Validation sets of all folds in the cross-validation are predicted by the single model\ntrained on the respective training data. The 5 models resulting from training on 5\nindividual folds are subsequently used as an ensemble for predicting test cases.\n\u2013 Inference is done patch based with the same patch size as used during training. Fully\nconvolutional inference is not recommended because it causes issues with zero-padded\nconvolutions and instance normalization.\n\u2013 To prevent stitching artifacts, adjacent predictions are done with a distance of patch_size\n/ 2. Predictions towards the border are less accurate, which is why we use Gaussian\nimportance weighting for softmax aggregation (the center voxels are weighted higher\nthen the border voxels).\nB.2\nInferred Parameters\nThese parameters are not \ufb01xed across datasets, but con\ufb01gured on-the-\ufb02y by nnU-Net according to the\ndata \ufb01ngerprint (low dimensional representation of dataset properties) of the task at hand.\n\u2022 Dynamic Network adaptation:\n\u2013 The network architecture needs to be adapted to the size and spacing of the input\npatches seen during training. This is necessary to ensure that the receptive \ufb01eld of the\nnetwork covers the entire input.\n\u2013 We perform downsampling until the feature maps are relatively small (minimum is\n4 \u00d7 4(\u00d74)) to ensure suf\ufb01cient context aggregation.\n\u2013 Due to having a \ufb01xed number of blocks per resolution step in both the encoder and\ndecoder, the network depth is coupled to its input patch size. The number of convolu-\ntional layers in the network (excluding segmentation layers) is (5 \u2217k + 2) where k is\nthe number of downsampling operations (5 per downsampling stems from 2 convs in\nthe encoder, 2 in the decoder plus the convolution transpose).\n\u2013 Additional loss functions are applied to all but the two lowest resolutions of the decoder\nto inject gradients deep into the network.\n\u2013 For anisotropic data, pooling is \ufb01rst exclusively performed in-plane until the resolution\nmatches between the axes. Initially, 3D convolutions use a kernel size of 1 (making\nthem effectively 2D convolutions) in the out of plane axis to prevent aggregation of\ninformation across distant slices. Once an axes becomes too small, downsampling is\nstopped individually for this axis.\n\u2022 Con\ufb01guration of the input patch size:\n25\n\u2013 Should be as large as possible while still allowing a batch size of 2 (under a given GPU\nmemory constraint). This maximizes the context available for decision making in the\nnetwork.\n\u2013 Aspect ratio of patch size follows the median shape (in voxels) of resampled training\ncases.\n\u2022 Batch size:\n\u2013 Batch size is con\ufb01gured with a minimum of 2 to ensure robust optimization, since\nnoise in gradients increases with fewer sample in the minibatch.\n\u2013 If GPU memory headroom is available after patch size con\ufb01guration, the batch size is\nincreased until GPU memory is maxed out.\n\u2022 Target spacing and resampling:\n\u2013 For isotropic data, the median spacing of training cases (computed independently\nfor each axis) is set as default. Resampling with third order spline (data) and linear\ninterpolation (one hot encoded segmentation maps such as training annotations) give\ngood results.\n\u2013 For anisotropic data, the target spacing in the out of plane axis should be smaller than\nthe median, resulting in higher resolution in order to reduce resampling artifacts. To\nachieve this we set the target spacing as the 10th percentile of the spacings found for\nthis axis in the training cases. Resampling across the out of plane axis is done with\nnearest neighbor for both data and one-hot encoded segmentation maps.\n\u2022 Intensity normalization:\n\u2013 Z-score per image (mean substraction and division by standard deviation) is a good\ndefault.\n\u2013 We deviate from this default only for CT images, where a global normalization scheme\nis determined based on the intensities found in foreground voxels across all training\ncases.\nB.3\nEmpirical Parameters\nSome parameters cannot be inferred by simply looking at the dataset \ufb01ngerprint of the training cases.\nThese are determined empirically by monitoring validation performance after training.\n\u2022 Model selection: While the 3D full resolution U-Net shows overall best performance,\nselection of the best model for a speci\ufb01c task at hand can not be predicted with perfect\naccuracy. Therefore, nnU-Net generates three U-Net con\ufb01gurations and automatically picks\nthe best performing method (or ensemble of methods) after cross-validation.\n\u2022 Postprocessing: Often, particularly in medical data, the image contains only one instance\nof the target structure. This prior knowledge can often be exploited by running connected\ncomponent analysis on the predicted segmentation maps and removing all but the largest\ncomponent. Whether to apply this postprocessing is determined by monitoring validation\nperformance after cross-validation. Speci\ufb01cally, postprocessing is triggered for individual\nclasses where the Dice score is improved by removing all but the largest component.\n26\nFigure C.1: Network architectures generated by nnU-Net for the ACDC dataset (D13)\nC\nAnalysis of exemplary nnU-Net-generated pipelines\nIn this section we brie\ufb02y introduce the pipelines generated by nnU-Net for D13 (ACDC) and D14\n(LiTS) to create an intuitive understanding of nnU-Nets design principles and the motivation behind\nthem.\nC.1\nACDC\nFigure C.1 provides a summary of the pipelines that were automatically generated by nnU-Net for\nthis dataset.\nDataset Description\nThe Automated Cardiac Diagnosis Challenge (ACDC) [1] was hosted by\nMICCAI in 2017. Since then it is running as an open challenge with data and current leaderboard\navailable at https://acdc.creatis.insa-lyon.fr. In the segmentation part of the challenge,\nparticipating teams were asked to generate algorithms for segmenting the right ventricle, the left\nmyocardium and the left ventricular cavity from cine MRI. For each patient, reference segmentations\nfor two time steps within the cardiac cycle were provided. With 100 training patients, this amounts to\na total of 200 annotated images. One key property of cine MRI is that slice acquisition takes place\nacross multiple cardiac cycles and breath holds. This results in a limited number of slices and thus a\nlow out of plane resolution as well as the possibility for slice misalignments. Figure C.1 provides a\nsummary of the pipelines that were automatically generated by nnU-Net for this dataset. The typical\n27\nimage shape (here the median image size is computed for each axis independently) is 9 \u00d7 237 \u00d7 256\nvoxels at a spacing of 10 \u00d7 1.56 \u00d7 1.56 mm.\nIntensity Normalization\nWith the images being MRI, nnU-Net normalizes all images individually\nby subtracting their mean and dividing by their standard deviation.\n2D U-Net\nAs target spacing for the in-plane resolution, 1.56 \u00d7 1.56 mm is determined. This is\nidentical for the 2D and the 3D full resolution U-Net. Due to the 2D U-Net operating on slices only,\nthe out of plane resolution for this con\ufb01guration is not altered and remains heterogeneous within the\ntraining set. The 2D U-Net is con\ufb01gured as described in the Online Methods 4 to have a patch size of\n256 \u00d7 224 voxels, which fully covers the typical image shape after in-plane resampling (237 \u00d7 208).\n3D U-Net The size and spacing anisotropy of this dataset causes the out-of-plane target spacing\nof the 3D full resolution U-Net to be selected as 5mm, corresponding to the 10th percentile of the\nspacings found in the training cases. In datasets such as ACDC, the segmentation contour can change\nsubstantially between slices due to the large slice to slice distance. Choosing the target spacing\nto be lower results in more images that are upsampled for U-Net training and then downsampled\nfor the \ufb01nal segmentation export. Preferring this variant over the median causes more images to\nbe downsampled for training and then upsampled for segmentation export and therefore reduces\ninterpolation artifacts substantially. Also note that resampling the out of plane axis is done with\nnearest neighbor interpolation.The median image shape after resampling for the 3D full resolution\nU-Net is 18 \u00d7 237 \u00d7 208 voxels. As described in the Online Methods 4 nnU-Net con\ufb01gures a patch\nsize of 20 \u00d7 256 \u00d7 224 for network training, which \ufb01ts into the memory budget with a batch size of 3.\nNote how the convolutional kernel sizes in the 3D U-Net start with (1 \u00d7 3 \u00d7 3) which is effectively a\n2D convolution for the initial layers (see also Figure C.1). The reasoning behind this is that due to the\nlarge discrepancy in voxel spacing, too many changes are expected across slices and the aggregation\nof imaging information may therefore not be bene\ufb01cial. Similarly, pooling is done in-plane only\n(conv kernel stride (1, 2, 2)) until the spacing between in-plane and out-of-plane axes are within a\nfactor of 2. Only after the spacings approximately match the pooling and the convolutional kernel\nsizes become isotropic.\n3D U-Net\ncascade Since the 3D U-Net already covers the whole median image shape, the U-Net\ncascade is not necessary and therefore omitted.\nTraining and Postprocessing\nDuring training, spatial augmentations for the 3D U-Net (such as\nscaling and rotation) are done in-plane only to prevent resampling of imaging information across\nslices which would cause interpolation artifacts. Each U-Net con\ufb01guration is trained in a \ufb01ve-fold\ncross-validation on the training cases. Note that we interfere with the splits in order to ensure that\npatients are properly strati\ufb01ed (since there are two images per patient). Thanks to the cross-validation,\nnnU-Net can use the entire training set for validation and ensembling. To this end, the validation splits\nof each of the \ufb01ve fold are aggregated. nnU-Net evaluates the performance (ensemble of models or\nsingle con\ufb01guration) by averaging the Dice scores over all foreground classes and cases, resulting in a\nsingle scalar value. Detailed results are omitted here for brevity (they are presented in Supplementary\nInformation F). Based on this evaluation scheme, the 2D U-Net obtains a score of 0.9165, the 3D full\nresolution a score of 0.9181 and the ensemble of the two a score of 0.9228. Therefore the ensemble\nis selected for predicting the test cases. Postprocessing is con\ufb01gured on the segmentation maps of\nthe ensemble. Removing all but the largest connected component was found bene\ufb01cial for the right\nventricle and the left ventricular cavity.\n28\nC.2\nLiTS\nFigure C.2 provides a summary of the pipelines that were automatically generated by nnU-Net for\nthis dataset.\nDataset Description\nThe Liver and Liver Tumor Segmentation challenge (LiTS) [2] was hosted by\nMICCAI in 2017. Due to the large, high quality dataset it provides, the challenge plays an important\nrole in concurrent research. The challenge is hosted at https://competitions.codalab.org/\ncompetitions/17094. The segmentation task in LiTS is the segmentation of the liver and liver\ntumors in abdominal CT scans. The challenge provides 131 training cases with reference annotations.\nThe test set has a size of 70 cases and the reference annotations are known only to the challenge\norganizers. The median image shape of the training cases is 432 \u00d7 512 \u00d7 512 voxels with a\ncorresponding voxel spacing of 1 \u00d7 0.77 \u00d7 0.77 mm.\nIntensity Normalization\nVoxel intensities in CT scans are linked to quantitative physical properties\nof the tissue. The intensities are therefore expected to be consistent between scanners. nnU-Net\nleverages this consistency by applying a global intensity normalization scheme (as opposed to\nACDC in Supplementary Information C.1, where cases are normalized individually using their mean\nand standard deviation). To this end, nnU-Net extracts intensity information as part of the dataset\n\ufb01ngerprint: the intensities of the voxels belonging to any of the foreground classes (liver and liver\ntumor) are collected across all training cases. Then, the mean and standard deviations of these values\nas well as their 0.5 and 99.5 percentiles are computed. Subsequently, all images are normalized by\nclipping them to the 0.5 and 99.5 percentiles, followed by subtraction of the global mean and division\nby the global standard deviation.\n2D U-Net\nThe target spacing for the 2D U-Net is determined to be NA \u00d7 0.77 \u00d7 0.77 mm, which\ncorresponds to the median voxel spacing encountered in the training cases. Note that the 2D U-Net\noperates on slices only, so the out of plane axis is left untouched. Resampling the training cases\nresults in a median image shape of NA \u00d7 512 \u00d7 512 voxels (we indicate by NA that this axis is not\nresampled). Since this is the median shape, cases in the training set can be smaller or larger than that.\nThe 2D U-Net is con\ufb01gured to have an input patch size of 512 \u00d7 512 voxels and a batch size of 12.\n3D U-Net\nThe target spacing for the 3D U-Net is determined to be 1 \u00d7 0.77 \u00d7 0.77 mm,\nwhich corresponds to the median voxel spacing. Because the median spacing is nearly isotropic,\nnnU-Net does not use the 10th percentile for the out of plane axis as was the case for ACDC\n(see Supplementary Information C.1). The resampling strategy is decided on a per-image basis.\nIsotropic cases (maximum axis spacing / minimum axis spacing < 3) are resampled with third order\nspline interpolation for the image data and linear interpolation for the segmentations. Note that\nsegmentation maps are always converted into a one hot representation prior to resampling which\nis converted back to a segmentation map after the interpolation. For anisotropic images, nnU-Net\nresamples the out-of-plane axis separately, as was done in ACDC.\nAfter resampling, the median image shape is 482 \u00d7 512 \u00d7 512.\nnnU-Net prioritizes a large\npatch size over a large batch size (note that these are coupled under a given GPU memory budget) to\ncapture as much contextual information as possible. The 3D U-Net is thus con\ufb01gured to have a patch\nsize of 128 \u00d7 128 \u00d7 128 voxels and a batch size of 2, which is the minimum allowed according to\n29\nFigure C.2: Network architectures generated by nnU-Net for the LiTS dataset (D14)\n30\nnnU-Net heuristics. Since The input patches have nearly isotropic spacing, all convolutional kernel\nsizes and downsampling strides are isotropic (3 \u00d7 3 \u00d7 3 and 2 \u00d7 2 \u00d7 2, respectively).\n3D U-Net cascade\nAlthough nnU-Net prioritizes large input patches, the patch size of the 3D full\nresolution U-Net is too small to capture suf\ufb01cient contextual information (it only covers 1/60 of the\nvoxels of the median image shape after resampling). This can cause misclassi\ufb01cations of voxels\nbecause the patches are too \u2018zoomed in\u2019, making for instance the distinction between the spleen\nand the liver particularly hard. The 3D U-Net cascade is designed to tackle this problem by \ufb01rst\ntraining a 3D U-Net on downsampled data and then re\ufb01ning the low-resolution segmentation output\nwith a second U-Net that operates as full resolution. Using the process described in the Online\nMethods 4 as well as Figure E.1 b), the target spacing for the low resolution U-Net is determined to\nbe 2.47 \u00d7 1.9 \u00d7 1.9 mm, resulting in a median image shape of 195 \u00d7 207 \u00d7 207 voxels. The 3D low\nresolution operates on 128 \u00d7 128 \u00d7 128 patches with a batch size of 2. Note that while this setting is\nidentical to the 3D U-Net con\ufb01guration here, this is not necessarily the case for other datasets. If the\n3D full resolution U-Net data was anisotropic, nnU-Net would prioritize to downsample the higher\nresolution axes \ufb01rst resulting in a deviating network architecture, patch size and batch size. After\n\ufb01ve-fold cross-validation of the 3D low resolution U-Net, the segmentation maps of the respective\nvalidation sets are upsampled to the target spacing of the 3D full resolution U-Net. The full resolution\nU-Net of the cascade (which has an identical con\ufb01guration to the regular 3D full resolution U-Net) is\nthen trained to re\ufb01ne the coarse segmentation maps and correct any errors it encounters. This is done\nby concatenating a one hot encoding of the upsampled segmentations to the input of the network.\nTraining and Postprocessing\nAll network con\ufb01gurations are trained as \ufb01ve fold cross-validation.\nnnU-Net again evaluates all con\ufb01gurations by computing the average Dice score across all foreground\nclasses, resulting in a scalar metric per con\ufb01guration. Based on this evaluation scheme, the scores are\n0.7625 for the 2D U-Net, 0.8044 for the 3D full resolution U-Net, 0.7796 for the 3D low resolution\nU-Net and 0.8017 for the full resolution 3D U-Net of the cascade. The best combination of two\nmodels was identi\ufb01ed as the one between the low and full resolution U-Nets with a score of 0.8111.\nPostprocessing is con\ufb01gured on the segmentation maps of this ensemble. Removing all but the largest\nconnected component was found bene\ufb01cial for the combined foreground region (union of liver and\nliver tumor label) as well as for the liver label alone, as both resulted in small performance gains\nwhen empirically testing it on the training data.\nD\nDetails on nnU-Net\u2019s Data Augmentation\nA variety of data augmentation techniques is applied during training. All augmentations are computed\non the \ufb02y on the CPU using background workers. The data augmentation pipeline is implemented\nwith the publicly available batchgenerators framework 2. nnU-Net does not vary the parameters of\nthe data augmentation pipeline between datasets.\nSampled patches are initially larger than the patch size used for training.\nThis results in\nless out of boundary values (here 0) being introduced during data augmentation when rotation and\nscaling is applied. As a part of the rotation and scaling augmentation, patches are center-cropped to\nthe \ufb01nal target patch size. To ensure that the borders of original images appear in the \ufb01nal patches,\npreliminary crops may initially extend outside the boundary of the image.\n2https://github.com/MIC-DKFZ/batchgenerators\n31\nSpatial augmentations (rotation, scaling, low resolution simulation) are applied in 3D for\nthe 3D U-Nets and applied in 2D when training the 2D U-Net or a 3D U-Net with anisotropic patch\nsize. A patch size is considered anisotropic if the largest edge length of the patch size is at least three\ntimes larger than the smallest.\nTo increase the variability in generated patches, most augmentations are varied with param-\neters drawn randomly from prede\ufb01ned ranges. In this context, x \u223cU(a, b) indicates that x was\ndrawn from a uniform distribution between a and b. Furthermore, all augmentations are applied\nstochastically according to a prede\ufb01ned probability.\nThe following augmentations are applied by nnU-Net (in the given order):\n1. Rotation and Scaling. Scaling and rotation are applied together for improved speed of\ncomputation. This approach reduces the amount of required data interpolations to one.\nScaling and rotation are applied with a probability of 0.2 each (resulting in probabilities of\n0.16 for only scaling, 0.16 for only rotation and 0.08 for both being triggered). If processing\nisotropic 3D patches, the angles of rotation (in degrees) \u03b1x, \u03b1y and \u03b1z are each drawn\nfrom U(\u221230, 30). If a patch is anisotropic or 2D, the angle of rotation is sampled from\nU(\u2212180, 180). If the 2D patch size is anisotropic, the angle is sampled from U(\u221215, 15).\nScaling is implemented via multiplying coordinates with a scaling factor in the voxel grid.\nThus, scale factors smaller than one result in a \"zoom out\" effect while values larger one\nresult in a \"zoom in\" effect. The scaling factor is sampled from U(0.7, 1.4) for all patch\ntypes.\n2. Gaussian Noise. Zero centered additive Gaussian noise is added to each voxel in the sample\nindependently. This augmentation is applied with a probability of 0.15. The variance of the\nnoise is drawn from U(0, 0.1) (note that the voxel intensities in all samples are close to zero\nmean and unit variance due to intensity normalization).\n3. Gaussian Blur. Blurring is applied with a probability of 0.2 per sample. If this augmentation\nis triggered in a sample, blurring is applied with a probability of 0.5 for each of the\nassociated modalities (resulting in a combined probability of only 0.1 for samples with a\nsingle modality). The width (in voxels) of the Gaussian kernel \u03c3 is sampled from U(0.5, 1.5)\nindependently for each modality.\n4. Brightness. Voxel intensities are multiplied by x \u223cU(0.7, 1.3) with a probability of 0.15.\n5. Contrast. Voxel intensities are multiplied by x \u223cU(0.65, 1.5) with a probability of 0.15.\nFollowing multiplication, the values are clipped to their original value range.\n6. Simulation of low resolution. This augmentation is applied with a probability of 0.25 per\nsample and 0.5 per associated modality. Triggered modalities are downsampled by a factor\nof U(1, 2) using nearest neighbor interpolation and then sampled back up to their original\nsize with cubic interpolation. For 2D patches or anisotropic 3D patches, this augmentation\nis applied only in 2D leaving the out of plane axis (if applicable) in its original state.\n7. Gamma augmentation. This augmentation is applied with a probability of 0.15. The\npatch intensities are scaled to a factor of [0, 1] of their respective value range. Then, a\nnonlinear intensity transformation is applied per voxel: inew = i\u03b3\nold with \u03b3 \u223cU(0.7, 1.5).\nThe voxel intensities are subsequently scaled back to their original value range. With a\n32\nprobability of 0.15, this augmentation is applied with the voxel intensities being inverted\nprior to transformation: (1 \u2212inew) = (1 \u2212iold)\u03b3.\n8. Mirroring. All patches are mirrored with a probability of 0.5 along all axes.\nFor the full resolution U-Net of the U-net cascade, nnU-Net additionally applies the following\naugmentations to the segmentation masks generated by the low resolution 3D U-net. Note that the\nsegmentations are stored as one hot encoding.\n1. Binary Operators. With probability 0.4, a binary operator is applied to all labels in the\npredicted masks. This operator is randomly chosen from [dilation, erosion, opening, closing].\nThe structure element is a sphere with radius r \u223cU(1, 8). The operator is applied to the\nlabels in random order. Hereby, the one hot encoding property is retained. Dilation of one\nlabel, for example, will result in removal of all other labels in the dilated area.\n2. Removal of Connected Components. With probability 0.2, connected components that\nare smaller than 15% of the patch size are removed from the one hot encoding.\nE\nNetwork Architecture Con\ufb01guration\nFigure E.1 serves as a visual aid for the iterative process of architecture con\ufb01guration described in\nthe online methods.\nF\nSummary of nnU-Net Challenge Participations\nIn this section we provide details of all challenge participations.\nIn some participations, manual intervention regarding the format of input data or the cross-validation\ndata splits was required for compatibility with nnU-Net. For each dataset, we disclose all manual\ninterventions in this section. The most common cause for manual intervention was training cases that\nwere related to each other (such as multiple time points of the same patient) and thus required to be\nseparated for mutual exclusivity between data splits. A detailed description of how to perform this\nintervention is further provided along with the source code.\nFor each dataset, we run all applicable nnU-Net con\ufb01gurations (2D, 3D fullres, 3D lowres,\n3D cascade) in 5-fold cross-validation. All models are trained from scratch without pretraining and\ntrained only on the provided training data of the challenge without external training data. Note that\nother participants may be using external data in some competitions. For each dataset, nnU-Net\nsubsequently identi\ufb01es the ideal con\ufb01guration(s) based on cross-validation and ensembling. Finally,\nThe best con\ufb01guration is used to predict the test cases.\nThe pipeline generated by nnU-Net is provided for each dataset in the compact representa-\ntion described in Section F.2. We furthermore provide a table containing detailed cross-validation as\nwell as test set results.\nAll leaderboards were last accessed on December 12th, 2019.\n33\nFigure E.1: Work\ufb02ow for network architecture con\ufb01guration. a) the con\ufb01guration of a U-Net\narchitecture given an input patch size and corresponding voxel spacing. Due to discontinuities in\nGPU memory consumption (due to changes in number of pooling operations and thus network depth),\nthe architecture con\ufb01guration cannot be solved analytically. b) Con\ufb01guration of the 3D low resolution\nU-Net of the U-Net cascade. The input patch size of the 3D lowres U-Net must cover at least 1/4 of\nthe median shape of the resampled trainig cases to ensure suf\ufb01cient contextual information. Higher\nresolution axes are downsampled \ufb01rst, resulting in a potentially different aspect ratio of the data\nrelative to the full resolution data. Due to the patch size following this aspect ratio, the network\narchitecture of the low resolution U-Net may differ from the full resolution U-Net. This requires\nrecon\ufb01guration of the network architecture as depicted in a) for each iteration. All computations are\nbased on memory consumption estimates resulting in fast computation times (sub 1s for con\ufb01guring\nall network architectures).\nF.1\nChallenge Inclusion Criteria\nWhen selecting challenges for participation, our goal was to apply nnU-Net to as many different\ndatasets as possible to demonstrate its robustness and \ufb02exibility. We applied the following criteria to\nensure a rigorous and sound testing environment:\n1. The task of the challenge is semantic segmentation in any 3D imaging modality with images\nof any size.\n2. Training cases are provided to the challenge participants.\n3. Test cases are separate, with the ground truth not being available to the challenge participants.\n4. Comparison to results from other participants is possible (e.g. through standardized evalua-\ntion with an online platform and a public leaderboard).\n34\nFigure F.1: Decoding the architecture. We provide all generated architectures in a compact represen-\ntation from which they can be fully reconstructed if desired. The architecture displayed here can be\nrepresented by means of kernel sizes [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]] and\nstrides [[1, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2]] (see description in the text).\nThe competitions outlined below are the ones who quali\ufb01ed under these criteria and were thus selected\nfor evaluation of nnU-Net. To our knowledge, CREMI 3 is the only competition from the biological\ndomain that meets these criteria.\nF.2\nCompact Architecture Representation\nIn the following sections, network architectures generated by nnU-Net will be presented in a\ncompact representation consisting of two lists: one for the convolutional kernel sizes and one for\nthe downsampling strides. As we describe in this section, this representation can be used to fully\nreconstruct the entire network architecture. The condensed representation is chosen to prevent an\nexcessive amount of \ufb01gures.\nFigure F.2 exemplary shows the 3D full resolution U-Net for the ACDC dataset (D13).\nThe architecture has 6 resolution stages. Each resolution stage in both encoder and decoder consists\nof two computational blocks. Each block is a sequence of (conv - instance norm - leaky ReLU), as\ndescribed in 4. In this \ufb01gure, one such block is represented by an outlined blue box. Within each\nbox, the stride of the convolution is indicated by the \ufb01rst three numbers (1,1,1 for the uppermost left\nbox) and the kernel size of the convolution is indicated by the second set of numbers (1,3,3 for the\nuppermost left box). Using this information, along with the template with which our architectures are\ndesigned, we can fully describe the presented architecture with the following lists:\n\u2022 Convolutional Kernel Sizes: The kernel sizes of this architecture are [[1, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]. Note that this list contains 6 elements, matching the\n6 resolutions encountered in the encoder. Each element in this list gives the kernel size of\nthe convolutional layers at this resolution (here this is three digits due to the convolutions\nbeing three dimensional). Within one resolution, both blocks use the same kernel size. The\nconvolutions in the decoder mirror the encoder (dropping the last entry in the list due to the\nbottleneck).\n3https://cremi.org/leaderboard/\n35\n\u2022 Downsampling strides: The strides for downsampling here are [[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[1, 2, 2], [1, 2, 2]]. Each downsampling step in the encoder is represented by one entry. A\nstride of 2 results in a downsampling of factor 2 along that axis which a stride of 1 leaves\nthe size unchanged. Note how the stride initially is [1, 2, 2] due to the spacing discrepancy.\nThis changes the initial spacing of 5 \u00d7 1.56 \u00d7 1.56 mm to a spacing of 5 \u00d7 3.12 \u00d7 3.12 mm\nin the second resolution step. The downsampling strides only apply to the \ufb01rst convolution\nof each resolution stage in the encoder. The second convolution always has a stride of [1,\n1, 1]. Again, the decoder mirrors the encoder, but the stride is used as output stride of the\nconvolution transposed (resulting in appropriate upscaling of feature maps). Outputs of all\nconvolutions transposed have the same shape as the skip connection originating from the\nencoder.\nSegmentation outputs for auxiliary losses are added to all but the two lowest resolution steps.\nF.3\nMedical Segmentation Decathlon\nChallenge summary\nThe Medical Segmentation Decathlon4 [15] is a competition that spans 10\ndifferent segmentation tasks. These tasks are selected to cover a large proportion of the dataset\nvariability in the medical domain. The overarching goal of the competition was to encourage\nresearchers to develop algorithms that can work with these datasets out of the box without manual\nintervention. Each of the tasks comes with respective training and test data. A detailed description of\ndatasets can be found on the challenge homepage. Originally, the challenge was divided into two\nphases: In phase I, 7 datasets were provided to the participants for algorithm development. In phase\nII, the algorithms were applied to three additional and previously unseen datasets without further\nchanges. Challenge evaluation was performed for the two phases individually and winners were\ndetermined based on their performance on the test cases.\nInitial version of nnU-Net\nA preliminary version of nnU-Net was developed as part\nof our entry in this competition, where it achieved the \ufb01rst rank in both phases (see\nhttp://medicaldecathlon.com/results.html).\nWe subsequently made the respective\nchallenge report available on arXiv [10].\nnnU-Net has since been re\ufb01ned using all ten tasks of the Medical Segmentation Decathlon.\nThe current version of nnU-Net as presented in this publication was again submitted to the open\nleaderboard (https://decathlon-10.grand-challenge.org/evaluation/results/), and\nachieved the \ufb01rst rank outperforming the initial nnU-Net as well as other methods that held the state\nof the art since the original competition [17].\nApplication of nnU-Net to the Medical Segmentation Decathlon\nnnU-Net was applied to all ten\ntasks of the Medical Segmentation Decathlon without any manual intervention.\nBrainTumour (D1)\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n4http://medicaldecathlon.com/\n36\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 1 x 1\n1 x 1 x 1\n-\nMedian image shape at\ntarget spacing:\nNA x 169 x 138\n138 169 138\n-\nPatch size:\n192 x 160\n128 x 128 x 128\n-\nBatch size:\n107\n2\n-\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\n-\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n-\nTable F.1: Network con\ufb01gurations generated by nnU-Net for the BrainTumour dataset from the\nMedical Segmentation Decathlon (D1). For more information on how to decode downsampling\nstrides and kernel sizes into an architecture, see F.2\nedema\nnon-enhancing tumor\nenhancing tumour\nmean\n2D\n0.7957\n0.5985\n0.7825\n0.7256\n3D_fullres *\n0.8101\n0.6199\n0.7934\n0.7411\nBest Ensemble\n0.8106\n0.6179\n0.7926\n0.7404\nPostprocessed\n0.8101\n0.6199\n0.7934\n0.7411\nTest set\n0.68\n0.47\n0.68\n0.61\nTable F.2: Decathlon BrainTumour (D1) results. Note that all reported Dice scores (except the test\nset) were computed using \ufb01ve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\") Note that the Dice scores for the test set are computed with the online platform and only\ntwo signi\ufb01cant digits are reported. Best ensemble on this dataset was the combination of the 2D\nU-Net and the 3D full resolution U-Net.\nHeart (D2)\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 1.25 x 1.25\n1.37 x 1.25 x 1.25\n-\nMedian image shape at\ntarget spacing:\nNA x 320 x 232\n115 x 320 x 232\n-\nPatch size:\n320 x 256\n80 x 192 x 160\n-\nBatch size:\n40\n2\n-\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 1]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\n-\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], ]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n-\nTable F.3: Network con\ufb01gurations generated by nnU-Net for the Heart dataset from the Medical\nSegmentation Decathlon (D2). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\n37\nleft atrium\nmean\n2D\n0.9090\n0.9090\n3D_fullres *\n0.9328\n0.9328\nBest Ensemble\n0.9268\n0.9268\nPostprocessed\n0.9329\n0.9329\nTest set\n0.93\n0.93\nTable F.4: Decathlon Heart (D2) results. Note that all reported Dice scores (except the test set)\nwere computed using \ufb01ve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signi\ufb01cant digits are reported. Best ensemble on this dataset was the combination of the 2D\nU-Net and the 3D full resolution U-Net.\nLiver (D3)\nNormalization: Clip to [\u221217, 201], then subtract 99.40 and \ufb01nally divide by 39.36.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.7676 x 0.7676\n1 x 0.7676 x 0.7676\n2.47 x 1.90 x 1.90\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n482 x 512 x 512\n195 x 207 x 207\nPatch size:\n512 x 512\n128 x 128 x 128\n128 x 128 x 128\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.5: Network con\ufb01gurations generated by nnU-Net for the Liver dataset from the Medical\nSegmentation Decathlon (D3). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\nliver\ncancer\nmean\n2D\n0.9547\n0.5637\n0.7592\n3D_fullres\n0.9571\n0.6372\n0.7971\n3D_lowres\n0.9563\n0.6028\n0.7796\n3D cascade\n0.9600\n0.6386\n0.7993\nBest Ensemble*\n0.9613\n0.6564\n0.8088\nPostprocessed\n0.9621\n0.6600\n0.8111\nTest set\n0.96\n0.76\n0.86\nTable F.6: Decathlon Liver (D3) results. Note that all reported Dice scores (except the test set)\nwere computed using \ufb01ve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signi\ufb01cant digits are reported. Best ensemble on this dataset was the combination of the 3D low\nresolution U-Net and the 3D full resolution U-Net.\nHippocampus (D4)\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n38\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 1 x 1\n1 x 1 x 1\n-\nMedian image shape at\ntarget spacing:\nNA x 50 x 35\n36 x 50 x 35\n-\nPatch size:\n56 x 40\n40 x 56 x 40\n-\nBatch size:\n366\n9\n-\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2]]\n-\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3],\n[3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3]]\n-\nTable F.7: Network con\ufb01gurations generated by nnU-Net for the Hippocampus dataset from the\nMedical Segmentation Decathlon (D4). For more information on how to decode downsampling\nstrides and kernel sizes into an architecture, see F.2\nAnterior\nPosterior\nmean\n2D\n0.8787\n0.8595\n0.8691\n3D_fullres *\n0.8975\n0.8807\n0.8891\nBest Ensemble\n0.8962\n0.8790\n0.8876\nPostprocessed\n0.8975\n0.8807\n0.8891\nTest set\n0.90\n0.89\n0.895\nTable F.8: Decathlon Hippocampus (D4) results. Note that all reported Dice scores (except the test\nset) were computed using \ufb01ve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signi\ufb01cant digits are reported. Best ensemble on this dataset was the combination of the 2D\nU-Net and the 3D full resolution U-Net.\nProstate (D5)\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.62 x 0.62\n3.6 x 0.62 x 0.62\n-\nMedian image shape at\ntarget spacing:\nNA x 320 x 319\n20 x 320 x 319\n-\nPatch size:\n320 x 320\n20 x 320 x 256\n-\nBatch size:\n32\n2\n-\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2]]\n[[1, 2, 2], [1, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2], [1, 2, 2]]\n-\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [1, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3]]\n-\nTable F.9: Network con\ufb01gurations generated by nnU-Net for the Prostate dataset from the Medical\nSegmentation Decathlon (D5). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\n39\nPZ\nTZ\nmean\n2D\n0.6285\n0.8380\n0.7333\n3D_fullres\n0.6663\n0.8410\n0.7537\nBest Ensemble *\n0.6611\n0.8575\n0.7593\nPostprocessed\n0.6611\n0.8577\n0.7594\nTest set\n0.77\n0.90\n0.835\nTable F.10: Decathlon Prostate (D5) results. Note that all reported Dice scores (except the test set)\nwere computed using \ufb01ve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signi\ufb01cant digits are reported. Best ensemble on this dataset was the combination of the 2D\nU-Net and the 3D full resolution U-Net.\nLung (D6)\nNormalization: Clip to [\u22121024, 325], then subtract \u2212158.58 and \ufb01nally divide by 324.70.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.79 x 0.79\n1.24 x 0.79 x 0.79\n2.35 x 1.48 x 1.48\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n252 x 512 x 512\n133 x 271 x 271\nPatch size:\n512 x 512\n80 x 192 x 160\n80 x 192 x 160\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.11: Network con\ufb01gurations generated by nnU-Net for the Lung dataset from the Medical\nSegmentation Decathlon (D6). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\ncancer\nmean\n2D\n0.4989\n0.4989\n3D_fullres\n0.7211\n0.7211\n3D_lowres\n0.7109\n0.7109\n3D cascade\n0.6980\n0.6980\nBest Ensemble*\n0.7241\n0.7241\nPostprocessed\n0.7241\n0.7241\nTest set\n0.74\n0.74\nTable F.12: Decathlon Lung (D6) results. Note that all reported Dice scores (except the test set)\nwere computed using \ufb01ve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signi\ufb01cant digits are reported. Best ensemble on this dataset was the combination of the 3D low\nresolution U-Net and the 3D full resolution U-Net.\nPancreas (D7)\nNormalization: Clip to [\u221296.0, 215.0], then subtract 77.99 and \ufb01nally divide by 75.40.\n40\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.8 x 0.8\n2.5 x 0.8 x 0.8\n2.58 x 1.29 x 1.29\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n96 x 512 x 512\n93 x 318 x 318\nPatch size:\n512 x 512\n40 x 224 x 224\n64 x 192 x 192\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.13: Network con\ufb01gurations generated by nnU-Net for the Pancreas dataset from the Medical\nSegmentation Decathlon (D7). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\npancreas\ncancer\nmean\n2D\n0.7738\n0.3501\n0.5619\n3D_fullres\n0.8217\n0.5274\n0.6745\n3D_lowres\n0.8118\n0.5286\n0.6702\n3D cascade\n0.8101\n0.5380\n0.6741\nBest Ensemble *\n0.8214\n0.5428\n0.6821\nPostprocessed\n0.8214\n0.5428\n0.6821\nTest set\n0.82\n0.53\n0.675\nTable F.14: Decathlon Pancreas (D7) results. Note that all reported Dice scores (except the test set)\nwere computed using \ufb01ve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signi\ufb01cant digits are reported. Best ensemble on this dataset was the combination of the 3D full\nresolution U-Net and the 3D U-Net cascade.\nHepatic Vessel (D8)\nNormalization: Clip to [\u22123, 243], then subtract 104.37 and \ufb01nally divide by 52.62.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.8 x 0.8\n1.5 x 0.8 x 0.8\n2.42 x 1.29 x 1.29\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n150 x 512 x 512\n93 x 318 x 318\nPatch size:\n512 x 512\n64 x 192 x 192\n64 x 192 x 192\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.15: Network con\ufb01gurations generated by nnU-Net for the HepaticVessel dataset from the\nMedical Segmentation Decathlon (D8). For more information on how to decode downsampling\nstrides and kernel sizes into an architecture, see F.2\n41\nVessel\nTumour\nmean\n2D\n0.6180\n0.6359\n0.6269\n3D_fullres\n0.6456\n0.7217\n0.6837\n3D_lowres\n0.6294\n0.7079\n0.6687\n3D cascade\n0.6424\n0.7138\n0.6781\nBest Ensemble *\n0.6485\n0.7250\n0.6867\nPostprocessed\n0.6485\n0.7250\n0.6867\nTest set\n0.66\n0.72\n0.69\nTable F.16: Decathlon HepaticVessel (D8) results. Note that all reported Dice scores (except the test\nset) were computed using \ufb01ve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signi\ufb01cant digits are reported. Best ensemble on this dataset was the combination of the 3D full\nresolution U-Net and the 3D low resolution U-Net.\nSpleen (D9)\nNormalization: Clip to [\u221241, 176], then subtract 99.29 and \ufb01nally divide by 39.47.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.79 x 0.79\n1.6 x 0.79 x 0.79\n2.77 x 1.38 x 1.38\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n187 x 512 x 512\n108 x 293 x 293\nPatch size:\n512 x 512\n64 x 192 x 160\n64 x 192 x 192\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.17: Network con\ufb01gurations generated by nnU-Net for the Spleen dataset from the Medical\nSegmentation Decathlon (D9). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\nspleen\nmean\n2D\n0.9492\n0.9492\n3D_fullres\n0.9638\n0.9638\n3D_lowres\n0.9683\n0.9683\n3D cascade\n0.9714\n0.9714\nBest Ensemble *\n0.9723\n0.9723\nPostprocessed\n0.9724\n0.9724\nTest set\n0.97\n0.97\nTable F.18: Decathlon Spleen (D9) results. Note that all reported Dice scores (except the test set)\nwere computed using \ufb01ve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signi\ufb01cant digits are reported. Best ensemble on this dataset was the combination of the 3D\nU-Net cascade and the 3D full resolution U-Net.\nColon (D10)\nNormalization: Clip to [\u221230.0, 165.82], then subtract 62.18 and \ufb01nally divide by 32.65.\n42\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.78 x 0.78\n3 x 0.78 x 0.78\n3.09 x 1.55 x 1.55\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n150 x 512 x 512\n146 x 258 x 258\nPatch size:\n512 x 512\n56 x 192 x 160\n96 x 160 x 160\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.19: Network con\ufb01gurations generated by nnU-Net for the Colon dataset from the Medical\nSegmentation Decathlon (D10). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\ncolon cancer primaries\nmean\n2D\n0.2852\n0.2852\n3D_fullres\n0.4553\n0.4553\n3D_lowres\n0.4538\n0.4538\n3D cascade *\n0.4937\n0.4937\nBest Ensemble\n0.4853\n0.4853\nPostprocessed\n0.4937\n0.4937\nTest set\n0.58\n0.58\nTable F.20: Decathlon Colon (D10) results. Note that all reported Dice scores (except the test set)\nwere computed using \ufb01ve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signi\ufb01cant digits are reported. Best ensemble on this dataset was the combination of the 3D\nU-Net cascade and the 3D full resolution U-Net.\nF.4\nMulti Atlas Labeling Beyond the Cranial Vault: Abdomen (D11)\nChallenge summary\nThe Multi Atlas Labeling Beyond the Cranial Vault - Abdomen Challenge5\n[12] (denoted BCV for brevity) comprises 30 CT images for training and 20 for testing. The\nsegmentation target are thirteen different organs in the abdomen.\nApplication of nnU-Net to BCV\nnnU-Net was applied to the BCV challenge without any manual\nintervention.\nNormalization: Clip to [\u2212958, 327], then subtract 82.92 and \ufb01nally divide by 136.97.\n5https://www.synapse.org/Synapse:syn3193805/wiki/217752\n43\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.76 x 0.76\n3 x 0.76 x 0.76\n3.18 x 1.60 x 1.60\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n148 x 512 x 512\n140 x 243 x 243\nPatch size:\n512 x 512\n48 x 192 x 192\n80 x 160 x 160\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.21: Network con\ufb01gurations generated by nnU-Net for the BCV challenge (D131. For more\ninformation on how to decode downsampling strides and kernel sizes into an architecture, see F.2\n1\n2\n3\n4\n5\n6\n7\n8\n2D\n0.8860\n0.8131\n0.8357\n0.6406\n0.7724\n0.9453\n0.8405\n0.9128\n3D_fullres\n0.9083\n0.8939\n0.8675\n0.6632\n0.7840\n0.9557\n0.8816\n0.9229\n3D_lowres\n0.9132\n0.9045\n0.9132\n0.6525\n0.7810\n0.9554\n0.8903\n0.9209\n3D cascade\n0.9166\n0.9069\n0.9137\n0.7036\n0.7885\n0.9587\n0.9037\n0.9215\nBest Ensemble *\n0.9135\n0.9065\n0.8971\n0.6955\n0.7897\n0.9589\n0.9026\n0.9248\nPostprocessed\n0.9135\n0.9065\n0.8971\n0.6959\n0.7897\n0.9590\n0.9026\n0.9248\nTest set\n0.9721\n0.9182\n0.9578\n0.7528\n0.8411\n0.9769\n0.9220\n0.9290\n9\n10\n11\n12\n13\nmean\n2D\n0.8140\n0.7046\n0.7367\n0.6269\n0.5909\n0.7784\n3D_fullres\n0.8638\n0.7659\n0.8176\n0.7148\n0.7238\n0.8279\n3D_lowres\n0.8571\n0.7469\n0.8003\n0.6688\n0.6851\n0.8223\n3D cascade\n0.8621\n0.7722\n0.8210\n0.7205\n0.7214\n0.8393\nBest Ensemble *\n0.8673\n0.7746\n0.8299\n0.7218\n0.7287\n0.8393\nPostprocessed\n0.8673\n0.7746\n0.8299\n0.7262\n0.7290\n0.8397\nTest set\n0.8809\n0.8317\n0.8515\n0.7887\n0.7674\n0.8762\nTable F.22: Multi Atlas Labeling Beyond the Cranial Vault Abdomen (D11) results. Note that all\nreported Dice scores (except the test set) were computed using \ufb01ve fold cross-validation on the training\ncases. Postprocessing was applied to the model marked with *. This model (incl postprocessing) was\nused for test set predictions. Note that the Dice scores for the test set are computed with the online\nplatform. Best ensemble on this dataset was the combination of the 3D U-Net cascade and the 3D\nfull resolution U-Net.\nF.5\nPROMISE12 (D12)\nChallenge summary\nThe segmentation target of the PROMISE12 challenge [13] is the prostate in\nT2 MRI images. 50 training cases with prostate annotations are provided for training. There are 30\ntest cases which need to be segmented by the challenge participants and are subsequently evaluated\non an online platform6.\nApplication of nnU-Net to PROMISE12\nnnU-Net was applied to the PROMISE12 challenge\nwithout any manual intervention.\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n6https://promise12.grand-challenge.org/\n44\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.61 x 0.61\n2.2 x 0.61 x 0.61\n-\nMedian image shape at\ntarget spacing:\nNA x 327 x 327\n39 x 327 x 327\n-\nPatch size:\n384 x 384\n28 x 256 x 256\n-\nBatch size:\n22\n2\n-\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[1, 2, 2], [1, 2, 2]]\n-\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n-\nTable F.23: Network con\ufb01gurations generated by nnU-Net for the PROMISE12 challenge (D12). For\nmore information on how to decode downsampling strides and kernel sizes into an architecture, see\nF.2\nprostate\nmean\n2D\n0.8932\n0.8932\n3D_fullres\n0.8891\n0.8891\nBest Ensemble *\n0.9029\n0.9029\nPostprocessed\n0.9030\n0.9030\nTest set\n0.9194\n0.9194\nTable F.24: PROMISE12 (D12) results. Note that all reported Dice scores (except the test set) were\ncomputed using \ufb01ve fold cross-validation on the training cases. * marks the best performing model\nselected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see \"Test set\").\nNote that the scores for the test set are computed with the online platform. The evaluation score of our\ntest set submission is 89.6507. The test set Dice score reported in the table was computed from the de-\ntailed submission results (Detailed results available here https://promise12.grand-challenge.\norg/evaluation/results/89044a85-6c13-49f4-9742-dea65013e971/). Best ensemble on\nthis dataset was the combination of the 2D U-Net and the 3D full resolution U-Net.\nF.6\nThe Automatic Cardiac Diagnosis Challenge (ACDC) (D13)\nChallenge summary\nThe Automatic Cardiac Diagnosis Challenge7 [1] (ACDC) comprises 100\ntraining patients and 50 test patients. The target structures are the cavity of the right ventricle,\nthe myocardium of the left ventricle and the cavity of the left ventricle. All images are cine MRI\nsequences of which the enddiastolic (ED) and endsystolic (ES) time points of the cardiac cycle were\nto be segmented. With two time instances per patient, the effective number of training/test images is\n200/100.\nApplication of nnU-Net to ACDC\nSince two time instances of the same patient were provided,\nwe manually interfered with the split for the 5-fold cross-validation of our models to ensure mutual\nexclusivity of patients between folds. A part from that, nnU-Net was applied without manual\nintervention.\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n7https://acdc.creatis.insa-lyon.fr\n45\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 1.56 x 1.56\n5 x 1.56 x 1.56\n-\nMedian image shape at\ntarget spacing:\nNA x 237 x 208\n18 x 237 x 208\n-\nPatch size:\n256 x 224\n20 x 256 x 224\n-\nBatch size:\n58\n3\n-\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[1, 2, 2], [1, 2, 2]]\n-\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n-\nTable F.25: Network con\ufb01gurations generated by nnU-Net for the ACDC challenge (D13). For more\ninformation on how to decode downsampling strides and kernel sizes into an architecture, see F.2\nRV\nMLV\nLVC\nmean\n2D\n0.9053\n0.8991\n0.9433\n0.9159\n3D_fullres\n0.9059\n0.9022\n0.9458\n0.9179\nBest Ensemble *\n0.9145\n0.9059\n0.9479\n0.9227\nPostprocessed\n0.9145\n0.9059\n0.9479\n0.9228\nTest set\n0.9295\n0.9183\n0.9407\n0.9295\nTable F.26: ACDC results (D13). Note that all reported Dice scores (except the test set) were\ncomputed using \ufb01ve fold cross-validation on the training cases. * marks the best performing model\nselected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see \"Test set\").\nNote that the Dice scores for the test set are computed with the online platform. The online platform\nreports the Dice scores for enddiastolic and endsystolic time points separately. We averaged these\nvalues for a more condensed presentation. Best ensemble on this dataset was the combination of the\n2D U-Net and the 3D full resolution U-Net.\nF.7\nLiver and Liver Tumor Segmentation Challenge (LiTS) (D14)\nChallenge summary\nThe Liver and Liver Tumor Segmentation challenge [3] provides 131 training\nCT images with ground truth annotations for the liver and liver tumors. 70 test images are provided\nwithout annotations. The predicted segmentation masks of the test cases are evaluated using the LiTS\nonline platform8.\nApplication of nnU-Net to LiTS\nnnU-Net was applied to the LiTS challenge without any manual\nintervention.\nNormalization: Clip to [\u221217, 201], then subtract 99.40 and \ufb01nally divide by 39.39.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.77 x 0.77\n1 x 0.77 x 0.77\n2.47 x 1.90 x 1.90\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n482 x 512 x 512\n195 x 207 x 207\nPatch size:\n512 x 512\n128 x 128 x 128\n128 x 128 x 128\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.27: Network con\ufb01gurations generated by nnU-Net for the LiTS challenge (D14). For more\ninformation on how to decode downsampling strides and kernel sizes into an architecture, see F.2\n8https://competitions.codalab.org/competitions/17094\n46\nliver\ncancer\nmean\n2D\n0.9547\n0.5603\n0.7575\n3D_fullres\n0.9576\n0.6253\n0.7914\n3D_lowres\n0.9585\n0.6161\n0.7873\n3D cascade\n0.9609\n0.6294\n0.7951\nBest Ensemble*\n0.9618\n0.6539\n0.8078\nPostprocessed\n0.9631\n0.6543\n0.8087\nTest set\n0.9670\n0.7630\n0.8650\nTable F.28: LiTS results (D14). Note that all reported Dice scores (except the test set) were computed\nusing \ufb01ve fold cross-validation on the training cases. * marks the best performing model selected for\nsubsequent postprocessing (see \"Postprocessed\") and test set submission (see \"Test set\"). Note that\nthe Dice scores for the test set are computed with the online platform. Best ensemble on this dataset\nwas the combination of the 3D low resolution U-Net and the 3D full resolution U-Net.\nF.8\nLongitudinal multiple sclerosis lesion segmentation challenge (MSLesion) (D15)\nChallenge summary\nThe longitudinal multiple sclerosis lesion segmentation challenge [4] pro-\nvides 5 training patients. For each patient, 4 to 5 images acquired at different time points are provided\n(4 patients with 4 time points each and one patient with 5 time points for a total of 21 images).\nEach time point is annotated by two different experts, resulting in 42 training annotations (on 21\nimages). The test set contains 14 patients, again with several time points each, for a total of 61 MRI\nacquisitions. Test set predictions are evaluated using the online platform9. Each train and test image\nconsists of four MRI modalities: MPRAGE, FLAIR, Proton Density, T2.\nApplication of nnU-Net to MSLesion\nWe manually interfere with the splits in the cross-validation\nto ensure mutual exclusivity of patients between folds. Each image was annotated by two different\nexperts. We treat these annotations as separate training images (of the same patient), resulting in a\ntraining set size of 2 \u00d7 21 = 42. We do not use the longitudinal nature of the scans and treat each\nimage individually during training and inference.\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 1 x 1\n1 x 1 x 1\n-\nMedian image shape at\ntarget spacing:\nNA x 180 x 137\n137 x 180 x 137\n-\nPatch size:\n192 x 160\n112 x 128 x 96\n-\nBatch size:\n107\n2\n-\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2]]\n[[1, 2, 1], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\n-\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n-\nTable F.29: Network con\ufb01gurations generated by nnU-Net for the MSLesion challenge (D15). For\nmore information on how to decode downsampling strides and kernel sizes into an architecture, see\nF.2\n9https://smart-stats-tools.org/lesion-challenge\n47\nlesion\nmean\n2D\n0.7339\n0.7339\n3D_fullres *\n0.7531\n0.7531\nBest Ensemble\n0.7494\n0.7494\nPostprocessed\n0.7531\n0.7531\nTest set\n0.6785\n0.6785\nTable F.30: MSLesion results (D15). Note that all reported Dice scores (except the test set) were\ncomputed using \ufb01ve fold cross-validation on the training cases. * marks the best performing model\nselected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see \"Test\nset\"). Note that the Dice scores for the test set are computed with the online platform based on the\ndetailed results (which are available here https://smart-stats-tools.org/sites/lesion_\nchallenge/temp/top25/nnUNetV2_12032019_0903.csv). The ranking is based on a score,\nwhich includes other metrics as well (see [4] for details). The score of our submission is 92.874. Best\nensemble on this dataset was the combination of the 2D U-Net and the 3D full resolution U-Net.\nF.9\nCombined Healthy Abdominal Organ Segmentation (CHAOS) (D16)\nChallenge summary\nThe CHAOS challenge [11] is divided into \ufb01ve tasks. Here we focused on\nTasks 3 (MRI Liver segmentation) and Task 5 (MRI multiorgan segmentation). Tasks 1, 2 and 4 also\nincluded the use of CT images, a modality for which plenty of public data is available (see e.g. BCV\nand LiTS challenge). To isolate the algorithmic performance of nnU-Net relative to other participants\nwe decided to only use the tasks for which a contamination with external data was unlikely. The target\nstructures of Task 5 are the liver, the spleen and the left and right kidneys. The CHAOS challenge\nprovides 20 training cases. For each training case, there is a T2 images with a corresponding ground\ntruth annotation as well as a T1 acquisition with its own, separate ground truth annotation. The T1\nacquisition has two modalities which are co-registered: T1 in-phase and T1 out-phase. Task 3 is a\nsubset of Task 5 with only the liver being the segmentation target. The 20 test cases are evaluated\nusing the online platform10.\nApplication of nnU-Net to CHAOS\nnnU-Net only supports images with a constant number of\ninput modalities. The training cases in CHAOS have either one (T2) or two (T1 in & out phase)\nmodalities. To ensure compatibility with nnU-Net we could have either duplicated the T2 image and\ntrained with two input modalities or use only one input modality and treat T1 in phase and out phase\nas separate training examples. We opted for the latter because this variant results in more (albeit\nhighly correlated) training images. With 20 training patients being provided, this approach resulted\nin 60 training images. For the cross-validation we ensure that the split is being done on patient level.\nDuring inference, nnU-Net will generate two separate predictions for T1 in and out phase which\nneed to be consolidated for test set evaluation. We achieve this by simply averaging the softmax\nprobabilities between the two to generate the \ufb01nal segmentation. We train nnU-Net only for Task 5.\nBecause task 3 represents a subset of Task 5, we extract the liver from our Task 5 predictions and\nsubmit it to Task 3.\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n10https://chaos.grand-challenge.org/\n48\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 1.66 x 1.66\n5.95 x 1.66 x 1.66\n-\nMedian image shape at\ntarget spacing:\nNA x 195 x 262\n45 x 195 x 262\n-\nPatch size:\n224 x 320\n40 x 192 x 256\n-\nBatch size:\n45\n2\n-\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [1, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2], [1, 1, 2]]\n-\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3]]\n-\nTable F.31: Network con\ufb01gurations generated by nnU-Net for the CHAOS challenge (D16). For more\ninformation on how to decode downsampling strides and kernel sizes into an architecture, see F.2.\nliver\nright kidney\nleft kidney\nspleen\nmean\n2D\n0.9132\n0.8991\n0.8897\n0.8720\n0.8935\n3D_fullres\n0.9202\n0.9274\n0.9209\n0.8938\n0.9156\nBest Ensemble *\n0.9184\n0.9283\n0.9255\n0.8911\n0.9158\nPostprocessed\n0.9345\n0.9289\n0.9212\n0.894\n0.9197\nTest set\n-\n-\n-\n-\n-\nTable F.32: CHAOS results (D16). Note that all reported Dice scores (except the test set) were\ncomputed using \ufb01ve fold cross-validation on the training cases. Postprocessing was applied to the\nmodel marked with *. This model (incl postprocessing) was used for test set predictions. Note that\nthe evaluation of the test set was performed with the online platform of the challenge which does not\nreport Dice scores for the individual organs. The score of our submission was 72.44 for Task 5 and\n75.10 for Task3 (see [11] for details). Best ensemble on this dataset was the combination of the 2D\nU-Net and the 3D full resolution U-Net.\nF.10\nKidney and Kidney Tumor Segmentation (KiTS) (D17)\nChallenge summary\nThe Kidney and Kidney Tumor Segmentation challenge [8] was the largest\ncompetition (in terms of number of participants) at MICCAI 2019. The target structures are the\nkidneys and kidney tumors. 210 training and 90 test cases are provided by the challenge organizers.\nThe organizers provide the data both in their original geometry (with voxel spacing varying between\ncases) as well as interpolated to a common voxel spacing. Evaluation of the test set predictions is\ndone on the online platform11.\nWe participated in the original KiTS 2019 MICCAI challenge with a manually designed\nresidual 3D U-Net. This algorithm, described in [9] obtained the \ufb01rst rank in the challenge. For this\nsubmission, we did slight modi\ufb01cations to the original training data: Cases 15 and 37 were con\ufb01rmed\nto be faulty by the challenge organizers (https://github.com/neheller/kits19/issues/21)\nwhich is why we replaced their respective segmentation masks with predictions of one of our\nnetworks. We furthermore excluded cases 23, 68, 125 and 133 because we suspected labeling\nerrors in these cases as well. At the time of conducting the experiments for this publication, no\nrevised segmentation masks were provided by the challenge organizers, which is why we re-used the\nmodi\ufb01ed training dataset for training nnU-Net.\nAfter the challenge event at MICCAI 2019, an open leaderboard was created.\nThe original\n11https://kits19.grand-challenge.org/\n49\nchallenge leaderboard is retained at http://results.kits-challenge.org/miccai2019/. All\nsubmissions of the original KiTS challenge were mirrored to the open leaderboard. The submission\nof nnU-Net as performed in the context of this manuscript is done on the open leaderboard, where\nmany more competitors have entered since the challenge. As presented in Figure 3, nnU-Net sets a\nnew state of the art on the open leaderboard, thus also outperforming our initial, manually optimized\nsolution.\nApplication of nnU-Net to KiTS\nSince nnU-Net is designed to automatically deal with varying\nvoxel spacings within a dataset, we chose the original, non-interpolated image data as provided by\nthe organizers and let nnU-Net deal with the homogenization of voxel spacing. nnU-Net was applied\nto the KiTS challenge without any manual intervention.\nNormalization: Clip to [\u221279, 304], then subtract 100.93 and \ufb01nally divide by 76.90.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.78 x 0.78\n0.78 x 0.78 x 0.78\n1.99 x 1.99 x 1.99\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n525 x 512 x 512\n206 x 201 x 201\nPatch size:\n512 x 512\n128 x 128 x 128\n128 x 128 x 128\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.33: Network con\ufb01gurations generated by nnU-Net for the KiTS challenge (D17). For more\ninformation on how to decode downsampling strides and kernel sizes into an architecture, see F.2\nKidney\nTumor\nmean\n2D\n0.9613\n0.7563\n0.8588\n3D_fullres\n0.9702\n0.8367\n0.9035\n3D_lowres\n0.9629\n0.8420\n0.9025\n3D cascade\n0.9702\n0.8546\n0.9124\nBest Ensemble*\n0.9707\n0.8620\n0.9163\nPostprocessed\n0.9707\n0.8620\n0.9163\nTest set\n-\n0.8542\n-\nTable F.34: KiTS results (D17). Note that all reported Dice scores (except the test set) were computed\nusing \ufb01ve fold cross-validation on the training cases. Postprocessing was applied to the model marked\nwith *. This model (incl postprocessing) was used for test set predictions. Note that the Dice scores\nfor the test set are computed with the online platform which computes the kidney Dice score based of\nthe union of the kidney and tumor labels whereas nnU-Net always evaluates labels independently,\nresulting in a missing value for kindey in the table. The reported kindey Dice by the platform (which\nis not comparable with the value computed by nnU-Net) is 0.9793. Best ensemble on this dataset was\nthe combination of the 3D U-Net cascade and the 3D full resolution U-Net.\nF.11\nSegmentation of THoracic Organs at Risk in CT images (SegTHOR) (D18)\nChallenge summary\nIn the Segmentation of THoracic Organs at Risk in CT images [16] challenge,\nfour abdominal organs (the heart, the aorta, the trachea and the esopahgus) are to be segmented in CT\n50\nimages. 40 training images are provided for training and another 20 images are provided for testing.\nEvaluation of the test images is done using the online platform12.\nApplication of nnU-Net to SegTHOR\nnnU-Net was applied to the SegTHOR challenge without\nany manual intervention.\nNormalization: Clip to [\u2212986, 271], then subtract 20.78 and \ufb01nally divide by 180.50.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.89 x 0.89\n2.50 x 0.89 x 0.89\n3.51 x 1.76 x 1.76\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n171 x 512 x 512\n122 x 285 x 285\nPatch size:\n512 x 512\n64 x 192 x 160\n80 x 192 x 160\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2], [2, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.35: Network con\ufb01gurations generated by nnU-Net for the SegTHOR challenge (D18). For\nmore information on how to decode downsampling strides and kernel sizes into an architecture, see\nF.2\nesophagus\nheart\ntrachea\naorta\nmean\n2D\n0.8181\n0.9407\n0.9077\n0.9277\n0.8986\n3D_fullres\n0.8495\n0.9527\n0.9055\n0.9426\n0.9126\n3D_lowres\n0.8110\n0.9464\n0.8930\n0.9284\n0.8947\n3D cascade\n0.8553\n0.9520\n0.9045\n0.9403\n0.9130\nBest Ensemble*\n0.8545\n0.9532\n0.9066\n0.9427\n0.9143\nPostprocessed\n0.8545\n0.9532\n0.9083\n0.9438\n0.9150\nTest set\n0.8890\n0.9570\n0.9228\n0.9510\n0.9300\nTable F.36: SegTHOR results (D18). Note that all reported Dice scores (except the test set) were\ncomputed using \ufb01ve fold cross-validation on the training cases. Postprocessing was applied to the\nmodel marked with *. This model (incl postprocessing) was used for test set predictions. Note that\nthe Dice scores for the test set are computed with the online platform. Best ensemble on this dataset\nwas the combination of the 3D U-Net cascade and the 3D full resolution U-Net.\nF.12\nChallenge on Circuit Reconstruction from Electron Microscopy Images (CREMI)\n(D19)\nChallenge summary\nThe Challenge on Circuit Reconstruction from Electron Microscopy Images\nis subdivided into three tasks. The synaptic cleft segmentation task can be formulated as semantic\nsegmentation (as opposed to e.g. instance segmentation) and is thus compatible with nnU-Net. In this\ntask, the segmentation target is the cell membrane in locations where the cells are forming a synapse.\nThe dataset consists of serial section Transmission Electron Microscopy scans of the Drosophila\nmelanogaster brain. Three volumes are provided for training and another three are provided for\ntesting. Test set evaluation is done using the online platform13.\nApplication of nnU-Net to CREMI\nSince to the number of training images is lower than the\nnumber of splits, we cannot run a 5-fold cross-validation. Thus, we trained 5 model instances,\n12https://competitions.codalab.org/competitions/21145\n13https://cremi.org/\n51\neach of them on all three training volumes and subsequently ensembled these models for test set\nprediction. Because this training scheme leaves no validation data, selection of the best of three\nmodel con\ufb01gurations as performed by nnU-Net after cross-validation was not possible. Hence, we\nintervened by only con\ufb01guring and training the 3D full resolution con\ufb01guration.\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\n-\n40 x 4 x 4\n-\nMedian image shape at\ntarget spacing:\n-\n125 x 1250 x 1250\n-\nPatch size:\n-\n24 x 256 x256\n-\nBatch size:\n-\n2\n-\nDownsampling strides:\n-\n[[1, 2, 2], [1, 2, 2], [1, 2, 2],\n[2, 2, 2], [2, 2, 2], [1, 2, 2]]\n-\nConvolution kernel sizes:\n-\n[[1, 3, 3], [1, 3, 3], [1, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3]]\n-\nTable F.37: Network con\ufb01gurations generated by nnU-Net for the CREMI challenge (D19). For more\ninformation on how to decode downsampling strides and kernel sizes into an architecture, see F.2\nResults\nBecause our training scheme for this challenge left no validation data, a performance\nestimate as given for the other datastes is not available for CREMI. The CREMI test set is evaluated\nby the online platform. The evaluation metric is the so called CREMI score, a description of which is\navailable here https://cremi.org/metrics/. Dice scores for the test set are not reported. The\nCREMI score of our test set submission was 74.96 (lower is better).\nG\nUsing nnU-Net with limited compute resources\nReduction of computational complexity was one of the key motivations driving the design of nnU-Net.\nThe effort of running all the con\ufb01gurations generated by nnU-Net should be manageable for most\nusers and researchers. There are, however, some shortcuts that can be be taken in case computational\nresources are extremely scarce.\nG.1\nReducing the number of network trainings\nDepending on whether the 3D U-Net cascade is con\ufb01gured for a given dataset, nnU-Net requires 10\n(2D and 3D U-Net with 5 models each) or 20 (2d, 3D, 3D cascade (low resolution and high resolution\nU-Net) with 5 models each) U-Net trainings to run, each of which takes a couple of days on a single\nGPU. While this approach guarantees the best possible performance, training all models may exceed\nreasonable computation time if only a single GPU is available. Therefore, we present two strategies\nto reduce the number of total network trainings when running nnU-Net.\nManual selection of U-Net con\ufb01gurations\nOverall, the 3D full resolution U-Net shows the best segmentation results. Thus, this con\ufb01guration is\na good starting point and could simply be selected as default choice. Users can decide whether to train\nthis con\ufb01guration using all training cases (to train a single model) or run a \ufb01ve-fold cross-validation\nand ensemble the 5 resulting models for test case predictions.\nIn some scenarios, other con\ufb01gurations than the 3D full resolution U-Net can yield best\n52\nperformance. Identifying such scenarios and selecting the respective most promising con\ufb01guration,\nhowever, requires domain knowledge for the dataset at hand. Datasets with highly anisotropic images\n(such as D12 PROMISE12), for instance, could be best suited for running a 2D U-Net. There is,\nhowever, no guarantee for this relation (see D13 ACDC). On datasets with very large images, the\n3D U-Net cascade seems to marginally outperform the 3D full resolution U-Net (for example D11,\nD14, D17, D18, ...) because it improves the capture of contextual information. Note that this is only\ntrue if the target structure requires a large receptive \ufb01eld for optimal recognition. On CREMI (D19)\nfor example, despite large image sizes, only a limited \ufb01eld of view is required, because the target\nstructure are relatively small synapses that can be identi\ufb01ed using only local information, which is\nwhy we selected the 3D full resolution U-Net for this dataset (see Section F.12).\nNot running all con\ufb01gurations as 5-fold cross-validation\nAnother computation shortcut is to not run all models as 5-fold cross-validation. For instance, only\none split for each con\ufb01guration can be run (note, however, that the 3D low resolution U-Net of\nthe cascade is required to be run as a 5-fold cross-validation in order to generate low resolution\nsegmentation maps of all training cases for the second full resolution U-net of the cascade). Even\nwhen running multiple con\ufb01gurations to rely on empirical selection of con\ufb01gurations by nnU-Net,\nthis reduces the total number of models to be trained to 2 if no cascade is con\ufb01gured or 8 if the\ncascade is con\ufb01gured (the cascade requires 6 model trainings: 5 3D low resolution U-Nets and 1 full\nresolution 3D U-Net training). nnU-Net subsequently bases selection of the best con\ufb01guration on this\nsingle train-val split. Note that this strategy provides less reliable performance estimates and may\nresult in sub optimal con\ufb01guration choices. Finally, users can decide whether they wish to re-train the\nselected con\ufb01guration on the entire training data or run a \ufb01ve-fold cross-validation for this selected\ncon\ufb01guration. The latter is expected to result in better test set performance because the 5 models can\nbe used as an ensemble.\nG.2\nReduction of GPU memory\nnnU-Net is con\ufb01gured to utilize 11GB of GPU memory. This requirement is, based on our experience,\na realistic requirement for a modern deep-learning capable GPU (such as a Nvidia GTX 1080 ti\n(11GB), Nvidia RTX 2080 ti (11GB), Nvidia TitanX(p) (12GB), Nvidia P100 (12/16 GB), Nvidia\nTitan RTX (24GB), Nvidia V100 (16/32 GB), ...). We strongly recommend using nnU-Net with\nthis default con\ufb01guration, because it has been tested extensively and, as we show in this manuscript,\nprovides excellent segmentation accuracy. Should users still desire to run nnU-Net on a smaller GPU,\nthe amount of GPU memory used for network con\ufb01guration can be adapted easily. Corresponding\ninstructions are provided along with the source code.\nReferences\n[1] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A. Heng, I. Cetin, K. Lekadir,\nO. Camara, M. A. G. Ballester, et al. Deep learning techniques for automatic mri cardiac multi-\nstructures segmentation and diagnosis: Is the problem solved? IEEE TMI, 37(11):2514\u20132525,\n2018.\n[2] P. Bilic, P. F. Christ, E. Vorontsov, G. Chlebus, H. Chen, Q. Dou, C.-W. Fu, X. Han, P.-\nA. Heng, J. Hesser, et al. The liver tumor segmentation benchmark (lits). arXiv preprint\narXiv:1901.04056ada, 2019.\n53\n[3] P. Bilic, P. F. Christ, E. Vorontsov, G. Chlebus, H. Chen, Q. Dou, C.-W. Fu, X. Han, P.-A. Heng,\nJ. Hesser, et al. The liver tumor segmentation benchmark (lits). arXiv preprint arXiv:1901.04056,\n2019.\n[4] A. Carass, S. Roy, A. Jog, J. L. Cuzzocreo, E. Magrath, A. Gherman, J. Button, J. Nguyen,\nF. Prados, C. H. Sudre, et al. Longitudinal multiple sclerosis lesion segmentation: resource and\nchallenge. NeuroImage, 148:77\u2013102, 2017.\n[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic\nimage segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.\nIEEE transactions on pattern analysis and machine intelligence, 40(4):834\u2013848, 2017.\n[6] L. Heinrich, J. Funke, C. Pape, J. Nunez-Iglesias, and S. Saalfeld. Synaptic cleft segmentation\nin non-isotropic volume electron microscopy of the complete drosophila brain. In International\nConference on Medical Image Computing and Computer-Assisted Intervention, pages 317\u2013325.\nSpringer, 2018.\n[7] N. Heller, F. Isensee, K. H. Maier-Hein, X. Hou, C. Xie, F. Li, Y. Nan, G. Mu, Z. Lin, M. Han,\net al. The state of the art in kidney and kidney tumor segmentation in contrast-enhanced ct\nimaging: Results of the kits19 challenge. arXiv preprint arXiv:1912.01054, 2019.\n[8] N. Heller, N. Sathianathen, A. Kalapara, E. Walczak, K. Moore, H. Kaluzniak, J. Rosen-\nberg, P. Blake, Z. Rengel, M. Oestreich, et al. The kits19 challenge data: 300 kidney tumor\ncases with clinical context, ct semantic segmentations, and surgical outcomes. arXiv preprint\narXiv:1904.00445, 2019.\n[9] F. Isensee and K. H. Maier-Hein.\nAn attempt at beating the 3d u-net.\narXiv preprint\narXiv:1908.02182, 2019.\n[10] F. Isensee, J. Petersen, A. Klein, D. Zimmerer, P. F. Jaeger, S. Kohl, J. Wasserthal, G. Koehler,\nT. Norajitra, S. Wirkert, et al. nnu-net: Self-adapting framework for u-net-based medical image\nsegmentation. arXiv preprint arXiv:1809.10486, 2018.\n[11] A. E. Kavur, N. S. Gezer, M. Bar\u0131\u00b8s, P.-H. Conze, V. Groza, D. D. Pham, S. Chatterjee, P. Ernst,\nS. \u00d6zkan, B. Baydar, et al. Chaos challenge\u2013combined (ct-mr) healthy abdominal organ\nsegmentation. arXiv preprint arXiv:2001.06535, 2020.\n[12] B. Landman, Z. Xu, J. Eugenio Igelsias, M. Styner, T. Langerak, and A. Klein. Miccai\nmulti-atlas labeling beyond the cranial vault\u2013workshop and challenge, 2015.\n[13] G. Litjens, R. Toth, W. van de Ven, C. Hoeks, S. Kerkstra, B. van Ginneken, G. Vincent,\nG. Guillard, N. Birbeck, J. Zhang, et al. Evaluation of prostate segmentation algorithms for mri:\nthe promise12 challenge. Med Image Analysis, 18(2):359\u2013373, 2014.\n[14] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, Y. Burren, N. Porz,\nJ. Slotboom, R. Wiest, et al. The multimodal brain tumor image segmentation benchmark\n(brats). IEEE transactions on medical imaging, 34(10):1993\u20132024, 2014.\n[15] A. L. Simpson, M. Antonelli, S. Bakas, M. Bilello, K. Farahani, B. van Ginneken, A. Kopp-\nSchneider, B. A. Landman, G. Litjens, B. Menze, et al. A large annotated medical image dataset\nfor the development and evaluation of segmentation algorithmsdelldatagrowth. arXiv preprint\narXiv:1902.09063, 2019.\n54\n[16] R. Trullo, C. Petitjean, B. Dubray, and S. Ruan. Multiorgan segmentation using distance-aware\nadversarial networks. Journal of Medical Imaging, 6(1):014001, 2019.\n[17] Q. Yu, D. Yang, H. Roth, Y. Bai, Y. Zhang, A. L. Yuille, and D. Xu. C2fnas: Coarse-to-\ufb01ne\nneural architecture search for 3d medical image segmentation. arXiv preprint arXiv:1912.09628,\n2019.\n55\n"
    },
    {
        "pdf_file": "paper3.pdf",
        "text": "403 Forbidden\n\u2022  Code: AccessDenied\n\u2022  Message: Access Denied\n\u2022  RequestId: 1BY7NQRZ6QW92NE8\n\u2022  HostId: A3U5k7XxAkhWhPp0HT6BVNY6iLuiV/\nmbOA1AqysSbCOccSSEXeUKfT9ggoeiskJw6JqwbDxXHogjjFkILq9a+\n"
    },
    {
        "pdf_file": "paper5.pdf",
        "text": "A preliminary version of this paper appears in the proceedings of the 23rd ACM Conference on Computer and Communications Security\n(CCS 2016). This is a full version.\nDeep Learning with Differential Privacy\nOctober 25, 2016\nMart\u00edn Abadi\u2217\nAndy Chu\u2217\nIan Goodfellow\u2020\nH. Brendan McMahan\u2217\nIlya Mironov\u2217\nKunal Talwar\u2217\nLi Zhang\u2217\nABSTRACT\nMachine learning techniques based on neural networks are\nachieving remarkable results in a wide variety of domains.\nOften, the training of models requires large, representative\ndatasets, which may be crowdsourced and contain sensitive\ninformation. The models should not expose private informa-\ntion in these datasets. Addressing this goal, we develop new\nalgorithmic techniques for learning and a re\ufb01ned analysis of\nprivacy costs within the framework of di\ufb00erential privacy.\nOur implementation and experiments demonstrate that we\ncan train deep neural networks with non-convex objectives,\nunder a modest privacy budget, and at a manageable cost in\nsoftware complexity, training e\ufb03ciency, and model quality.\n1.\nINTRODUCTION\nRecent progress in neural networks has led to impressive\nsuccesses in a wide range of applications, including image\nclassi\ufb01cation, language representation, move selection for\nGo, and many more (e.g., [54, 28, 56, 38, 51]). These ad-\nvances are enabled, in part, by the availability of large and\nrepresentative datasets for training neural networks. These\ndatasets are often crowdsourced, and may contain sensitive\ninformation.\nTheir use requires techniques that meet the\ndemands of the applications while o\ufb00ering principled and\nrigorous privacy guarantees.\nIn this paper, we combine state-of-the-art machine learn-\ning methods with advanced privacy-preserving mechanisms,\ntraining neural networks within a modest (\u201csingle-digit\u201d) pri-\nvacy budget. We treat models with non-convex objectives,\nseveral layers, and tens of thousands to millions of param-\neters. (In contrast, previous work obtains strong results on\nconvex models with smaller numbers of parameters, or treats\ncomplex neural networks but with a large privacy loss.) For\nthis purpose, we develop new algorithmic techniques, a re-\n\ufb01ned analysis of privacy costs within the framework of dif-\nferential privacy, and careful implementation strategies:\n\u2217Google.\n\u2020OpenAI. Work done while at Google.\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor pro\ufb01t or commercial advantage and that copies bear this notice and the full citation\non the \ufb01rst page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nCCS\u201916 October 24-28, 2016, Vienna, Austria\nc\u20dd2016 Copyright held by the owner/author(s).\nACM ISBN 978-1-4503-4139-4/16/10.\nDOI: http://dx.doi.org/10.1145/2976749.2978318\n1. We demonstrate that, by tracking detailed information\n(higher moments) of the privacy loss, we can obtain\nmuch tighter estimates on the overall privacy loss, both\nasymptotically and empirically.\n2. We improve the computational e\ufb03ciency of di\ufb00eren-\ntially private training by introducing new techniques.\nThese techniques include e\ufb03cient algorithms for com-\nputing gradients for individual training examples, sub-\ndividing tasks into smaller batches to reduce memory\nfootprint, and applying di\ufb00erentially private principal\nprojection at the input layer.\n3. We build on the machine learning framework Tensor-\nFlow [3] for training models with di\ufb00erential privacy.\nWe evaluate our approach on two standard image clas-\nsi\ufb01cation tasks, MNIST and CIFAR-10.\nWe chose\nthese two tasks because they are based on public data-\nsets and have a long record of serving as benchmarks\nin machine learning.\nOur experience indicates that\nprivacy protection for deep neural networks can be\nachieved at a modest cost in software complexity, train-\ning e\ufb03ciency, and model quality.\nMachine learning systems often comprise elements that\ncontribute to protecting their training data. In particular,\nregularization techniques, which aim to avoid over\ufb01tting to\nthe examples used for training, may hide details of those\nexamples. On the other hand, explaining the internal rep-\nresentations in deep neural networks is notoriously di\ufb03cult,\nand their large capacity entails that these representations\nmay potentially encode \ufb01ne details of at least some of the\ntraining data. In some cases, a determined adversary may\nbe able to extract parts of the training data. For example,\nFredrikson et al. demonstrated a model-inversion attack that\nrecovers images from a facial recognition system [24].\nWhile the model-inversion attack requires only \u201cblack-\nbox\u201d access to a trained model (that is, interaction with the\nmodel via inputs and outputs), we consider adversaries with\nadditional capabilities, much like Shokri and Shmatikov [50].\nOur approach o\ufb00ers protection against a strong adversary\nwith full knowledge of the training mechanism and access\nto the model\u2019s parameters.\nThis protection is attractive,\nin particular, for applications of machine learning on mobile\nphones, tablets, and other devices. Storing models on-device\nenables power-e\ufb03cient, low-latency inference, and may con-\ntribute to privacy since inference does not require commu-\nnicating user data to a central server; on the other hand,\nwe must assume that the model parameters themselves may\nbe exposed to hostile inspection. Furthermore, when we are\narXiv:1607.00133v2  [stat.ML]  24 Oct 2016\nconcerned with preserving the privacy of one record in the\ntraining data, we allow for the possibility that the adversary\ncontrols some or even all of the rest of the training data. In\npractice, this possibility cannot always be excluded, for ex-\nample when the data is crowdsourced.\nThe next section reviews background on deep learning and\non di\ufb00erential privacy.\nSections 3 and 4 explain our ap-\nproach and implementation. Section 5 describes our exper-\nimental results. Section 6 discusses related work, and Sec-\ntion 7 concludes. Deferred proofs appear in the Appendix.\n2.\nBACKGROUND\nIn this section we brie\ufb02y recall the de\ufb01nition of di\ufb00erential\nprivacy, introduce the Gaussian mechanism and composition\ntheorems, and overview basic principles of deep learning.\n2.1\nDifferential Privacy\nDi\ufb00erential privacy [19, 16, 20] constitutes a strong stan-\ndard for privacy guarantees for algorithms on aggregate data-\nbases. It is de\ufb01ned in terms of the application-speci\ufb01c con-\ncept of adjacent databases. In our experiments, for instance,\neach training dataset is a set of image-label pairs; we say\nthat two of these sets are adjacent if they di\ufb00er in a single\nentry, that is, if one image-label pair is present in one set\nand absent in the other.\nDe\ufb01nition 1. A randomized mechanism M: D \u2192R with\ndomain D and range R satis\ufb01es (\u03b5, \u03b4)-di\ufb00erential privacy if\nfor any two adjacent inputs d, d\u2032 \u2208D and for any subset of\noutputs S \u2286R it holds that\nPr[M(d) \u2208S] \u2264e\u03b5 Pr[M(d\u2032) \u2208S] + \u03b4.\nThe original de\ufb01nition of \u03b5-di\ufb00erential privacy does not in-\nclude the additive term \u03b4. We use the variant introduced by\nDwork et al. [17], which allows for the possibility that plain\n\u03b5-di\ufb00erential privacy is broken with probability \u03b4 (which is\npreferably smaller than 1/|d|).\nDi\ufb00erential privacy has several properties that make it\nparticularly useful in applications such as ours: composabil-\nity, group privacy, and robustness to auxiliary information.\nComposability enables modular design of mechanisms: if all\nthe components of a mechanism are di\ufb00erentially private,\nthen so is their composition. Group privacy implies graceful\ndegradation of privacy guarantees if datasets contain cor-\nrelated inputs, such as the ones contributed by the same\nindividual. Robustness to auxiliary information means that\nprivacy guarantees are not a\ufb00ected by any side information\navailable to the adversary.\nA common paradigm for approximating a deterministic\nreal-valued function f : D \u2192R with a di\ufb00erentially private\nmechanism is via additive noise calibrated to f\u2019s sensitivity\nSf, which is de\ufb01ned as the maximum of the absolute distance\n|f(d) \u2212f(d\u2032)| where d and d\u2032 are adjacent inputs.\n(The\nrestriction to a real-valued function is intended to simplify\nthis review, but is not essential.) For instance, the Gaussian\nnoise mechanism is de\ufb01ned by\nM(d)\n\u2206= f(d) + N(0, S2\nf \u00b7 \u03c32),\nwhere N(0, S2\nf \u00b7 \u03c32) is the normal (Gaussian) distribution\nwith mean 0 and standard deviation Sf\u03c3. A single applica-\ntion of the Gaussian mechanism to function f of sensitivity\nSf satis\ufb01es (\u03b5, \u03b4)-di\ufb00erential privacy if \u03b4 \u22654\n5 exp(\u2212(\u03c3\u03b5)2/2)\nand \u03b5 < 1 [20, Theorem 3.22]. Note that this analysis of\nthe mechanism can be applied post hoc, and, in particular,\nthat there are in\ufb01nitely many (\u03b5, \u03b4) pairs that satisfy this\ncondition.\nDi\ufb00erential privacy for repeated applications of additive-\nnoise mechanisms follows from the basic composition theo-\nrem [17, 18], or from advanced composition theorems and\ntheir re\ufb01nements [22, 32, 21, 10]. The task of keeping track\nof the accumulated privacy loss in the course of execution\nof a composite mechanism, and enforcing the applicable pri-\nvacy policy, can be performed by the privacy accountant,\nintroduced by McSherry [40].\nThe basic blueprint for designing a di\ufb00erentially private\nadditive-noise mechanism that implements a given function-\nality consists of the following steps: approximating the func-\ntionality by a sequential composition of bounded-sensitivity\nfunctions; choosing parameters of additive noise; and per-\nforming privacy analysis of the resulting mechanism.\nWe\nfollow this approach in Section 3.\n2.2\nDeep Learning\nDeep neural networks, which are remarkably e\ufb00ective for\nmany machine learning tasks, de\ufb01ne parameterized func-\ntions from inputs to outputs as compositions of many layers\nof basic building blocks, such as a\ufb03ne transformations and\nsimple nonlinear functions. Commonly used examples of the\nlatter are sigmoids and recti\ufb01ed linear units (ReLUs). By\nvarying parameters of these blocks, we can\u201ctrain\u201dsuch a pa-\nrameterized function with the goal of \ufb01tting any given \ufb01nite\nset of input/output examples.\nMore precisely, we de\ufb01ne a loss function L that represents\nthe penalty for mismatching the training data. The loss L(\u03b8)\non parameters \u03b8 is the average of the loss over the training\nexamples {x1, . . . , xN}, so L(\u03b8) =\n1\nN\nP\ni L(\u03b8, xi). Training\nconsists in \ufb01nding \u03b8 that yields an acceptably small loss,\nhopefully the smallest loss (though in practice we seldom\nexpect to reach an exact global minimum).\nFor complex networks, the loss function L is usually non-\nconvex and di\ufb03cult to minimize. In practice, the minimiza-\ntion is often done by the mini-batch stochastic gradient de-\nscent (SGD) algorithm.\nIn this algorithm, at each step,\none forms a batch B of random examples and computes\ngB = 1/|B| P\nx\u2208B \u2207\u03b8L(\u03b8, x) as an estimation to the gra-\ndient \u2207\u03b8L(\u03b8).\nThen \u03b8 is updated following the gradient\ndirection \u2212gB towards a local minimum.\nSeveral systems have been built to support the de\ufb01nition\nof neural networks, to enable e\ufb03cient training, and then\nto perform e\ufb03cient inference (execution for \ufb01xed parame-\nters) [29, 12, 3]. We base our work on TensorFlow, an open-\nsource data\ufb02ow engine released by Google [3]. TensorFlow\nallows the programmer to de\ufb01ne large computation graphs\nfrom basic operators, and to distribute their execution across\na heterogeneous distributed system. TensorFlow automates\nthe creation of the computation graphs for gradients; it also\nmakes it easy to batch computation.\n3.\nOUR APPROACH\nThis section describes the main components of our ap-\nproach toward di\ufb00erentially private training of neural net-\nworks: a di\ufb00erentially private stochastic gradient descent\n(SGD) algorithm, the moments accountant, and hyperpa-\nrameter tuning.\n3.1\nDifferentially Private SGD Algorithm\nOne might attempt to protect the privacy of training data\nby working only on the \ufb01nal parameters that result from the\ntraining process, treating this process as a black box. Un-\nfortunately, in general, one may not have a useful, tight\ncharacterization of the dependence of these parameters on\nthe training data; adding overly conservative noise to the pa-\nrameters, where the noise is selected according to the worst-\ncase analysis, would destroy the utility of the learned model.\nTherefore, we prefer a more sophisticated approach in which\nwe aim to control the in\ufb02uence of the training data during\nthe training process, speci\ufb01cally in the SGD computation.\nThis approach has been followed in previous works (e.g., [52,\n7]); we make several modi\ufb01cations and extensions, in par-\nticular in our privacy accounting.\nAlgorithm 1 outlines our basic method for training a model\nwith parameters \u03b8 by minimizing the empirical loss function\nL(\u03b8). At each step of the SGD, we compute the gradient\n\u2207\u03b8L(\u03b8, xi) for a random subset of examples, clip the \u21132 norm\nof each gradient, compute the average, add noise in order to\nprotect privacy, and take a step in the opposite direction of\nthis average noisy gradient. At the end, in addition to out-\nputting the model, we will also need to compute the privacy\nloss of the mechanism based on the information maintained\nby the privacy accountant. Next we describe in more detail\neach component of this algorithm and our re\ufb01nements.\nAlgorithm 1 Di\ufb00erentially private SGD (Outline)\nInput: Examples\n{x1, . . . , xN},\nloss\nfunction\nL(\u03b8)\n=\n1\nN\nP\ni L(\u03b8, xi). Parameters: learning rate \u03b7t, noise scale\n\u03c3, group size L, gradient norm bound C.\nInitialize \u03b80 randomly\nfor t \u2208[T] do\nTake a random sample Lt with sampling probability\nL/N\nCompute gradient\nFor each i \u2208Lt, compute gt(xi) \u2190\u2207\u03b8tL(\u03b8t, xi)\nClip gradient\n\u00afgt(xi) \u2190gt(xi)/ max\n\u00001, \u2225gt(xi)\u22252\nC\n\u0001\nAdd noise\n\u02dcgt \u21901\nL\n\u0000P\ni \u00afgt(xi) + N(0, \u03c32C2I)\n\u0001\nDescent\n\u03b8t+1 \u2190\u03b8t \u2212\u03b7t\u02dcgt\nOutput \u03b8T and compute the overall privacy cost (\u03b5, \u03b4)\nusing a privacy accounting method.\nNorm clipping: Proving the di\ufb00erential privacy guarantee\nof Algorithm 1 requires bounding the in\ufb02uence of each indi-\nvidual example on \u02dcgt. Since there is no a priori bound on\nthe size of the gradients, we clip each gradient in \u21132 norm;\ni.e., the gradient vector g is replaced by g/ max\n\u00001, \u2225g\u22252\nC\n\u0001\n,\nfor a clipping threshold C.\nThis clipping ensures that if\n\u2225g\u22252 \u2264C, then g is preserved, whereas if \u2225g\u22252 > C, it gets\nscaled down to be of norm C. We remark that gradient clip-\nping of this form is a popular ingredient of SGD for deep\nnetworks for non-privacy reasons, though in that setting it\nusually su\ufb03ces to clip after averaging.\nPer-layer and time-dependent parameters: The pseu-\ndocode for Algorithm 1 groups all the parameters into a\nsingle input \u03b8 of the loss function L(\u00b7). For multi-layer neu-\nral networks, we consider each layer separately, which allows\nsetting di\ufb00erent clipping thresholds C and noise scales \u03c3 for\ndi\ufb00erent layers. Additionally, the clipping and noise param-\neters may vary with the number of training steps t. In results\npresented in Section 5 we use constant settings for C and \u03c3.\nLots: Like the ordinary SGD algorithm, Algorithm 1 esti-\nmates the gradient of L by computing the gradient of the\nloss on a group of examples and taking the average. This av-\nerage provides an unbiased estimator, the variance of which\ndecreases quickly with the size of the group. We call such a\ngroup a lot, to distinguish it from the computational group-\ning that is commonly called a batch. In order to limit mem-\nory consumption, we may set the batch size much smaller\nthan the lot size L, which is a parameter of the algorithm.\nWe perform the computation in batches, then group several\nbatches into a lot for adding noise. In practice, for e\ufb03ciency,\nthe construction of batches and lots is done by randomly per-\nmuting the examples and then partitioning them into groups\nof the appropriate sizes. For ease of analysis, however, we as-\nsume that each lot is formed by independently picking each\nexample with probability q = L/N, where N is the size of\nthe input dataset.\nAs is common in the literature, we normalize the running\ntime of a training algorithm by expressing it as the number\nof epochs, where each epoch is the (expected) number of\nbatches required to process N examples. In our notation,\nan epoch consists of N/L lots.\nPrivacy accounting: For di\ufb00erentially private SGD, an\nimportant issue is computing the overall privacy cost of the\ntraining.\nThe composability of di\ufb00erential privacy allows\nus to implement an \u201caccountant\u201d procedure that computes\nthe privacy cost at each access to the training data, and\naccumulates this cost as the training progresses. Each step\nof training typically requires gradients at multiple layers,\nand the accountant accumulates the cost that corresponds\nto all of them.\nMoments accountant: Much research has been devoted\nto studying the privacy loss for a particular noise distribu-\ntion as well as the composition of privacy losses. For the\nGaussian noise that we use, if we choose \u03c3 in Algorithm 1\nto be\nq\n2 log 1.25\n\u03b4 /\u03b5, then by standard arguments [20] each\nstep is (\u03b5, \u03b4)-di\ufb00erentially private with respect to the lot.\nSince the lot itself is a random sample from the database,\nthe privacy ampli\ufb01cation theorem [33, 8] implies that each\nstep is (O(q\u03b5), q\u03b4)-di\ufb00erentially private with respect to the\nfull database where q = L/N is the sampling ratio per lot\nand \u03b5 \u22641. The result in the literature that yields the best\noverall bound is the strong composition theorem [22].\nHowever, the strong composition theorem can be loose,\nand does not take into account the particular noise distribu-\ntion under consideration. In our work, we invent a stronger\naccounting method, which we call the moments accountant.\nIt allows us to prove that Algorithm 1 is (O(q\u03b5\n\u221a\nT), \u03b4)-\ndi\ufb00erentially private for appropriately chosen settings of the\nnoise scale and the clipping threshold. Compared to what\none would obtain by the strong composition theorem, our\nbound is tighter in two ways: it saves a\np\nlog(1/\u03b4) factor in\nthe \u03b5 part and a Tq factor in the \u03b4 part. Since we expect\n\u03b4 to be small and T \u226b1/q (i.e., each example is examined\nmultiple times), the saving provided by our bound is quite\nsigni\ufb01cant. This result is one of our main contributions.\nTheorem 1. There exist constants c1 and c2 so that given\nthe sampling probability q = L/N and the number of steps\nT, for any \u03b5 < c1q2T, Algorithm 1 is (\u03b5, \u03b4)-di\ufb00erentially\nprivate for any \u03b4 > 0 if we choose\n\u03c3 \u2265c2\nq\np\nT log(1/\u03b4)\n\u03b5\n.\nIf we use the strong composition theorem, we will then\nneed to choose \u03c3 = \u2126(q\np\nT log(1/\u03b4) log(T/\u03b4)/\u03b5). Note that\nwe save a factor of\np\nlog(T/\u03b4) in our asymptotic bound. The\nmoments accountant is bene\ufb01cial in theory, as this result\nindicates, and also in practice, as can be seen from Figure 2\nin Section 4. For example, with L = 0.01N, \u03c3 = 4, \u03b4 =\n10\u22125, and T = 10000, we have \u03b5 \u22481.26 using the moments\naccountant. As a comparison, we would get a much larger\n\u03b5 \u22489.34 using the strong composition theorem.\n3.2\nThe Moments Accountant: Details\nThe moments accountant keeps track of a bound on the\nmoments of the privacy loss random variable (de\ufb01ned be-\nlow in Eq. (1)).\nIt generalizes the standard approach of\ntracking (\u03b5, \u03b4) and using the strong composition theorem.\nWhile such an improvement was known previously for com-\nposing Gaussian mechanisms, we show that it applies also\nfor composing Gaussian mechanisms with random sampling\nand can provide much tighter estimate of the privacy loss of\nAlgorithm 1.\nPrivacy loss is a random variable dependent on the ran-\ndom noise added to the algorithm. That a mechanism M\nis (\u03b5, \u03b4)-di\ufb00erentially private is equivalent to a certain tail\nbound on M\u2019s privacy loss random variable. While the tail\nbound is very useful information on a distribution, compos-\ning directly from it can result in quite loose bounds. We in-\nstead compute the log moments of the privacy loss random\nvariable, which compose linearly. We then use the moments\nbound, together with the standard Markov inequality, to ob-\ntain the tail bound, that is the privacy loss in the sense of\ndi\ufb00erential privacy.\nMore speci\ufb01cally, for neighboring databases d, d\u2032 \u2208Dn, a\nmechanism M, auxiliary input aux, and an outcome o \u2208R,\nde\ufb01ne the privacy loss at o as\nc(o; M, aux, d, d\u2032)\n\u2206= log Pr[M(aux, d) = o]\nPr[M(aux, d\u2032) = o].\n(1)\nA common design pattern, which we use extensively in the\npaper, is to update the state by sequentially applying di\ufb00er-\nentially private mechanisms. This is an instance of adaptive\ncomposition, which we model by letting the auxiliary input\nof the kth mechanism Mk be the output of all the previous\nmechanisms.\nFor a given mechanism M, we de\ufb01ne the \u03bbth moment\n\u03b1M(\u03bb; aux, d, d\u2032) as the log of the moment generating func-\ntion evaluated at the value \u03bb:\n\u03b1M(\u03bb; aux, d, d\u2032)\n\u2206=\nlog Eo\u223cM(aux,d)[exp(\u03bbc(o; M, aux, d, d\u2032))].\n(2)\nIn order to prove privacy guarantees of a mechanism, it is\nuseful to bound all possible \u03b1M(\u03bb; aux, d, d\u2032). We de\ufb01ne\n\u03b1M(\u03bb)\n\u2206= max\naux,d,d\u2032 \u03b1M(\u03bb; aux, d, d\u2032) ,\nwhere the maximum is taken over all possible aux and all\nthe neighboring databases d, d\u2032.\nWe state the properties of \u03b1 that we use for the moments\naccountant.\nTheorem 2. Let \u03b1M(\u03bb) de\ufb01ned as above. Then\n1. [Composability] Suppose that a mechanism M con-\nsists of a sequence of adaptive mechanisms M1, . . . , Mk\nwhere Mi : Qi\u22121\nj=1 Rj \u00d7 D \u2192Ri. Then, for any \u03bb\n\u03b1M(\u03bb) \u2264\nk\nX\ni=1\n\u03b1Mi(\u03bb) .\n2. [Tail bound] For any \u03b5 > 0, the mechanism M is\n(\u03b5, \u03b4)-di\ufb00erentially private for\n\u03b4 = min\n\u03bb exp(\u03b1M(\u03bb) \u2212\u03bb\u03b5) .\nIn particular, Theorem 2.1 holds when the mechanisms\nthemselves are chosen based on the (public) output of the\nprevious mechanisms.\nBy Theorem 2, it su\ufb03ces to compute, or bound, \u03b1Mi(\u03bb) at\neach step and sum them to bound the moments of the mech-\nanism overall. We can then use the tail bound to convert the\nmoments bound to the (\u03b5, \u03b4)-di\ufb00erential privacy guarantee.\nThe main challenge that remains is to bound the value\n\u03b1Mt(\u03bb) for each step. In the case of a Gaussian mechanism\nwith random sampling, it su\ufb03ces to estimate the following\nmoments.\nLet \u00b50 denote the probability density function\n(pdf) of N(0, \u03c32), and \u00b51 denote the pdf of N(1, \u03c32). Let \u00b5\nbe the mixture of two Gaussians \u00b5 = (1 \u2212q)\u00b50 + q\u00b51. Then\nwe need to compute \u03b1(\u03bb) = log max(E1, E2) where\nE1 = Ez\u223c\u00b50[(\u00b50(z)/\u00b5(z))\u03bb] ,\n(3)\nE2 = Ez\u223c\u00b5 [(\u00b5(z)/\u00b50(z))\u03bb] .\n(4)\nIn the implementation of the moments accountant, we\ncarry out numerical integration to compute \u03b1(\u03bb).\nIn ad-\ndition, we can show the asymptotic bound\n\u03b1(\u03bb) \u2264q2\u03bb(\u03bb + 1)/(1 \u2212q)\u03c32 + O(q3/\u03c33) .\nTogether with Theorem 2, the above bound implies our\nmain Theorem 1. The details can be found in the Appendix.\n3.3\nHyperparameter Tuning\nWe identify characteristics of models relevant for privacy\nand, speci\ufb01cally, hyperparameters that we can tune in order\nto balance privacy, accuracy, and performance. In particu-\nlar, through experiments, we observe that model accuracy\nis more sensitive to training parameters such as batch size\nand noise level than to the structure of a neural network.\nIf we try several settings for the hyperparameters, we can\ntrivially add up the privacy costs of all the settings, possibly\nvia the moments accountant. However, since we care only\nabout the setting that gives us the most accurate model, we\ncan do better, such as applying a version of a result from\nGupta et al. [27] restated as Theorem D.1 in the Appendix.\nWe can use insights from theory to reduce the number of\nhyperparameter settings that need to be tried. While di\ufb00er-\nentially private optimization of convex objective functions\nis best achieved using batch sizes as small as 1, non-convex\nlearning, which is inherently less stable, bene\ufb01ts from ag-\ngregation into larger batches. At the same time, Theorem 1\nsuggests that making batches too large increases the pri-\nvacy cost, and a reasonable tradeo\ufb00is to take the number\nof batches per epoch to be of the same order as the desired\nnumber of epochs. The learning rate in non-private train-\ning is commonly adjusted downwards carefully as the model\nconverges to a local optimum. In contrast, we never need\nto decrease the learning rate to a very small value, because\ndi\ufb00erentially private training never reaches a regime where\nit would be justi\ufb01ed.\nOn the other hand, in our experi-\nments, we do \ufb01nd that there is a small bene\ufb01t to starting\nwith a relatively large learning rate, then linearly decaying\nit to a smaller value in a few epochs, and keeping it constant\nafterwards.\n4.\nIMPLEMENTATION\nWe have implemented the di\ufb00erentially private SGD al-\ngorithms in TensorFlow. The source code is available under\nan Apache 2.0 license from github.com/tensor\ufb02ow/models.\nFor privacy protection, we need to \u201csanitize\u201d the gradient\nbefore using it to update the parameters. In addition, we\nneed to keep track of the \u201cprivacy spending\u201d based on how\nthe sanitization is done. Hence our implementation mainly\nconsists of two components: sanitizer, which preprocesses\nthe gradient to protect privacy, and privacy_accountant,\nwhich keeps track of the privacy spending over the course of\ntraining.\nFigure 1 contains the TensorFlow code snippet (in Python)\nof DPSGD_Optimizer, which minimizes a loss function us-\ning a di\ufb00erentially private SGD, and DPTrain, which itera-\ntively invokes DPSGD_Optimizer using a privacy accountant\nto bound the total privacy loss.\nIn many cases, the neural network model may bene\ufb01t from\nthe processing of the input by projecting it on the principal\ndirections (PCA) or by feeding it through a convolutional\nlayer. We implement di\ufb00erentially private PCA and apply\npre-trained convolutional layers (learned on public data).\nSanitizer. In order to achieve privacy protection, the sani-\ntizer needs to perform two operations: (1) limit the sensitiv-\nity of each individual example by clipping the norm of the\ngradient for each example; and (2) add noise to the gradient\nof a batch before updating the network parameters.\nIn TensorFlow, the gradient computation is batched for\nperformance reasons, yielding gB = 1/|B| P\nx\u2208B \u2207\u03b8L(\u03b8, x)\nfor a batch B of training examples. To limit the sensitivity\nof updates, we need to access each individual \u2207\u03b8L(\u03b8, x). To\nthis end, we implemented per_example_gradient operator\nin TensorFlow, as described by Goodfellow [25]. This opera-\ntor can compute a batch of individual \u2207\u03b8L(\u03b8, x). With this\nimplementation there is only a modest slowdown in train-\ning, even for larger batch size. Our current implementation\nsupports batched computation for the loss function L, where\neach xi is singly connected to L, allowing us to handle most\nhidden layers but not, for example, convolutional layers.\nOnce we have the access to the per-example gradient, it\nis easy to use TensorFlow operators to clip its norm and to\nadd noise.\nPrivacy accountant. The main component in our imple-\nmentation is PrivacyAccountant which keeps track of pri-\nvacy spending over the course of training. As discussed in\nSection 3, we implemented the moments accountant that ad-\nditively accumulates the log of the moments of the privacy\nloss at each step. Dependent on the noise distribution, one\ncan compute \u03b1(\u03bb) by either applying an asymptotic bound,\nevaluating a closed-form expression, or applying numerical\nclass DPSGD_Optimizer():\ndef __init__(self, accountant, sanitizer):\nself._accountant = accountant\nself._sanitizer = sanitizer\ndef Minimize(self, loss, params,\nbatch_size, noise_options):\n# Accumulate privacy spending before computing\n# and using the gradients.\npriv_accum_op =\nself._accountant.AccumulatePrivacySpending(\nbatch_size, noise_options)\nwith tf.control_dependencies(priv_accum_op):\n# Compute per example gradients\npx_grads = per_example_gradients(loss, params)\n# Sanitize gradients\nsanitized_grads = self._sanitizer.Sanitize(\npx_grads, noise_options)\n# Take a gradient descent step\nreturn apply_gradients(params, sanitized_grads)\ndef DPTrain(loss, params, batch_size, noise_options):\naccountant = PrivacyAccountant()\nsanitizer = Sanitizer()\ndp_opt = DPSGD_Optimizer(accountant, sanitizer)\nsgd_op = dp_opt.Minimize(\nloss, params, batch_size, noise_options)\neps, delta = (0, 0)\n# Carry out the training as long as the privacy\n# is within the pre-set limit.\nwhile within_limit(eps, delta):\nsgd_op.run()\neps, delta = accountant.GetSpentPrivacy()\nFigure 1: Code snippet of DPSGD_Optimizer and DP-\nTrain.\nintegration. The \ufb01rst option would recover the generic ad-\nvanced composition theorem, and the latter two give a more\naccurate accounting of the privacy loss.\nFor the Gaussian mechanism we use, \u03b1(\u03bb) is de\ufb01ned ac-\ncording to Eqs. (3) and (4).\nIn our implementation, we\ncarry out numerical integration to compute both E1 and E2\nin those equations. Also we compute \u03b1(\u03bb) for a range of\n\u03bb\u2019s so we can compute the best possible (\u03b5, \u03b4) values using\nTheorem 2.2. We \ufb01nd that for the parameters of interest to\nus, it su\ufb03ces to compute \u03b1(\u03bb) for \u03bb \u226432.\nAt any point during training, one can query the privacy\nloss in the more interpretable notion of (\u03b5, \u03b4) privacy using\nTheorem 2.2. Rogers et al. [47] point out risks associated\nwith adaptive choice of privacy parameters. We avoid their\nattacks and negative results by \ufb01xing the number of iter-\nations and privacy parameters ahead of time.\nMore gen-\neral implementations of a privacy accountant must correctly\ndistinguish between two modes of operation\u2014as a privacy\nodometer or a privacy \ufb01lter (see [47] for more details).\nDi\ufb00erentially private PCA. Principal component analy-\nsis (PCA) is a useful method for capturing the main features\nof the input data. We implement the di\ufb00erentially private\nPCA algorithm as described in [23]. More speci\ufb01cally, we\ntake a random sample of the training examples, treat them\nas vectors, and normalize each vector to unit \u21132 norm to\nform the matrix A, where each vector is a row in the ma-\ntrix. We then add Gaussian noise to the covariance matrix\nAT A and compute the principal directions of the noisy co-\nvariance matrix. Then for each input example we apply the\nprojection to these principal directions before feeding it into\nthe neural network.\nWe incur a privacy cost due to running a PCA. However,\nwe \ufb01nd it useful for both improving the model quality and for\nreducing the training time, as suggested by our experiments\non the MNIST data. See Section 4 for details.\nConvolutional layers. Convolutional layers are useful for\ndeep neural networks.\nHowever, an e\ufb03cient per-example\ngradient computation for convolutional layers remains a chal-\nlenge within the TensorFlow framework, which motivates\ncreating a separate work\ufb02ow.\nFor example, some recent\nwork argues that even random convolutions often su\ufb03ce [46,\n13, 49, 55, 14].\nAlternatively, we explore the idea of learning convolu-\ntional layers on public data, following Jarrett et al. [30].\nSuch convolutional layers can be based on GoogLeNet or\nAlexNet features [54, 35] for image models or on pretrained\nword2vec or GloVe embeddings in language models [41, 44].\n5.\nEXPERIMENTAL RESULTS\nThis section reports on our evaluation of the moments ac-\ncountant, and results on two popular image datasets: MNIST\nand CIFAR-10.\n5.1\nApplying the Moments Accountant\nAs shown by Theorem 1, the moments accountant pro-\nvides a tighter bound on the privacy loss compared to the\ngeneric strong composition theorem. Here we compare them\nusing some concrete values. The overall privacy loss (\u03b5, \u03b4)\ncan be computed from the noise level \u03c3, the sampling ra-\ntio of each lot q = L/N (so each epoch consists of 1/q\nbatches), and the number of epochs E (so the number of\nsteps is T = E/q). We \ufb01x the target \u03b4 = 10\u22125, the value\nused for our MNIST and CIFAR experiments.\nIn our experiment, we set q = 0.01, \u03c3 = 4, and \u03b4 = 10\u22125,\nand compute the value of \u03b5 as a function of the training\nepoch E. Figure 2 shows two curves corresponding to, re-\nspectively, using the strong composition theorem and the\nmoments accountant. We can see that we get a much tighter\nestimation of the privacy loss by using the moments accoun-\ntant.\nFor examples, when E = 100, the values are 9.34\nand 1.26 respectively, and for E = 400, the values are 24.22\nand 2.55 respectively. That is, using the moments bound,\nwe achieve (2.55, 10\u22125)-di\ufb00erential privacy, whereas previ-\nous techniques only obtain the signi\ufb01cantly worse guarantee\nof (24.22, 10\u22125).\n5.2\nMNIST\nWe conduct experiments on the standard MNIST dataset\nfor handwritten digit recognition consisting of 60,000 train-\ning examples and 10,000 testing examples [36]. Each exam-\nple is a 28 \u00d7 28 size gray-level image. We use a simple feed-\nforward neural network with ReLU units and softmax of 10\nclasses (corresponding to the 10 digits) with cross-entropy\nloss and an optional PCA input layer.\nBaseline model.\nOur baseline model uses a 60-dimensional PCA projection\nlayer and a single hidden layer with 1,000 hidden units. Us-\ning the lot size of 600, we can reach accuracy of 98.30% in\nabout 100 epochs. This result is consistent with what can\nbe achieved with a vanilla neural network [36].\nFigure 2: The \u03b5 value as a function of epoch E for\nq = 0.01, \u03c3 = 4, \u03b4 = 10\u22125, using the strong composition\ntheorem and the moments accountant respectively.\nDifferentially private model.\nFor the di\ufb00erentially private version, we experiment with\nthe same architecture with a 60-dimensional PCA projection\nlayer, a single 1,000-unit ReLU hidden layer, and a lot size of\n600. To limit sensitivity, we clip the gradient norm of each\nlayer at 4. We report results for three choices of the noise\nscale, which we call small (\u03c3 = 2, \u03c3p = 4), medium (\u03c3 =\n4, \u03c3p = 7), and large (\u03c3 = 8, \u03c3p = 16). Here \u03c3 represents\nthe noise level for training the neural network, and \u03c3p the\nnoise level for PCA projection. The learning rate is set at 0.1\ninitially and linearly decreased to 0.052 over 10 epochs and\nthen \ufb01xed to 0.052 thereafter. We have also experimented\nwith multi-hidden-layer networks.\nFor MNIST, we found\nthat one hidden layer combined with PCA works better than\na two-layer network.\nFigure 3 shows the results for di\ufb00erent noise levels.\nIn\neach plot, we show the evolution of the training and testing\naccuracy as a function of the number of epochs as well as\nthe corresponding \u03b4 value, keeping \u03b5 \ufb01xed. We achieve 90%,\n95%, and 97% test set accuracy for (0.5, 10\u22125), (2, 10\u22125),\nand (8, 10\u22125)-di\ufb00erential privacy respectively.\nOne attractive consequence of applying di\ufb00erentially pri-\nvate SGD is the small di\ufb00erence between the model\u2019s ac-\ncuracy on the training and the test sets, which is consis-\ntent with the theoretical argument that di\ufb00erentially private\ntraining generalizes well [6]. In contrast, the gap between\ntraining and testing accuracy in non-private training, i.e.,\nevidence of over\ufb01tting, increases with the number of epochs.\nBy using the moments accountant, we can obtain a \u03b4 value\nfor any given \u03b5. We record the accuracy for di\ufb00erent (\u03b5, \u03b4)\npairs in Figure 4. In the \ufb01gure, each curve corresponds to the\nbest accuracy achieved for a \ufb01xed \u03b4, as it varies between 10\u22125\nand 10\u22122. For example, we can achieve 90% accuracy for\n\u03b5 = 0.25 and \u03b4 = 0.01. As can be observed from the \ufb01gure,\nfor a \ufb01xed \u03b4, varying the value of \u03b5 can have large impact\non accuracy, but for any \ufb01xed \u03b5, there is less di\ufb00erence with\ndi\ufb00erent \u03b4 values.\nEffect of the parameters.\nClassi\ufb01cation accuracy is determined by multiple factors\n(1) Large noise\n(2) Medium noise\n(3) Small noise\nFigure 3: Results on the accuracy for di\ufb00erent noise levels on the MNIST dataset. In all the experiments, the\nnetwork uses 60 dimension PCA projection, 1,000 hidden units, and is trained using lot size 600 and clipping\nthreshold 4. The noise levels (\u03c3, \u03c3p) for training the neural network and for PCA projection are set at (8, 16),\n(4, 7), and (2, 4), respectively, for the three experiments.\nFigure 4: Accuracy of various (\u03b5, \u03b4) privacy values\non the MNIST dataset. Each curve corresponds to\na di\ufb00erent \u03b4 value.\nthat must be carefully tuned for optimal performance. These\nfactors include the topology of the network, the number of\nPCA dimensions and the number of hidden units, as well as\nparameters of the training procedure such as the lot size and\nthe learning rate. Some parameters are speci\ufb01c to privacy,\nsuch as the gradient norm clipping bound and the noise level.\nTo demonstrate the e\ufb00ects of these parameters, we manip-\nulate them individually, keeping the rest constant. We set\nthe reference values as follows: 60 PCA dimensions, 1,000\nhidden units, 600 lot size, gradient norm bound of 4, ini-\ntial learning rate of 0.1 decreasing to a \ufb01nal learning rate\nof 0.052 in 10 epochs, and noise \u03c3 equal to 4 and 7 respec-\ntively for training the neural network parameters and for the\nPCA projection. For each combination of values, we train\nuntil the point at which (2, 10\u22125)-di\ufb00erential privacy would\nbe violated (so, for example, a larger \u03c3 allows more epochs\nof training). The results are presented in Figure 5.\nPCA projection.\nIn our experiments, the accuracy is\nfairly stable as a function of the PCA dimension, with the\nbest results achieved for 60. (Not doing PCA reduces ac-\ncuracy by about 2%.) Although in principle the PCA pro-\njection layer can be replaced by an additional hidden layer,\nwe achieve better accuracy by training the PCA layer sep-\narately. By reducing the input size from 784 to 60, PCA\nleads to an almost 10\u00d7 reduction in training time. The re-\nsult is fairly stable over a large range of the noise levels for\nthe PCA projection and consistently better than the accu-\nracy using random projection, which is at about 92.5% and\nshown as a horizontal line in the plot.\nNumber of hidden units. Including more hidden units\nmakes it easier to \ufb01t the training set. For non-private train-\ning, it is often preferable to use more units, as long as we\nemploy techniques to avoid over\ufb01tting. However, for di\ufb00er-\nentially private training, it is not a priori clear if more hidden\nunits improve accuracy, as more hidden units increase the\nsensitivity of the gradient, which leads to more noise added\nat each update.\nSomewhat counterintuitively, increasing the number of\nhidden units does not decrease accuracy of the trained model.\nOne possible explanation that calls for further analysis is\nthat larger networks are more tolerant to noise. This prop-\nerty is quite encouraging as it is common in practice to use\nvery large networks.\nLot size. According to Theorem 1, we can run N/L epochs\nwhile staying within a constant privacy budget. Choosing\nthe lot size must balance two con\ufb02icting objectives. On the\none hand, smaller lots allow running more epochs, i.e., passes\nover data, improving accuracy.\nOn the other hand, for a\nlarger lot, the added noise has a smaller relative e\ufb00ect.\nOur experiments show that the lot size has a relatively\nlarge impact on accuracy. Empirically, the best lot size is\nroughly\n\u221a\nN where N is the number of training examples.\nLearning rate. Accuracy is stable for a learning rate in\nthe range of [0.01, 0.07] and peaks at 0.05, as shown in Fig-\nure 5(4).\nHowever, accuracy decreases signi\ufb01cantly if the\nlearning rate is too large. Some additional experiments sug-\ngest that, even for large learning rates, we can reach similar\nlevels of accuracy by reducing the noise level and, accord-\ningly, by training less in order to avoid exhausting the pri-\nvacy budget.\nClipping bound. Limiting the gradient norm has two op-\nposing e\ufb00ects: clipping destroys the unbiasedness of the gra-\ndient estimate, and if the clipping parameter is too small,\nthe average clipped gradient may point in a very di\ufb00erent\n(1) variable projection dimensions\n(2) variable hidden units\n(3) variable lot size\n(4) variable learning rate\n(5) variable gradient clipping norm\n(6) variable noise level\nFigure 5: MNIST accuracy when one parameter varies, and the others are \ufb01xed at reference values.\ndirection from the true gradient.\nOn the other hand, in-\ncreasing the norm bound C forces us to add more noise to\nthe gradients (and hence the parameters), since we add noise\nbased on \u03c3C. In practice, a good way to choose a value for\nC is by taking the median of the norms of the unclipped\ngradients over the course of training.\nNoise level. By adding more noise, the per-step privacy\nloss is proportionally smaller, so we can run more epochs\nwithin a given cumulative privacy budget. In Figure 5(5),\nthe x-axis is the noise level \u03c3. The choice of this value has\na large impact on accuracy.\nFrom the experiments, we observe the following.\n1. The PCA projection improves both model accuracy\nand training performance.\nAccuracy is quite stable\nover a large range of choices for the projection dimen-\nsions and the noise level used in the PCA stage.\n2. The accuracy is fairly stable over the network size.\nWhen we can only run smaller number of epochs, it is\nmore bene\ufb01cial to use a larger network.\n3. The training parameters, especially the lot size and\nthe noise scale \u03c3, have a large impact on the model\naccuracy. They both determine the \u201cnoise-to-signal\u201d\nratio of the sanitized gradients as well as the number\nof epochs we are able to go through the data before\nreaching the privacy limit.\nOur framework allows for adaptive control of the training\nparameters, such as the lot size, the gradient norm bound\nC, and noise level \u03c3. Our initial experiments with decreas-\ning noise as training progresses did not show a signi\ufb01cant\nimprovement, but it is interesting to consider more sophis-\nticated schemes for adaptively choosing these parameters.\n5.3\nCIFAR\nWe also conduct experiments on the CIFAR-10 dataset,\nwhich consists of color images classi\ufb01ed into 10 classes such\nas ships, cats, and dogs, and partitioned into 50,000 training\nexamples and 10,000 test examples [1]. Each example is a\n32 \u00d7 32 image with three channels (RGB). For this learning\ntask, nearly all successful networks use convolutional layers.\nThe CIFAR-100 dataset has similar parameters, except that\nimages are classi\ufb01ed into 100 classes; the examples and the\nimage classes are di\ufb00erent from those of CIFAR-10.\nWe use the network architecture from the TensorFlow con-\nvolutional neural networks tutorial [2]. Each 32 \u00d7 32 image\nis \ufb01rst cropped to a 24 \u00d7 24 one by taking the center patch.\nThe network architecture consists of two convolutional lay-\ners followed by two fully connected layers. The convolutional\nlayers use 5 \u00d7 5 convolutions with stride 1, followed by a\nReLU and 2 \u00d7 2 max pools, with 64 channels each. Thus\nthe \ufb01rst convolution outputs a 12 \u00d7 12 \u00d7 64 tensor for each\nimage, and the second outputs a 6\u00d76\u00d764 tensor. The latter\nis \ufb02attened to a vector that gets fed into a fully connected\nlayer with 384 units, and another one of the same size.\nThis architecture, non-privately, can get to about 86% ac-\ncuracy in 500 epochs. Its simplicity makes it an appealing\nchoice for our work. We should note however that by us-\ning deeper networks with di\ufb00erent non-linearities and other\nadvanced techniques, one can obtain signi\ufb01cantly better ac-\ncuracy, with the state-of-the-art being about 96.5% [26].\nAs is standard for such image datasets, we use data aug-\nmentation during training. For each training image, we gen-\nerate a new distorted image by randomly picking a 24 \u00d7 24\npatch from the image, randomly \ufb02ipping the image along\nthe left-right direction, and randomly distorting the bright-\nness and the contrast of the image. In each epoch, these\n(1) \u03b5 = 2\n(2) \u03b5 = 4\n(3) \u03b5 = 8\nFigure 6: Results on accuracy for di\ufb00erent noise levels on CIFAR-10. With \u03b4 set to 10\u22125, we achieve accuracy\n67%, 70%, and 73%, with \u03b5 being 2, 4, and 8, respectively. The \ufb01rst graph uses a lot size of 2,000, (2) and (3)\nuse a lot size of 4,000. In all cases, \u03c3 is set to 6, and clipping is set to 3.\ndistortions are done independently. We refer the reader to\nthe TensorFlow tutorial [2] for additional details.\nAs the convolutional layers have shared parameters, com-\nputing per-example gradients has a larger computational\noverhead. Previous work has shown that convolutional lay-\ners are often transferable: parameters learned from one data-\nset can be used on another one without retraining [30]. We\ntreat the CIFAR-100 dataset as a public dataset and use it\nto train a network with the same architecture. We use the\nconvolutions learned from training this dataset.\nRetrain-\ning only the fully connected layers with this architecture for\nabout 250 epochs with a batch size of 120 gives us approxi-\nmately 80% accuracy, which is our non-private baseline.\nDifferentially private version.\nFor the di\ufb00erentially private version, we use the same ar-\nchitecture. As discussed above, we use pre-trained convolu-\ntional layers. The fully connected layers are initialized from\nthe pre-trained network as well. We train the softmax layer,\nand either the top or both fully connected layers. Based on\nlooking at gradient norms, the softmax layer gradients are\nroughly twice as large as the other two layers, and we keep\nthis ratio when we try clipping at a few di\ufb00erent values be-\ntween 3 and 10. The lot size is an additional knob that we\ntune: we tried 600, 2,000, and 4,000. With these settings,\nthe per-epoch training time increases from approximately 40\nseconds to 180 seconds.\nIn Figure 6, we show the evolution of the accuracy and\nthe privacy cost, as a function of the number of epochs, for\na few di\ufb00erent parameter settings.\nThe various parameters in\ufb02uence the accuracy one gets, in\nways not too di\ufb00erent from that in the MNIST experiments.\nA lot size of 600 leads to poor results on this dataset and\nwe need to increase it to 2,000 or more for results reported\nin Figure 6.\nCompared to the MNIST dataset, where the di\ufb00erence in\naccuracy between a non-private baseline and a private model\nis about 1.3%, the corresponding drop in accuracy in our\nCIFAR-10 experiment is much larger (about 7%). We leave\nclosing this gap as an interesting test for future research in\ndi\ufb00erentially private machine learning.\n6.\nRELATED WORK\nThe problem of privacy-preserving data mining, or ma-\nchine learning, has been a focus of active work in several\nresearch communities since the late 90s [5, 37]. The exist-\ning literature can be broadly classi\ufb01ed along several axes:\nthe class of models, the learning algorithm, and the privacy\nguarantees.\nPrivacy guarantees. Early works on privacy-preserving\nlearning were done in the framework of secure function eval-\nuation (SFE) and secure multi-party computations (MPC),\nwhere the input is split between two or more parties, and\nthe focus is on minimizing information leaked during the\njoint computation of some agreed-to functionality. In con-\ntrast, we assume that data is held centrally, and we are\nconcerned with leakage from the functionality\u2019s output (i.e.,\nthe model).\nAnother approach, k-anonymity and closely related no-\ntions [53], seeks to o\ufb00er a degree of protection to underlying\ndata by generalizing and suppressing certain identifying at-\ntributes. The approach has strong theoretical and empirical\nlimitations [4, 9] that make it all but inapplicable to de-\nanonymization of high-dimensional, diverse input datasets.\nRather than pursue input sanitization, we keep the under-\nlying raw records intact and perturb derived data instead.\nThe theory of di\ufb00erential privacy, which provides the an-\nalytical framework for our work, has been applied to a large\ncollection of machine learning tasks that di\ufb00ered from ours\neither in the training mechanism or in the target model.\nThe moments accountant is closely related to the notion of\nR\u00b4enyi di\ufb00erential privacy [42], which proposes (scaled) \u03b1(\u03bb)\nas a means of quantifying privacy guarantees. In a concur-\nrent and independent work Bun and Steinke [10] introduce\na relaxation of di\ufb00erential privacy (generalizing the work\nof Dwork and Rothblum [20]) de\ufb01ned via a linear upper\nbound on \u03b1(\u03bb). Taken together, these works demonstrate\nthat the moments accountant is a useful technique for theo-\nretical and empirical analyses of complex privacy-preserving\nalgorithms.\nLearning algorithm. A common target for learning with\nprivacy is a class of convex optimization problems amenable\nto a wide variety of techniques [18, 11, 34]. In concurrent\nwork, Wu et al. achieve 83% accuracy on MNIST via con-\nvex empirical risk minimization [57]. Training multi-layer\nneural networks is non-convex, and typically solved by an\napplication of SGD, whose theoretical guarantees are poorly\nunderstood.\nFor the CIFAR neural network we incorporate di\ufb00eren-\ntially private training of the PCA projection matrix [23],\nwhich is used to reduce dimensionality of inputs.\nModel class. The \ufb01rst end-to-end di\ufb00erentially private sys-\ntem was evaluated on the Net\ufb02ix Prize dataset [39], a version\nof a collaborative \ufb01ltering problem. Although the problem\nshared many similarities with ours\u2014high-dimensional in-\nputs, non-convex objective function\u2014the approach taken by\nMcSherry and Mironov di\ufb00ered signi\ufb01cantly. They identi\ufb01ed\nthe core of the learning task, e\ufb00ectively su\ufb03cient statistics,\nthat can be computed in a di\ufb00erentially private manner via\na Gaussian mechanism. In our approach no such su\ufb03cient\nstatistics exist.\nIn a recent work Shokri and Shmatikov [50] designed and\nevaluated a system for distributed training of a deep neural\nnetwork. Participants, who hold their data closely, commu-\nnicate sanitized updates to a central authority. The sani-\ntization relies on an additive-noise mechanism, based on a\nsensitivity estimate, which could be improved to a hard sen-\nsitivity guarantee. They compute privacy loss per param-\neter (not for an entire model). By our preferred measure,\nthe total privacy loss per participant on the MNIST dataset\nexceeds several thousand.\nA di\ufb00erent, recent approach towards di\ufb00erentially private\ndeep learning is explored by Phan et al. [45].\nThis work\nfocuses on learning autoencoders. Privacy is based on per-\nturbing the objective functions of these autoencoders.\n7.\nCONCLUSIONS\nWe demonstrate the training of deep neural networks with\ndi\ufb00erential privacy, incurring a modest total privacy loss,\ncomputed over entire models with many parameters.\nIn\nour experiments for MNIST, we achieve 97% training accu-\nracy and for CIFAR-10 we achieve 73% accuracy, both with\n(8, 10\u22125)-di\ufb00erential privacy. Our algorithms are based on a\ndi\ufb00erentially private version of stochastic gradient descent;\nthey run on the TensorFlow software library for machine\nlearning.\nSince our approach applies directly to gradient\ncomputations, it can be adapted to many other classical\nand more recent \ufb01rst-order optimization methods, such as\nNAG [43], Momentum [48], AdaGrad [15], or SVRG [31].\nA new tool, which may be of independent interest, is a\nmechanism for tracking privacy loss, the moments accoun-\ntant. It permits tight automated analysis of the privacy loss\nof complex composite mechanisms that are currently beyond\nthe reach of advanced composition theorems.\nA number of avenues for further work are attractive. In\nparticular, we would like to consider other classes of deep\nnetworks. Our experience with MNIST and CIFAR-10 should\nbe helpful, but we see many opportunities for new research,\nfor example in applying our techniques to LSTMs used for\nlanguage modeling tasks. In addition, we would like to ob-\ntain additional improvements in accuracy.\nMany training\ndatasets are much larger than those of MNIST and CIFAR-\n10; accuracy should bene\ufb01t from their size.\n8.\nACKNOWLEDGMENTS\nWe are grateful to \u00b4Ulfar Erlingsson and Dan Ramage for\nmany useful discussions, and to Mark Bun and Thomas\nSteinke for sharing a draft of [10].\n9.\nREFERENCES\n[1] CIFAR-10 and CIFAR-100 datasets.\nwww.cs.toronto.edu/\u02dckriz/cifar.html.\n[2] TensorFlow convolutional neural networks tutorial.\nwww.tensor\ufb02ow.org/tutorials/deep cnn.\n[3] TensorFlow: Large-scale machine learning on\nheterogeneous systems, 2015. Software available from\ntensor\ufb02ow.org.\n[4] C. C. Aggarwal. On k-anonymity and the curse of\ndimensionality. In VLDB, pages 901\u2013909, 2005.\n[5] R. Agrawal and R. Srikant. Privacy-preserving data\nmining. In SIGMOD, pages 439\u2013450. ACM, 2000.\n[6] R. Bassily, K. Nissim, A. Smith, T. Steinke,\nU. Stemmer, and J. Ullman. Algorithmic stability for\nadaptive data analysis. In STOC, pages 1046\u20131059.\nACM, 2016.\n[7] R. Bassily, A. D. Smith, and A. Thakurta. Private\nempirical risk minimization: E\ufb03cient algorithms and\ntight error bounds. In FOCS, pages 464\u2013473. IEEE,\n2014.\n[8] A. Beimel, H. Brenner, S. P. Kasiviswanathan, and\nK. Nissim. Bounds on the sample complexity for\nprivate learning and private data release. Machine\nLearning, 94(3):401\u2013437, 2014.\n[9] J. Brickell and V. Shmatikov. The cost of privacy:\nDestruction of data-mining utility in anonymized data\npublishing. In KDD, pages 70\u201378. ACM, 2008.\n[10] M. Bun and T. Steinke. Concentrated di\ufb00erential\nprivacy: Simpli\ufb01cations, extensions, and lower bounds.\nIn TCC-B.\n[11] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate.\nDi\ufb00erentially private empirical risk minimization.\nJ. Machine Learning Research, 12:1069\u20131109, 2011.\n[12] R. Collobert, K. Kavukcuoglu, and C. Farabet.\nTorch7: A Matlab-like environment for machine\nlearning. In BigLearn, NIPS Workshop, number\nEPFL-CONF-192376, 2011.\n[13] D. D. Cox and N. Pinto. Beyond simple features: A\nlarge-scale feature search approach to unconstrained\nface recognition. In FG 2011, pages 8\u201315. IEEE, 2011.\n[14] A. Daniely, R. Frostig, and Y. Singer. Toward deeper\nunderstanding of neural networks: The power of\ninitialization and a dual view on expressivity. CoRR,\nabs/1602.05897, 2016.\n[15] J. Duchi, E. Hazan, and Y. Singer. Adaptive\nsubgradient methods for online learning and stochastic\noptimization. J. Machine Learning Research,\n12:2121\u20132159, July 2011.\n[16] C. Dwork. A \ufb01rm foundation for private data analysis.\nCommun. ACM, 54(1):86\u201395, Jan. 2011.\n[17] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov,\nand M. Naor. Our data, ourselves: Privacy via\ndistributed noise generation. In EUROCRYPT, pages\n486\u2013503. Springer, 2006.\n[18] C. Dwork and J. Lei. Di\ufb00erential privacy and robust\nstatistics. In STOC, pages 371\u2013380. ACM, 2009.\n[19] C. Dwork, F. McSherry, K. Nissim, and A. Smith.\nCalibrating noise to sensitivity in private data\nanalysis. In TCC, pages 265\u2013284. Springer, 2006.\n[20] C. Dwork and A. Roth. The algorithmic foundations\nof di\ufb00erential privacy. Foundations and Trends in\nTheoretical Computer Science, 9(3\u20134):211\u2013407, 2014.\n[21] C. Dwork and G. N. Rothblum. Concentrated\ndi\ufb00erential privacy. CoRR, abs/1603.01887, 2016.\n[22] C. Dwork, G. N. Rothblum, and S. Vadhan. Boosting\nand di\ufb00erential privacy. In FOCS, pages 51\u201360. IEEE,\n2010.\n[23] C. Dwork, K. Talwar, A. Thakurta, and L. Zhang.\nAnalyze Gauss: Optimal bounds for\nprivacy-preserving principal component analysis. In\nSTOC, pages 11\u201320. ACM, 2014.\n[24] M. Fredrikson, S. Jha, and T. Ristenpart. Model\ninversion attacks that exploit con\ufb01dence information\nand basic countermeasures. In CCS, pages 1322\u20131333.\nACM, 2015.\n[25] I. Goodfellow. E\ufb03cient per-example gradient\ncomputations. CoRR, abs/1510.01799v2, 2015.\n[26] B. Graham. Fractional max-pooling. CoRR,\nabs/1412.6071, 2014.\n[27] A. Gupta, K. Ligett, F. McSherry, A. Roth, and\nK. Talwar. Di\ufb00erentially private combinatorial\noptimization. In SODA, pages 1106\u20131125, 2010.\n[28] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep\ninto recti\ufb01ers: Surpassing human-level performance on\nImageNet classi\ufb01cation. In ICCV, pages 1026\u20131034.\nIEEE, 2015.\n[29] R. Ierusalimschy, L. H. de Figueiredo, and W. Filho.\nLua\u2014an extensible extension language. Software:\nPractice and Experience, 26(6):635\u2013652, 1996.\n[30] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and\nY. LeCun. What is the best multi-stage architecture\nfor object recognition? In ICCV, pages 2146\u20132153.\nIEEE, 2009.\n[31] R. Johnson and T. Zhang. Accelerating stochastic\ngradient descent using predictive variance reduction.\nIn NIPS, pages 315\u2013323, 2013.\n[32] P. Kairouz, S. Oh, and P. Viswanath. The composition\ntheorem for di\ufb00erential privacy. In ICML, pages\n1376\u20131385. ACM, 2015.\n[33] S. P. Kasiviswanathan, H. K. Lee, K. Nissim,\nS. Raskhodnikova, and A. Smith. What can we learn\nprivately? SIAM J. Comput., 40(3):793\u2013826, 2011.\n[34] D. Kifer, A. D. Smith, and A. Thakurta. Private\nconvex optimization for empirical risk minimization\nwith applications to high-dimensional regression. In\nCOLT, pages 25.1\u201325.40, 2012.\n[35] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImageNet classi\ufb01cation with deep convolutional neural\nnetworks. In NIPS, pages 1097\u20131105, 2012.\n[36] Y. LeCun, L. Bottou, Y. Bengio, and P. Ha\ufb00ner.\nGradient-based learning applied to document\nrecognition. Proceedings of the IEEE, 86(11), 1998.\n[37] Y. Lindell and B. Pinkas. Privacy preserving data\nmining. In CRYPTO, pages 36\u201354. Springer, 2000.\n[38] C. J. Maddison, A. Huang, I. Sutskever, and D. Silver.\nMove evaluation in Go using deep convolutional neural\nnetworks. In ICLR, 2015.\n[39] F. McSherry and I. Mironov. Di\ufb00erentially private\nrecommender systems: Building privacy into the\nNet\ufb02ix Prize contenders. In KDD, pages 627\u2013636.\nACM, 2009.\n[40] F. D. McSherry. Privacy integrated queries: An\nextensible platform for privacy-preserving data\nanalysis. In SIGMOD, pages 19\u201330. ACM, 2009.\n[41] T. Mikolov, K. Chen, G. Corrado, and J. Dean.\nE\ufb03cient estimation of word representations in vector\nspace. CoRR, abs/1301.3781, 2013.\n[42] I. Mironov. R\u00b4enyi di\ufb00erential privacy. Private\ncommunication, 2016.\n[43] Y. Nesterov. Introductory Lectures on Convex\nOptimization. A Basic Course. Springer, 2004.\n[44] J. Pennington, R. Socher, and C. D. Manning. GloVe:\nGlobal vectors for word representation. In EMNLP,\npages 1532\u20131543, 2014.\n[45] N. Phan, Y. Wang, X. Wu, and D. Dou. Di\ufb00erential\nprivacy preservation for deep auto-encoders: an\napplication of human behavior prediction. In AAAI,\npages 1309\u20131316, 2016.\n[46] N. Pinto, Z. Stone, T. E. Zickler, and D. Cox. Scaling\nup biologically-inspired computer vision: A case study\nin unconstrained face recognition on Facebook. In\nCVPR, pages 35\u201342. IEEE, 2011.\n[47] R. M. Rogers, A. Roth, J. Ullman, and S. P. Vadhan.\nPrivacy odometers and \ufb01lters: Pay-as-you-go\ncomposition. In NIPS.\n[48] D. E. Rumelhart, G. E. Hinton, and R. J. Williams.\nLearning representations by back-propagating errors.\nNature, 323:533\u2013536, Oct. 1986.\n[49] A. Saxe, P. W. Koh, Z. Chen, M. Bhand, B. Suresh,\nand A. Ng. On random weights and unsupervised\nfeature learning. In ICML, pages 1089\u20131096. ACM,\n2011.\n[50] R. Shokri and V. Shmatikov. Privacy-preserving deep\nlearning. In CCS, pages 1310\u20131321. ACM, 2015.\n[51] D. Silver, A. Huang, C. J. Maddison, A. Guez,\nL. Sifre, G. van den Driessche, J. Schrittwieser,\nI. Antonoglou, V. Panneershelvam, M. Lanctot,\nS. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner,\nI. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu,\nT. Graepel, and D. Hassabis. Mastering the game of\nGo with deep neural networks and tree search. Nature,\n529(7587):484\u2013489, 2016.\n[52] S. Song, K. Chaudhuri, and A. Sarwate. Stochastic\ngradient descent with di\ufb00erentially private updates. In\nGlobalSIP Conference, 2013.\n[53] L. Sweeney. k-anonymity: A model for protecting\nprivacy. International J. of Uncertainty, Fuzziness and\nKnowledge-Based Systems, 10(05):557\u2013570, 2002.\n[54] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,\nD. Anguelov, D. Erhan, V. Vanhoucke, and\nA. Rabinovich. Going deeper with convolutions. In\nCVPR, pages 1\u20139. IEEE, 2015.\n[55] S. Tu, R. Roelofs, S. Venkataraman, and B. Recht.\nLarge scale kernel learning using block coordinate\ndescent. CoRR, abs/1602.05310, 2016.\n[56] O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever,\nand G. E. Hinton. Grammar as a foreign language. In\nNIPS, pages 2773\u20132781, 2015.\n[57] X. Wu, A. Kumar, K. Chaudhuri, S. Jha, and J. F.\nNaughton. Di\ufb00erentially private stochastic gradient\ndescent for in-RDBMS analytics. CoRR,\nabs/1606.04722, 2016.\nAPPENDIX\nA.\nPROOF OF THEOREM 2\nHere we restate and prove Theorem 2.\nTheorem 2. Let \u03b1M(\u03bb) de\ufb01ned as\n\u03b1M(\u03bb)\n\u2206= max\naux,d,d\u2032 \u03b1M(\u03bb; aux, d, d\u2032),\nwhere the maximum is taken over all auxiliary inputs and\nneighboring databases d, d\u2032. Then\n1. [Composability] Suppose that a mechanism M con-\nsists of a sequence of adaptive mechanisms M1, . . . , Mk\nwhere Mi : Qi\u22121\nj=1 Rj \u00d7 D \u2192Ri. Then, for any \u03bb\n\u03b1M(\u03bb) \u2264\nk\nX\ni=1\n\u03b1Mi(\u03bb) .\n2. [Tail bound] For any \u03b5 > 0, the mechanism M is\n(\u03b5, \u03b4)-di\ufb00erentially private for\n\u03b4 = min\n\u03bb exp(\u03b1M(\u03bb) \u2212\u03bb\u03b5) .\nProof. Composition of moments.\nFor brevity, let\nM1:i denote (M1, . . . , Mi), and similarly let o1:i denote\n(o1, . . . , oi). For neighboring databases d, d\u2032 \u2208Dn, and a\nsequence of outcomes o1, . . . , ok we write\nc(o1:k; M1:k, o1:(k\u22121), d, d\u2032)\n= log Pr[M1:k(d; o1:(k\u22121)) = o1:k]\nPr[M1:k(d\u2032; o1:(k\u22121)) = o1:k]\n= log\nk\nY\ni=1\nPr[Mi(d) = oi | M1:(i\u22121)(d) = o1:(i\u22121)]\nPr[Mi(d\u2032) = oi | M1:(i\u22121)(d\u2032) = o1:(i\u22121)]\n=\nk\nX\ni=1\nlog Pr[Mi(d) = oi | M1:(i\u22121)(d) = o1:(i\u22121)]\nPr[Mi(d\u2032) = oi | M1:(i\u22121)(d\u2032) = o1:(i\u22121)]\n=\nk\nX\ni=1\nc(oi; Mi, o1:(i\u22121), d, d\u2032).\nThus\nEo\u2032\n1:k\u223cM1:k(d)\n\u0002\nexp(\u03bbc(o\u2032\n1:k; M1:k, d, d\u2032))\n\f\f \u2200i < k: o\u2032\ni = oi\n\u0003\n= Eo\u2032\n1:k\u223cM1:k(d)\n\"\nexp\n \n\u03bb\nk\nX\ni=1\nc(o\u2032\ni; Mi, o1:(i\u22121), d, d\u2032)\n!#\n= Eo\u2032\n1:k\u223cM1:k(d)\n\" k\nY\ni=1\nexp\n\u0000\u03bbc(o\u2032\ni; Mi, o1:(i\u22121), d, d\u2032)\n\u0001\n#\n(by independence of noise)\n=\nk\nY\ni=1\nEo\u2032\ni\u223cMi(d)\n\u0002\nexp(\u03bbc(o\u2032\ni; Mi, o1:(i\u22121), d, d\u2032))\n\u0003\n=\nk\nY\ni=1\nexp\n\u0000\u03b1Mi(\u03bb; o1:(i\u22121), d, d\u2032)\n\u0001\n= exp\n k\nX\ni=1\n\u03b1i(\u03bb; o1:(i\u22121), d, d\u2032)\n!\n.\nThe claim follows.\nTail bound by moments. The proof is based on the stan-\ndard Markov\u2019s inequality argument used in proofs of mea-\nsure concentration. We have\nPr\no\u223cM(d)[c(o) \u2265\u03b5]\n=\nPr\no\u223cM(d)[exp(\u03bbc(o)) \u2265exp(\u03bb\u03b5))]\n\u2264Eo\u223cM(d)[exp(\u03bbc(o))]\nexp(\u03bb\u03b5)\n\u2264exp(\u03b1 \u2212\u03bb\u03b5).\nLet B = {o: c(o) \u2265\u03b5}. Then for any S,\nPr[M(d) \u2208S]\n= Pr[M(d) \u2208S \u2229Bc] + Pr[M(d) \u2208S \u2229B]\n\u2264exp(\u03b5) Pr[M(d\u2032) \u2208S \u2229Bc] + Pr[M(d) \u2208B]\n\u2264exp(\u03b5) Pr[M(d\u2032) \u2208S] + exp(\u03b1 \u2212\u03bb\u03b5).\nThe second part follows by an easy calculation.\nThe proof demonstrates a tail bound on the privacy loss,\nmaking it stronger than di\ufb00erential privacy for a \ufb01xed value\nof \u03b5, \u03b4.\nB.\nPROOF OF LEMMA 3\nThe proof of the main theorem relies on the following mo-\nments bound on Gaussian mechanism with random sam-\npling.\nLemma 3. Suppose that f : D \u2192Rp with \u2225f(\u00b7)\u22252 \u22641.\nLet \u03c3 \u22651 and let J be a sample from [n] where each i \u2208[n]\nis chosen independently with probability q <\n1\n16\u03c3 . Then for\nany positive integer \u03bb \u2264\u03c32 ln\n1\nq\u03c3 , the mechanism M(d) =\nP\ni\u2208J f(di) + N(0, \u03c32I) satis\ufb01es\n\u03b1M(\u03bb) \u2264q2\u03bb(\u03bb + 1)\n(1 \u2212q)\u03c32 + O(q3\u03bb3/\u03c33).\nProof. Fix d\u2032 and let d = d\u2032 \u222a{dn}. Without loss of\ngenerality, f(dn) = e1 and P\ni\u2208J\\[n] f(di) = 0. Thus M(d)\nand M(d\u2032) are distributed identically except for the \ufb01rst\ncoordinate and hence we have a one-dimensional problem.\nLet \u00b50 denote the pdf of N(0, \u03c32) and let \u00b51 denote the pdf\nof N(1, \u03c32). Thus:\nM(d\u2032) \u223c\u00b50,\nM(d) \u223c\u00b5\n\u2206= (1 \u2212q)\u00b50 + q\u00b51.\nWe want to show that\nEz\u223c\u00b5[(\u00b5(z)/\u00b50(z))\u03bb] \u2264\u03b1,\nand Ez\u223c\u00b50[(\u00b50(z)/\u00b5(z))\u03bb] \u2264\u03b1,\nfor some explicit \u03b1 to be determined later.\nWe will use the same method to prove both bounds. As-\nsume we have two distributions \u03bd0 and \u03bd1, and we wish to\nbound\nEz\u223c\u03bd0[(\u03bd0(z)/\u03bd1(z))\u03bb] = Ez\u223c\u03bd1[(\u03bd0(z)/\u03bd1(z))\u03bb+1] .\nUsing binomial expansion, we have\nEz\u223c\u03bd1[(\u03bd0(z)/\u03bd1(z))\u03bb+1]\n= Ez\u223c\u03bd1[(1 + (\u03bd0(z) \u2212\u03bd1(z))/\u03bd1(z))\u03bb+1]\n= Ez\u223c\u03bd1[(1 + (\u03bd0(z) \u2212\u03bd1(z))/\u03bd1(z))\u03bb+1]\n=\n\u03bb+1\nX\nt=0\n \n\u03bb + 1\nt\n!\nEz\u223c\u03bd1[((\u03bd0(z) \u2212\u03bd1(z))/\u03bd1(z))t] .\n(5)\nThe \ufb01rst term in (5) is 1, and the second term is\nEz\u223c\u03bd1\n\u0014\u03bd0(z) \u2212\u03bd1(z)\n\u03bd1(z)\n\u0015\n=\nZ \u221e\n\u2212\u221e\n\u03bd1(z)\u03bd0(z) \u2212\u03bd1(z)\n\u03bd1(z)\ndz\n=\nZ \u221e\n\u2212\u221e\n\u03bd0(z) dz \u2212\nZ \u221e\n\u2212\u221e\n\u03bd1(z) dz\n= 1 \u22121 = 0.\nTo prove the lemma it su\ufb03ces to show show that for both\n\u03bd0 = \u00b5, \u03bd1 = \u00b50 and \u03bd0 = \u00b50, \u03bd1 = \u00b5, the third term is\nbounded by q2\u03bb(\u03bb + 1)/(1 \u2212q)\u03c32 and that this bound dom-\ninates the sum of the remaining terms. We will prove the\nmore di\ufb03cult second case (\u03bd0 = \u00b50, \u03bd1 = \u00b5); the proof of the\nother case is similar.\nTo upper bound the third term in (5), we note that \u00b5(z) \u2265\n(1 \u2212q)\u00b50(z), and write\nEz\u223c\u00b5\n\"\u0012\u00b50(z) \u2212\u00b5(z)\n\u00b5(z)\n\u00132#\n= q2Ez\u223c\u00b5\n\"\u0012\u00b50(z) \u2212\u00b51(z)\n\u00b5(z)\n\u00132#\n= q2\nZ \u221e\n\u2212\u221e\n(\u00b50(z) \u2212\u00b51(z))2\n\u00b5(z)\ndz\n\u2264\nq2\n1 \u2212q\nZ \u221e\n\u2212\u221e\n(\u00b50(z) \u2212\u00b51(z))2\n\u00b50(z)\ndz\n=\nq2\n1 \u2212q Ez\u223c\u00b50\n\"\u0012\u00b50(z) \u2212\u00b51(z)\n\u00b50(z)\n\u00132#\n.\nAn easy fact is that for any a \u2208R, Ez\u223c\u00b50 exp(2az/2\u03c32) =\nexp(a2/2\u03c32). Thus,\nEz\u223c\u00b50\n\"\u0012\u00b50(z) \u2212\u00b51(z)\n\u00b50(z)\n\u00132#\n= Ez\u223c\u00b50\n\"\u0012\n1 \u2212exp(2z \u22121\n2\u03c32 )\n\u00132#\n= 1 \u22122Ez\u223c\u00b50\n\u0014\nexp(2z \u22121\n2\u03c32 )\n\u0015\n+ Ez\u223c\u00b50\n\u0014\nexp(4z \u22122\n2\u03c32 )\n\u0015\n= 1 \u22122 exp\n\u0012 1\n2\u03c32\n\u0013\n\u00b7 exp\n\u0012 \u22121\n2\u03c32\n\u0013\n+ exp\n\u0012 4\n2\u03c32\n\u0013\n\u00b7 exp\n\u0012 \u22122\n2\u03c32\n\u0013\n= exp(1/\u03c32) \u22121.\nThus the third term in the binomial expansion (5)\n \n1 + \u03bb\n2\n!\nEz\u2208\u00b5\n\"\u0012\u00b50(z) \u2212\u00b5(z)\n\u00b5(z)\n\u00132#\n\u2264\u03bb(\u03bb + 1)q2\n(1 \u2212q)\u03c32 .\nTo bound the remaining terms, we \ufb01rst note that by stan-\ndard calculus, we get:\n\u2200z \u22640 : |\u00b50(z) \u2212\u00b51(z)| \u2264\u2212(z \u22121)\u00b50(z)/\u03c32,\n\u2200z \u22651 : |\u00b50(z) \u2212\u00b51(z)| \u2264z\u00b51(z)/\u03c32,\n\u22000 \u2264z \u22641 : |\u00b50(z) \u2212\u00b51(z)| \u2264\u00b50(z)(exp(1/2\u03c32) \u22121)\n\u2264\u00b50(z)/\u03c32.\nWe can then write\nEz\u223c\u00b5\n\"\u0012\u00b50(z) \u2212\u00b5(z)\n\u00b5(z)\n\u0013t#\n\u2264\nZ 0\n\u2212\u221e\n\u00b5(z)\n\f\f\f\f\f\n\u0012\u00b50(z) \u2212\u00b5(z)\n\u00b5(z)\n\u0013t\f\f\f\f\f dz\n+\nZ 1\n0\n\u00b5(z)\n\f\f\f\f\f\n\u0012\u00b50(z) \u2212\u00b5(z)\n\u00b5(z)\n\u0013t\f\f\f\f\f dz\n+\nZ \u221e\n1\n\u00b5(z)\n\f\f\f\f\f\n\u0012\u00b50(z) \u2212\u00b5(z)\n\u00b5(z)\n\u0013t\f\f\f\f\f dz.\nWe consider these terms individually. We repeatedly make\nuse of three observations: (1) \u00b50 \u2212\u00b5 = q(\u00b50 \u2212\u00b51), (2)\n\u00b5 \u2265(1 \u2212q)\u00b50, and (3) E\u00b50[|z|t] \u2264\u03c3t(t \u22121)!!. The \ufb01rst term\ncan then be bounded by\nqt\n(1 \u2212q)t\u22121\u03c32t\nZ 0\n\u2212\u221e\n\u00b50(z)|z \u22121|t dz\n\u2264(2q)t(t \u22121)!!\n2(1 \u2212q)t\u22121\u03c3t .\nThe second term is at most\nqt\n(1 \u2212q)t\nZ 1\n0\n\u00b5(z)\n\f\f\f\f\f\n\u0012\u00b50(z) \u2212\u00b51(z)\n\u00b50(z)\n\u0013t\f\f\f\f\f dz\n\u2264\nqt\n(1 \u2212q)t\nZ 1\n0\n\u00b5(z) 1\n\u03c32t dz\n\u2264\nqt\n(1 \u2212q)t\u03c32t .\nSimilarly, the third term is at most\nqt\n(1 \u2212q)t\u22121\u03c32t\nZ \u221e\n1\n\u00b50(z)\n\u0012z\u00b51(z)\n\u00b50(z)\n\u0013t\ndz\n\u2264\nqt\n(1 \u2212q)t\u22121\u03c32t\nZ \u221e\n1\n\u00b50(z) exp((2tz \u2212t)/2\u03c32)zt dz\n\u2264qt exp((t2 \u2212t)/2\u03c32)\n(1 \u2212q)t\u22121\u03c32t\nZ \u221e\n0\n\u00b50(z \u2212t)zt dz\n\u2264(2q)t exp((t2 \u2212t)/2\u03c32)(\u03c3t(t \u22121)!! + tt)\n2(1 \u2212q)t\u22121\u03c32t\n.\nUnder the assumptions on q, \u03c3, and \u03bb, it is easy to check\nthat the three terms, and their sum, drop o\ufb00geometrically\nfast in t for t > 3.\nHence the binomial expansion (5) is\ndominated by the t = 3 term, which is O(q3\u03bb3/\u03c33). The\nclaim follows.\nTo derive Theorem 1, we use the above moments bound\nalong with the tail bound from Theorem 2, optimizing over\nthe choice of \u03bb.\nTheorem 1. There exist constants c1 and c2 so that given\nthe sampling probability q = L/N and the number of steps\nT, for any \u03b5 < c1q2T, Algorithm 1 is (\u03b5, \u03b4)-di\ufb00erentially\nprivate for any \u03b4 > 0 if we choose\n\u03c3 \u2265c2\nq\np\nT log(1/\u03b4)\n\u03b5\n.\nProof. Assume for now that \u03c3, \u03bb satisfy the conditions\nin Lemma 3. By Theorem 2.1 and Lemma 3, the log mo-\nment of Algorithm 1 can be bounded as follows \u03b1(\u03bb) \u2264\nTq2\u03bb2/\u03c32. By Theorem 2, to guarantee Algorithm 1 to be\n(\u03b5, \u03b4)-di\ufb00erentially private, it su\ufb03ces that\nTq2\u03bb2/\u03c32 \u2264\u03bb\u03b5/2 ,\nexp(\u2212\u03bb\u03b5/2) \u2264\u03b4 .\nIn addition, we need\n\u03bb \u2264\u03c32 log(1/q\u03c3) .\nIt is now easy to verify that when \u03b5 = c1q2T, we can satisfy\nall these conditions by setting\n\u03c3 = c2\nq\np\nT log(1/\u03b4)\n\u03b5\nfor some explicit constants c1 and c2.\nC.\nFROM DIFFERENTIAL PRIVACY TO MO-\nMENTS BOUNDS\nOne can also translate a di\ufb00erential privacy guarantee into\na moment bound.\nLemma C.1. Let M be \u03b5-di\ufb00erentially private. Then for\nany \u03bb > 0, M satis\ufb01es\n\u03b1\u03bb \u2264\u03bb\u03b5(e\u03b5 \u22121) + \u03bb2\u03b52e2\u03b5/2.\nProof. Let Z denote the random variable c(M(d)). Then\ndi\ufb00erential privacy implies that\n\u2022 \u00b5\n\u2206= E[Z] \u2264\u03b5(exp(\u03b5) \u22121).\n\u2022 |Z| \u2264\u03b5, so that |Z \u2212\u00b5| \u2264\u03b5 exp(\u03b5).\nThen E[exp(\u03bbZ)] = exp(\u03bb\u00b5)\u00b7E[exp(\u03bb(Z \u2212\u00b5))]. Since Z is in\na bounded range [\u2212\u03b5 exp(\u03b5), \u03b5 exp(\u03b5)] and f(x) = exp(\u03bbx) is\nconvex, we can bound f(x) by a linear interpolation between\nthe values at the two endpoints of the range. Basic calculus\nthen implies that\nE[f(Z)] \u2264f(E[Z]) \u00b7 exp(\u03bb2\u03b52 exp(2\u03b5)/2),\nwhich concludes the proof.\nLemma C.1 and Theorem 2 give a way of getting a compo-\nsition theorem for di\ufb00erentially private mechanisms, which\nis roughly equivalent to unrolling the proof of the strong\ncomposition theorem of [22]. The power of the moments ac-\ncountant comes from the fact that, for many mechanisms of\nchoice, directly bounding in the moments gives a stronger\nguarantee than one would get by establishing di\ufb00erential\nprivacy and applying Lemma C.1.\nD.\nHYPERPARAMETER SEARCH\nHere we state Theorem 10.2 from [27] that we use to ac-\ncount for the cost of hyperparameter search.\nTheorem D.1\n(Gupta et al. [27]). Let M be an \u03b5 -\ndi\ufb00erentially private mechanism such that for a query func-\ntion q with sensitivity 1, and a parameter Q, it holds that\nPrr\u223cM(d)[q(d, r) \u2265Q] \u2265p for some p \u2208(0, 1). Then for any\n\u03b4 > 0 and any \u03b5\u2032 \u2208(0, 1\n2), there is a mechanism M \u2032 which\nsatis\ufb01es the following properties:\n\u2022 Prr\u223cM\u2032(d)\nh\nq(d, r) \u2265Q \u22124\n\u03b5\u2032 log(\n1\n\u03b5\u2032\u03b4p)\ni\n\u22651 \u2212\u03b4.\n\u2022 M \u2032 makes (\n1\n\u03b5\u2032\u03b4p)2 log(\n1\n\u03b5\u2032\u03b4p) calls to M.\n\u2022 M \u2032 is (\u03b5 + 8\u03b5\u2032)-di\ufb00erentially private.\nSuppose that we have a di\ufb00erentially private mechanism\nMi for each of K choices of hyperparameters. Let \u02dc\nM be the\nmechanism that picks a random choice of hyperparameters,\nand runs the corresponding Mi. Let q(d, r) denote the num-\nber of examples from the validation set the r labels correctly,\nand let Q be a target accuracy. Assuming that one of the\nhyperparameter settings gets accuracy at least Q, \u02dc\nM satis-\n\ufb01es the pre-conditions of the theorem for p =\n1\nK . Then with\nhigh probability, the mechanism implied by the theorem gets\naccuracy close to Q.\nWe remark that the proof of Theo-\nrem D.1 actually implies a stronger max(\u03b5, 8\u03b5\u2032)-di\ufb00erential\nprivacy for the setting of interest here.\nPutting in some numbers, for a target accuracy of 95% on\na validation set of size 10,000, we get Q = 9, 500. Thus, if,\nfor instance, we allow \u03b5\u2032 = 0.5, and \u03b4 = 0.05, we lose at most\n1% in accuracy as long as 100 > 8 ln 40\np . This is satis\ufb01ed as\nlong as p \u2265\n1\n6700. In other words, one can try 6,700 di\ufb00erent\nparameter settings at privacy cost \u03b5 = 4 for the validation\nset. In our experiments, we tried no more than a hundred\nsettings, so that this bound is easily satis\ufb01ed. In practice,\nas our graphs show, p for our hyperparameter search is sig-\nni\ufb01cantly larger than\n1\nK , so that a slightly smaller \u03b5\u2032 should\nsu\ufb03ce.\n"
    },
    {
        "pdf_file": "paper6.pdf",
        "text": "THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n1\nObject Detection with Deep Learning: A Review\nZhong-Qiu Zhao, Member, IEEE, Peng Zheng,\nShou-tao Xu, and Xindong Wu, Fellow, IEEE\nAbstract\u2014Due to object detection\u2019s close relationship with\nvideo analysis and image understanding, it has attracted much\nresearch attention in recent years. Traditional object detection\nmethods are built on handcrafted features and shallow trainable\narchitectures. Their performance easily stagnates by constructing\ncomplex ensembles which combine multiple low-level image\nfeatures with high-level context from object detectors and scene\nclassi\ufb01ers. With the rapid development in deep learning, more\npowerful tools, which are able to learn semantic, high-level,\ndeeper features, are introduced to address the problems existing\nin traditional architectures. These models behave differently\nin network architecture, training strategy and optimization\nfunction, etc. In this paper, we provide a review on deep\nlearning based object detection frameworks. Our review begins\nwith a brief introduction on the history of deep learning and\nits representative tool, namely Convolutional Neural Network\n(CNN). Then we focus on typical generic object detection\narchitectures along with some modi\ufb01cations and useful tricks\nto improve detection performance further. As distinct speci\ufb01c\ndetection tasks exhibit different characteristics, we also brie\ufb02y\nsurvey several speci\ufb01c tasks, including salient object detection,\nface detection and pedestrian detection. Experimental analyses\nare also provided to compare various methods and draw some\nmeaningful conclusions. Finally, several promising directions and\ntasks are provided to serve as guidelines for future work in\nboth object detection and relevant neural network based learning\nsystems.\nIndex Terms\u2014deep learning, object detection, neural network\nI. INTRODUCTION\nT\nO gain a complete image understanding, we should not\nonly concentrate on classifying different images, but\nalso try to precisely estimate the concepts and locations of\nobjects contained in each image. This task is referred as object\ndetection [1][S1], which usually consists of different subtasks\nsuch as face detection [2][S2], pedestrian detection [3][S2]\nand skeleton detection [4][S3]. As one of the fundamental\ncomputer vision problems, object detection is able to provide\nvaluable information for semantic understanding of images\nand videos, and is related to many applications, including\nimage classi\ufb01cation [5], [6], human behavior analysis [7][S4],\nface recognition [8][S5] and autonomous driving [9], [10].\nMeanwhile, Inheriting from neural networks and related learn-\ning systems, the progress in these \ufb01elds will develop neural\nnetwork algorithms, and will also have great impacts on object\ndetection techniques which can be considered as learning\nsystems. [11]\u2013[14][S6]. However, due to large variations in\nviewpoints, poses, occlusions and lighting conditions, it\u2019s dif\ufb01-\ncult to perfectly accomplish object detection with an additional\nZhong-Qiu Zhao, Peng Zheng and Shou-Tao Xu are with the College of\nComputer Science and Information Engineering, Hefei University of Technol-\nogy, China. Xindong Wu is with the School of Computing and Informatics,\nUniversity of Louisiana at Lafayette, USA.\nManuscript received August xx, 2017; revised xx xx, 2017.\nobject localization task. So much attention has been attracted\nto this \ufb01eld in recent years [15]\u2013[18].\nThe problem de\ufb01nition of object detection is to determine\nwhere objects are located in a given image (object localization)\nand which category each object belongs to (object classi\ufb01ca-\ntion). So the pipeline of traditional object detection models\ncan be mainly divided into three stages: informative region\nselection, feature extraction and classi\ufb01cation.\nInformative region selection. As different objects may appear\nin any positions of the image and have different aspect ratios\nor sizes, it is a natural choice to scan the whole image with a\nmulti-scale sliding window. Although this exhaustive strategy\ncan \ufb01nd out all possible positions of the objects, its short-\ncomings are also obvious. Due to a large number of candidate\nwindows, it is computationally expensive and produces too\nmany redundant windows. However, if only a \ufb01xed number of\nsliding window templates are applied, unsatisfactory regions\nmay be produced.\nFeature extraction. To recognize different objects, we need\nto extract visual features which can provide a semantic and\nrobust representation. SIFT [19], HOG [20] and Haar-like [21]\nfeatures are the representative ones. This is due to the fact\nthat these features can produce representations associated with\ncomplex cells in human brain [19]. However, due to the diver-\nsity of appearances, illumination conditions and backgrounds,\nit\u2019s dif\ufb01cult to manually design a robust feature descriptor to\nperfectly describe all kinds of objects.\nClassi\ufb01cation. Besides, a classi\ufb01er is needed to distinguish\na target object from all the other categories and to make the\nrepresentations more hierarchical, semantic and informative\nfor visual recognition. Usually, the Supported Vector Machine\n(SVM) [22], AdaBoost [23] and Deformable Part-based Model\n(DPM) [24] are good choices. Among these classi\ufb01ers, the\nDPM is a \ufb02exible model by combining object parts with\ndeformation cost to handle severe deformations. In DPM, with\nthe aid of a graphical model, carefully designed low-level\nfeatures and kinematically inspired part decompositions are\ncombined. And discriminative learning of graphical models\nallows for building high-precision part-based models for a\nvariety of object classes.\nBased on these discriminant local feature descriptors and\nshallow learnable architectures, state of the art results have\nbeen obtained on PASCAL VOC object detection competition\n[25] and real-time embedded systems have been obtained with\na low burden on hardware. However, small gains are obtained\nduring 2010-2012 by only building ensemble systems and\nemploying minor variants of successful methods [15]. This fact\nis due to the following reasons: 1) The generation of candidate\nbounding boxes with a sliding window strategy is redundant,\ninef\ufb01cient and inaccurate. 2) The semantic gap cannot be\narXiv:1807.05511v2  [cs.CV]  16 Apr 2019\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n2\nPedestrian \ndetection\nSalient object \ndetection \nFace\ndetection \nGeneric object \ndetection\nObject \ndetection\nBounding box \nregression\nLocal contrast \nSegmentation\nMulti-feature\nBoosting forest\nMulti-scale\nadaption\nFig. 1. The application domains of object detection.\nbridged by the combination of manually engineered low-level\ndescriptors and discriminatively-trained shallow models.\nThanks to the emergency of Deep Neural Networks (DNNs)\n[6][S7], a more signi\ufb01cant gain is obtained with the introduc-\ntion of Regions with CNN features (R-CNN) [15]. DNNs, or\nthe most representative CNNs, act in a quite different way from\ntraditional approaches. They have deeper architectures with the\ncapacity to learn more complex features than the shallow ones.\nAlso the expressivity and robust training algorithms allow to\nlearn informative object representations without the need to\ndesign features manually [26].\nSince the proposal of R-CNN, a great deal of improved\nmodels have been suggested, including Fast R-CNN which\njointly optimizes classi\ufb01cation and bounding box regression\ntasks [16], Faster R-CNN which takes an additional sub-\nnetwork to generate region proposals [18] and YOLO which\naccomplishes object detection via a \ufb01xed-grid regression [17].\nAll of them bring different degrees of detection performance\nimprovements over the primary R-CNN and make real-time\nand accurate object detection become more achievable.\nIn this paper, a systematic review is provided to summarise\nrepresentative models and their different characteristics in\nseveral application domains, including generic object detec-\ntion [15], [16], [18], salient object detection [27], [28], face\ndetection [29]\u2013[31] and pedestrian detection [32], [33]. Their\nrelationships are depicted in Figure 1. Based on basic CNN ar-\nchitectures, generic object detection is achieved with bounding\nbox regression, while salient object detection is accomplished\nwith local contrast enhancement and pixel-level segmentation.\nFace detection and pedestrian detection are closely related\nto generic object detection and mainly accomplished with\nmulti-scale adaption and multi-feature fusion/boosting forest,\nrespectively. The dotted lines indicate that the corresponding\ndomains are associated with each other under certain con-\nditions. It should be noticed that the covered domains are\ndiversi\ufb01ed. Pedestrian and face images have regular structures,\nwhile general objects and scene images have more complex\nvariations in geometric structures and layouts. Therefore,\ndifferent deep models are required by various images.\nThere has been a relevant pioneer effort [34] which mainly\nfocuses on relevant software tools to implement deep learning\ntechniques for image classi\ufb01cation and object detection, but\npays little attention on detailing speci\ufb01c algorithms. Different\nfrom it, our work not only reviews deep learning based object\ndetection models and algorithms covering different applica-\ntion domains in detail, but also provides their corresponding\nexperimental comparisons and meaningful analyses.\nThe rest of this paper is organized as follows. In Section\n2, a brief introduction on the history of deep learning and the\nbasic architecture of CNN is provided. Generic object detec-\ntion architectures are presented in Section 3. Then reviews\nof CNN applied in several speci\ufb01c tasks, including salient\nobject detection, face detection and pedestrian detection, are\nexhibited in Section 4-6, respectively. Several promising future\ndirections are proposed in Section 7. At last, some concluding\nremarks are presented in Section 8.\nII. A BRIEF OVERVIEW OF DEEP LEARNING\nPrior to overview on deep learning based object detection\napproaches, we provide a review on the history of deep\nlearning along with an introduction on the basic architecture\nand advantages of CNN.\nA. The History: Birth, Decline and Prosperity\nDeep models can be referred to as neural networks with\ndeep structures. The history of neural networks can date back\nto 1940s [35], and the original intention was to simulate the\nhuman brain system to solve general learning problems in a\nprincipled way. It was popular in 1980s and 1990s with the\nproposal of back-propagation algorithm by Hinton et al. [36].\nHowever, due to the over\ufb01tting of training, lack of large scale\ntraining data, limited computation power and insigni\ufb01cance\nin performance compared with other machine learning tools,\nneural networks fell out of fashion in early 2000s.\nDeep learning has become popular since 2006 [37][S7] with\na break through in speech recognition [38]. The recovery of\ndeep learning can be attributed to the following factors.\n\u2022 The emergence of large scale annotated training data, such\nas ImageNet [39], to fully exhibit its very large learning\ncapacity;\n\u2022 Fast development of high performance parallel computing\nsystems, such as GPU clusters;\n\u2022 Signi\ufb01cant advances in the design of network structures\nand training strategies. With unsupervised and layerwise\npre-training guided by Auto-Encoder (AE) [40] or Re-\nstricted Boltzmann Machine (RBM) [41], a good initializa-\ntion is provided. With dropout and data augmentation, the\nover\ufb01tting problem in training has been relieved [6], [42].\nWith batch normalization (BN), the training of very deep\nneural networks becomes quite ef\ufb01cient [43]. Meanwhile,\nvarious network structures, such as AlexNet [6], Overfeat\n[44], GoogLeNet [45], VGG [46] and ResNet [47], have\nbeen extensively studied to improve the performance.\nWhat prompts deep learning to have a huge impact on the\nentire academic community? It may owe to the contribution of\nHinton\u2019s group, whose continuous efforts have demonstrated\nthat deep learning would bring a revolutionary breakthrough\non grand challenges rather than just obvious improvements on\nsmall datasets. Their success results from training a large CNN\non 1.2 million labeled images together with a few techniques\n[6] (e.g., ReLU operation [48] and \u2018dropout\u2019 regularization).\nB. Architecture and Advantages of CNN\nCNN is the most representative model of deep learning [26].\nA typical CNN architecture, which is referred to as VGG16,\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n3\ncan be found in Fig. S1. Each layer of CNN is known as a\nfeature map. The feature map of the input layer is a 3D matrix\nof pixel intensities for different color channels (e.g. RGB). The\nfeature map of any internal layer is an induced multi-channel\nimage, whose \u2018pixel\u2019 can be viewed as a speci\ufb01c feature. Every\nneuron is connected with a small portion of adjacent neurons\nfrom the previous layer (receptive \ufb01eld). Different types of\ntransformations [6], [49], [50] can be conducted on feature\nmaps, such as \ufb01ltering and pooling. Filtering (convolution)\noperation convolutes a \ufb01lter matrix (learned weights) with\nthe values of a receptive \ufb01eld of neurons and takes a non-\nlinear function (such as sigmoid [51], ReLU) to obtain \ufb01nal\nresponses. Pooling operation, such as max pooling, average\npooling, L2-pooling and local contrast normalization [52],\nsummaries the responses of a receptive \ufb01eld into one value\nto produce more robust feature descriptions.\nWith an interleave between convolution and pooling, an\ninitial feature hierarchy is constructed, which can be \ufb01ne-tuned\nin a supervised manner by adding several fully connected (FC)\nlayers to adapt to different visual tasks. According to the tasks\ninvolved, the \ufb01nal layer with different activation functions [6]\nis added to get a speci\ufb01c conditional probability for each\noutput neuron. And the whole network can be optimized on\nan objective function (e.g. mean squared error or cross-entropy\nloss) via the stochastic gradient descent (SGD) method. The\ntypical VGG16 has totally 13 convolutional (conv) layers, 3\nfully connected layers, 3 max-pooling layers and a softmax\nclassi\ufb01cation layer. The conv feature maps are produced by\nconvoluting 3*3 \ufb01lter windows, and feature map resolutions\nare reduced with 2 stride max-pooling layers. An arbitrary test\nimage of the same size as training samples can be processed\nwith the trained network. Re-scaling or cropping operations\nmay be needed if different sizes are provided [6].\nThe advantages of CNN against traditional methods can be\nsummarised as follows.\n\u2022 Hierarchical feature representation, which is the multi-\nlevel representations from pixel to high-level semantic fea-\ntures learned by a hierarchical multi-stage structure [15],\n[53], can be learned from data automatically and hidden\nfactors of input data can be disentangled through multi-level\nnonlinear mappings.\n\u2022 Compared with traditional shallow models, a deeper\narchitecture provides an exponentially increased expressive\ncapability.\n\u2022 The architecture of CNN provides an opportunity to\njointly optimize several related tasks together (e.g. Fast R-\nCNN combines classi\ufb01cation and bounding box regression\ninto a multi-task leaning manner).\n\u2022 Bene\ufb01tting from the large learning capacity of deep\nCNNs, some classical computer vision challenges can be\nrecast as high-dimensional data transform problems and\nsolved from a different viewpoint.\nDue to these advantages, CNN has been widely applied\ninto many research \ufb01elds, such as image super-resolution\nreconstruction [54], [55], image classi\ufb01cation [5], [56], im-\nage retrieval [57], [58], face recognition [8][S5], pedestrian\ndetection [59]\u2013[61] and video analysis [62], [63].\nIII. GENERIC OBJECT DETECTION\nGeneric object detection aims at locating and classifying\nexisting objects in any one image, and labeling them with\nrectangular bounding boxes to show the con\ufb01dences of exis-\ntence. The frameworks of generic object detection methods\ncan mainly be categorized into two types (see Figure 2).\nOne follows traditional object detection pipeline, generating\nregion proposals at \ufb01rst and then classifying each proposal into\ndifferent object categories. The other regards object detection\nas a regression or classi\ufb01cation problem, adopting a uni\ufb01ed\nframework to achieve \ufb01nal results (categories and locations)\ndirectly. The region proposal based methods mainly include\nR-CNN [15], SPP-net [64], Fast R-CNN [16], Faster R-CNN\n[18], R-FCN [65], FPN [66] and Mask R-CNN [67], some of\nwhich are correlated with each other (e.g. SPP-net modi\ufb01es R-\nCNN with a SPP layer). The regression/classi\ufb01cation based\nmethods mainly includes MultiBox [68], AttentionNet [69],\nG-CNN [70], YOLO [17], SSD [71], YOLOv2 [72], DSSD\n[73] and DSOD [74]. The correlations between these two\npipelines are bridged by the anchors introduced in Faster R-\nCNN. Details of these methods are as follows.\nA. Region Proposal Based Framework\nThe region proposal based framework, a two-step process,\nmatches the attentional mechanism of human brain to some\nextent, which gives a coarse scan of the whole scenario \ufb01rstly\nand then focuses on regions of interest. Among the pre-related\nworks [44], [75], [76], the most representative one is Overfeat\n[44]. This model inserts CNN into sliding window method,\nwhich predicts bounding boxes directly from locations of\nthe topmost feature map after obtaining the con\ufb01dences of\nunderlying object categories.\n1) R-CNN: It is of signi\ufb01cance to improve the quality of\ncandidate bounding boxes and to take a deep architecture to\nextract high-level features. To solve these problems, R-CNN\n[15] was proposed by Ross Girshick in 2014 and obtained a\nmean average precision (mAP) of 53.3% with more than 30%\nimprovement over the previous best result (DPM HSC [77]) on\nPASCAL VOC 2012. Figure 3 shows the \ufb02owchart of R-CNN,\nwhich can be divided into three stages as follows.\nRegion proposal generation. The R-CNN adopts selective\nsearch [78] to generate about 2k region proposals for each\nimage. The selective search method relies on simple bottom-up\ngrouping and saliency cues to provide more accurate candidate\nboxes of arbitrary sizes quickly and to reduce the searching\nspace in object detection [24], [39].\nCNN based deep feature extraction. In this stage, each\nregion proposal is warped or cropped into a \ufb01xed resolution\nand the CNN module in [6] is utilized to extract a 4096-\ndimensional feature as the \ufb01nal representation. Due to large\nlearning capacity, dominant expressive power and hierarchical\nstructure of CNNs, a high-level, semantic and robust feature\nrepresentation for each region proposal can be obtained.\nClassi\ufb01cation and localization. With pre-trained category-\nspeci\ufb01c linear SVMs for multiple classes, different region pro-\nposals are scored on a set of positive regions and background\n(negative) regions. The scored regions are then adjusted with\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n4\nGeneric object \ndetection\nRegion proposal \nbased\nRegression/\nClassification \nbased \nR-CNN\n(2014)\nSPP-net\n(2015)\nFRCN\n(2015)\nFaster \nR-CNN\n(2015)\nR-FCN\n(2016)\nFPN\n(2017)\nMask R-CNN\n(2017)\nMultiBox\n(2014)\nAttentionNet\n(2015)\nG-CNN\n(2016)\nYOLO\n(2016)\nSSD\n(2016)\nYOLOv2\n(2017)\nSPP \nlayer\nMulti-\ntask\nRPN\nFCN\nFeature\npyramid\nInstance\nSegmentation\nRegion\nproposal\nUnified\nloss\nDirection\niteration\nJoint Grid\nregression\nRPN\nBN\nMulti-scale\nGrid\nregression\nDSSD\n(2017)\nDSOD\n(2017)\nStem block\nDense block\nResNet101 \nDeconv layers\nFig. 2. Two types of frameworks: region proposal based and regression/classi\ufb01cation based. SPP: Spatial Pyramid Pooling [64], FRCN: Faster R-CNN [16],\nRPN: Region Proposal Network [18], FCN: Fully Convolutional Network [65], BN: Batch Normalization [43], Deconv layers: Deconvolution layers [54]\n.\n1. Input \nimage\n2. Extract region \nproposals (~2k)\n3. Compute \nCNN features\naeroplane? no.\n...\nperson? yes.\ntvmonitor? no.\n4. Classify \nregions\nwarped region\n...\nCNN\nR-CNN: Regions with CNN features\nFig. 3. The \ufb02owchart of R-CNN [15], which consists of 3 stages: (1) extracts\nbottom-up region proposals, (2) computes features for each proposal using a\nCNN, and then (3) classi\ufb01es each region with class-speci\ufb01c linear SVMs.\nbounding box regression and \ufb01ltered with a greedy non-\nmaximum suppression (NMS) to produce \ufb01nal bounding boxes\nfor preserved object locations.\nWhen there are scarce or insuf\ufb01cient labeled data, pre-\ntraining is usually conducted. Instead of unsupervised pre-\ntraining [79], R-CNN \ufb01rstly conducts supervised pre-training\non ILSVRC, a very large auxiliary dataset, and then takes a\ndomain-speci\ufb01c \ufb01ne-tuning. This scheme has been adopted by\nmost of subsequent approaches [16], [18].\nIn spite of its improvements over traditional methods and\nsigni\ufb01cance in bringing CNN into practical object detection,\nthere are still some disadvantages.\n\u2022 Due to the existence of FC layers, the CNN requires a\n\ufb01xed-size (e.g., 227\u00d7227) input image, which directly leads\nto the re-computation of the whole CNN for each evaluated\nregion, taking a great deal of time in the testing period.\n\u2022 Training of R-CNN is a multi-stage pipeline. At \ufb01rst,\na convolutional network (ConvNet) on object proposals is\n\ufb01ne-tuned. Then the softmax classi\ufb01er learned by \ufb01ne-\ntuning is replaced by SVMs to \ufb01t in with ConvNet features.\nFinally, bounding-box regressors are trained.\n\u2022 Training is expensive in space and time. Features are\nextracted from different region proposals and stored on the\ndisk. It will take a long time to process a relatively small\ntraining set with very deep networks, such as VGG16. At the\nsame time, the storage memory required by these features\nshould also be a matter of concern.\n\u2022 Although selective search can generate region proposals\nwith relatively high recalls, the obtained region proposals\nare still redundant and this procedure is time-consuming\n(around 2 seconds to extract 2k region proposals).\nTo solve these problems, many methods have been pro-\nposed. GOP [80] takes a much faster geodesic based segmen-\ntation to replace traditional graph cuts. MCG [81] searches\ndifferent scales of the image for multiple hierarchical segmen-\ntations and combinatorially groups different regions to produce\nproposals. Instead of extracting visually distinct segments,\nthe edge boxes method [82] adopts the idea that objects are\nmore likely to exist in bounding boxes with fewer contours\nstraggling their boundaries. Also some researches tried to\nre-rank or re\ufb01ne pre-extracted region proposals to remove\nunnecessary ones and obtained a limited number of valuable\nones, such as DeepBox [83] and SharpMask [84].\nIn addition, there are some improvements to solve the\nproblem of inaccurate localization. Zhang et al. [85] utilized\na bayesian optimization based search algorithm to guide\nthe regressions of different bounding boxes sequentially, and\ntrained class-speci\ufb01c CNN classi\ufb01ers with a structured loss\nto penalize the localization inaccuracy explicitly. Saurabh\nGupta et al. improved object detection for RGB-D images\nwith semantically rich image and depth features [86], and\nlearned a new geocentric embedding for depth images to\nencode each pixel. The combination of object detectors and\nsuperpixel classi\ufb01cation framework gains a promising result\non semantic scene segmentation task. Ouyang et al. proposed\na deformable deep CNN (DeepID-Net) [87] which introduces\na novel deformation constrained pooling (def-pooling) layer\nto impose geometric penalty on the deformation of various\nobject parts and makes an ensemble of models with different\nsettings. Lenc et al. [88] provided an analysis on the role\nof proposal generation in CNN-based detectors and tried to\nreplace this stage with a constant and trivial region generation\nscheme. The goal is achieved by biasing sampling to match\nthe statistics of the ground truth bounding boxes with K-means\nclustering. However, more candidate boxes are required to\nachieve comparable results to those of R-CNN.\n2) SPP-net: FC layers must take a \ufb01xed-size input. That\u2019s\nwhy R-CNN chooses to warp or crop each region proposal\ninto the same size. However, the object may exist partly in\nthe cropped region and unwanted geometric distortion may be\nproduced due to the warping operation. These content losses or\ndistortions will reduce recognition accuracy, especially when\nthe scales of objects vary.\nTo solve this problem, He et al. took the theory of spatial\npyramid matching (SPM) [89], [90] into consideration and\nproposed a novel CNN architecture named SPP-net [64]. SPM\ntakes several \ufb01ner to coarser scales to partition the image into\na number of divisions and aggregates quantized local features\ninto mid-level representations.\nThe architecture of SPP-net for object detection can be\nfound in Figure 4. Different from R-CNN, SPP-net reuses\nfeature maps of the 5-th conv layer (conv5) to project region\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n5\nspatial pyramid \npooling layer\nfeature maps of conv5\nconvolutional layers\nfixed-length representation\ninput image\nwindow\n\u2026...\nfully-connected layers (fc6, fc7)\nFig. 4. The architecture of SPP-net for object detection [64].\nDeep\nConvNet\nConv\nfeature map\nRoI\nprojection\nRoI\npooling\nlayer\nFCs\nRoI feature\nvector\nsoftmax\nbbox\nregressor\nOutputs:\nFC\nFC\nFor each RoI\nFig. 5. The architecture of Fast R-CNN [16].\nproposals of arbitrary sizes to \ufb01xed-length feature vectors. The\nfeasibility of the reusability of these feature maps is due to\nthe fact that the feature maps not only involve the strength of\nlocal responses, but also have relationships with their spatial\npositions [64]. The layer after the \ufb01nal conv layer is referred\nto as spatial pyramid pooling layer (SPP layer). If the number\nof feature maps in conv5 is 256, taking a 3-level pyramid,\nthe \ufb01nal feature vector for each region proposal obtained after\nSPP layer has a dimension of 256 \u00d7 (12 + 22 + 42) = 5376.\nSPP-net not only gains better results with correct estimation\nof different region proposals in their corresponding scales, but\nalso improves detection ef\ufb01ciency in testing period with the\nsharing of computation cost before SPP layer among different\nproposals.\n3) Fast R-CNN: Although SPP-net has achieved impressive\nimprovements in both accuracy and ef\ufb01ciency over R-CNN,\nit still has some notable drawbacks. SPP-net takes almost\nthe same multi-stage pipeline as R-CNN, including feature\nextraction, network \ufb01ne-tuning, SVM training and bounding-\nbox regressor \ufb01tting. So an additional expense on storage space\nis still required. Additionally, the conv layers preceding the\nSPP layer cannot be updated with the \ufb01ne-tuning algorithm\nintroduced in [64]. As a result, an accuracy drop of very deep\nnetworks is unsurprising. To this end, Girshick [16] introduced\na multi-task loss on classi\ufb01cation and bounding box regression\nand proposed a novel CNN architecture named Fast R-CNN.\nThe architecture of Fast R-CNN is exhibited in Figure 5.\nSimilar to SPP-net, the whole image is processed with conv\nlayers to produce feature maps. Then, a \ufb01xed-length feature\nvector is extracted from each region proposal with a region of\ninterest (RoI) pooling layer. The RoI pooling layer is a special\ncase of the SPP layer, which has only one pyramid level. Each\nfeature vector is then fed into a sequence of FC layers before\n\ufb01nally branching into two sibling output layers. One output\nlayer is responsible for producing softmax probabilities for\nall C + 1 categories (C object classes plus one \u2018background\u2019\nclass) and the other output layer encodes re\ufb01ned bounding-\nbox positions with four real-valued numbers. All parameters\nin these procedures (except the generation of region proposals)\nare optimized via a multi-task loss in an end-to-end way.\nThe multi-tasks loss L is de\ufb01ned as below to jointly train\nclassi\ufb01cation and bounding-box regression,\nL(p, u, tu, v) = Lcls(p, u) + \u03bb[u \u22651]Lloc(tu, v)\n(1)\nwhere Lcls(p, u) = \u2212log pu calculates the log loss for ground\ntruth class u and pu is driven from the discrete probability\ndistribution p = (p0, \u00b7 \u00b7 \u00b7 , pC) over the C +1 outputs from the\nlast FC layer. Lloc(tu, v) is de\ufb01ned over the predicted offsets\ntu = (tu\nx, tu\ny, tu\nw, tu\nh) and ground-truth bounding-box regression\ntargets v = (vx, vy, vw, vh), where x, y, w, h denote the two\ncoordinates of the box center, width, and height, respectively.\nEach tu adopts the parameter settings in [15] to specify an\nobject proposal with a log-space height/width shift and scale-\ninvariant translation. The Iverson bracket indicator function\n[u \u22651] is employed to omit all background RoIs. To provide\nmore robustness against outliers and eliminate the sensitivity\nin exploding gradients, a smooth L1 loss is adopted to \ufb01t\nbounding-box regressors as below\nLloc(tu, v) =\nX\ni\u2208x,y,w,h\nsmoothL1(tu\ni \u2212vi)\n(2)\nwhere\nsmoothL1(x) =\n(\n0.5x2\nif |x| < 1\n|x| \u22120.5\notherwise\n(3)\nTo accelerate the pipeline of Fast R-CNN, another two tricks\nare of necessity. On one hand, if training samples (i.e. RoIs)\ncome from different images, back-propagation through the\nSPP layer becomes highly inef\ufb01cient. Fast R-CNN samples\nmini-batches hierarchically, namely N images sampled ran-\ndomly at \ufb01rst and then R/N RoIs sampled in each image,\nwhere R represents the number of RoIs. Critically, computa-\ntion and memory are shared by RoIs from the same image in\nthe forward and backward pass. On the other hand, much time\nis spent in computing the FC layers during the forward pass\n[16]. The truncated Singular Value Decomposition (SVD) [91]\ncan be utilized to compress large FC layers and to accelerate\nthe testing procedure.\nIn the Fast R-CNN, regardless of region proposal genera-\ntion, the training of all network layers can be processed in\na single-stage with a multi-task loss. It saves the additional\nexpense on storage space, and improves both accuracy and\nef\ufb01ciency with more reasonable training schemes.\n4) Faster R-CNN: Despite the attempt to generate candi-\ndate boxes with biased sampling [88], state-of-the-art object\ndetection networks mainly rely on additional methods, such as\nselective search and Edgebox, to generate a candidate pool of\nisolated region proposals. Region proposal computation is also\na bottleneck in improving ef\ufb01ciency. To solve this problem,\nRen et al. introduced an additional Region Proposal Network\n(RPN) [18], [92], which acts in a nearly cost-free way by\nsharing full-image conv features with detection network.\nRPN is achieved with a fully-convolutional network, which\nhas the ability to predict object bounds and scores at each\nposition simultaneously. Similar to [78], RPN takes an image\nof arbitrary size to generate a set of rectangular object propos-\nals. RPN operates on a speci\ufb01c conv layer with the preceding\nlayers shared with object detection network.\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n6\nconv feature map\nintermediate layer\n256-d\n2k scores\n4k coordinates\nsliding window\nreg layer\ncls layer\nk anchor boxes\nFig. 6.\nThe RPN in Faster R-CNN [18]. K prede\ufb01ned anchor boxes are\nconvoluted with each sliding window to produce \ufb01xed-length vectors which\nare taken by cls and reg layer to obtain corresponding outputs.\nThe architecture of RPN is shown in Figure 6. The network\nslides over the conv feature map and fully connects to an\nn \u00d7 n spatial window. A low dimensional vector (512-d for\nVGG16) is obtained in each sliding window and fed into two\nsibling FC layers, namely box-classi\ufb01cation layer (cls) and\nbox-regression layer (reg). This architecture is implemented\nwith an n \u00d7 n conv layer followed by two sibling 1 \u00d7 1 conv\nlayers. To increase non-linearity, ReLU is applied to the output\nof the n \u00d7 n conv layer.\nThe regressions towards true bounding boxes are achieved\nby comparing proposals relative to reference boxes (anchors).\nIn the Faster R-CNN, anchors of 3 scales and 3 aspect ratios\nare adopted. The loss function is similar to (1).\nL(pi, ti) =\n1\nNcls\nX\ni\nLcls(pi, p\u2217\ni ) + \u03bb\n1\nNreg\nX\ni\np\u2217\ni Lreg(ti, t\u2217\ni )\n(4)\nwhere pi shows the predicted probability of the i-th anchor\nbeing an object. The ground truth label p\u2217\ni is 1 if the anchor is\npositive, otherwise 0. ti stores 4 parameterized coordinates of\nthe predicted bounding box while t\u2217\ni is related to the ground-\ntruth box overlapping with a positive anchor. Lcls is a binary\nlog loss and Lreg is a smoothed L1 loss similar to (2). These\ntwo terms are normalized with the mini-batch size (Ncls)\nand the number of anchor locations (Nreg), respectively. In\nthe form of fully-convolutional networks, Faster R-CNN can\nbe trained end-to-end by back-propagation and SGD in an\nalternate training manner.\nWith the proposal of Faster R-CNN, region proposal based\nCNN architectures for object detection can really be trained\nin an end-to-end way. Also a frame rate of 5 FPS (Frame\nPer Second) on a GPU is achieved with state-of-the-art object\ndetection accuracy on PASCAL VOC 2007 and 2012. How-\never, the alternate training algorithm is very time-consuming\nand RPN produces object-like regions (including backgrounds)\ninstead of object instances and is not skilled in dealing with\nobjects with extreme scales or shapes.\n5) R-FCN: Divided by the RoI pooling layer, a prevalent\nfamily [16], [18] of deep networks for object detection are\ncomposed of two subnetworks: a shared fully convolutional\nsubnetwork (independent of RoIs) and an unshared RoI-wise\nsubnetwork. This decomposition originates from pioneering\nclassi\ufb01cation architectures (e.g. AlexNet [6] and VGG16 [46])\nwhich consist of a convolutional subnetwork and several FC\nlayers separated by a speci\ufb01c spatial pooling layer.\nRecent state-of-the-art image classi\ufb01cation networks, such\nas Residual Nets (ResNets) [47] and GoogLeNets [45], [93],\nare fully convolutional. To adapt to these architectures, it\u2019s\n(a) Featurized image pyramid\npredict\npredict\npredict\npredict\n(b) Single feature map\npredict\n(d) Feature Pyramid Network\npredict\npredict\npredict\n(c) Pyramidal feature hierarchy\npredict\npredict\npredict\nFig. 7. The main concern of FPN [66]. (a) It is slow to use an image pyramid\nto build a feature pyramid. (b) Only single scale features is adopted for faster\ndetection. (c) An alternative to the featurized image pyramid is to reuse the\npyramidal feature hierarchy computed by a ConvNet. (d) FPN integrates both\n(b) and (c). Blue outlines indicate feature maps and thicker outlines denote\nsemantically stronger features.\nnatural to construct a fully convolutional object detection net-\nwork without RoI-wise subnetwork. However, it turns out to be\ninferior with such a naive solution [47]. This inconsistence is\ndue to the dilemma of respecting translation variance in object\ndetection compared with increasing translation invariance in\nimage classi\ufb01cation. In other words, shifting an object inside\nan image should be indiscriminative in image classi\ufb01cation\nwhile any translation of an object in a bounding box may\nbe meaningful in object detection. A manual insertion of\nthe RoI pooling layer into convolutions can break down\ntranslation invariance at the expense of additional unshared\nregion-wise layers. So Li et al. [65] proposed a region-based\nfully convolutional networks (R-FCN, Fig. S2).\nDifferent from Faster R-CNN, for each category, the last\nconv layer of R-FCN produces a total of k2 position-sensitive\nscore maps with a \ufb01xed grid of k \u00d7 k \ufb01rstly and a position-\nsensitive RoI pooling layer is then appended to aggregate the\nresponses from these score maps. Finally, in each RoI, k2\nposition-sensitive scores are averaged to produce a C + 1-d\nvector and softmax responses across categories are computed.\nAnother 4k2-d conv layer is appended to obtain class-agnostic\nbounding boxes.\nWith R-FCN, more powerful classi\ufb01cation networks can be\nadopted to accomplish object detection in a fully-convolutional\narchitecture by sharing nearly all the layers, and state-of-the-\nart results are obtained on both PASCAL VOC and Microsoft\nCOCO [94] datasets at a test speed of 170ms per image.\n6) FPN: Feature pyramids built upon image pyramids\n(featurized image pyramids) have been widely applied in\nmany object detection systems to improve scale invariance\n[24], [64] (Figure 7(a)). However, training time and memory\nconsumption increase rapidly. To this end, some techniques\ntake only a single input scale to represent high-level semantics\nand increase the robustness to scale changes (Figure 7(b)),\nand image pyramids are built at test time which results in\nan inconsistency between train/test-time inferences [16], [18].\nThe in-network feature hierarchy in a deep ConvNet produces\nfeature maps of different spatial resolutions while introduces\nlarge semantic gaps caused by different depths (Figure 7(c)).\nTo avoid using low-level features, pioneer works [71], [95]\nusually build the pyramid starting from middle layers or\njust sum transformed feature responses, missing the higher-\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n7\nFig. 8. The Mask R-CNN framework for instance segmentation [67].\nresolution maps of the feature hierarchy.\nDifferent from these approaches, FPN [66] holds an ar-\nchitecture with a bottom-up pathway, a top-down pathway\nand several lateral connections to combine low-resolution and\nsemantically strong features with high-resolution and seman-\ntically weak features (Figure 7(d)). The bottom-up pathway,\nwhich is the basic forward backbone ConvNet, produces a\nfeature hierarchy by downsampling the corresponding feature\nmaps with a stride of 2. The layers owning the same size of\noutput maps are grouped into the same network stage and the\noutput of the last layer of each stage is chosen as the reference\nset of feature maps to build the following top-down pathway.\nTo build the top-down pathway, feature maps from higher\nnetwork stages are upsampled at \ufb01rst and then enhanced with\nthose of the same spatial size from the bottom-up pathway\nvia lateral connections. A 1 \u00d7 1 conv layer is appended to\nthe upsampled map to reduce channel dimensions and the\nmergence is achieved by element-wise addition. Finally, a 3\u00d73\nconvolution is also appended to each merged map to reduce\nthe aliasing effect of upsampling and the \ufb01nal feature map is\ngenerated. This process is iterated until the \ufb01nest resolution\nmap is generated.\nAs feature pyramid can extract rich semantics from all\nlevels and be trained end-to-end with all scales, state-of-the-\nart representation can be obtained without sacri\ufb01cing speed\nand memory. Meanwhile, FPN is independent of the backbone\nCNN architectures and can be applied to different stages of\nobject detection (e.g. region proposal generation) and to many\nother computer vision tasks (e.g. instance segmentation).\n7) Mask R-CNN: Instance segmentation [96] is a challeng-\ning task which requires detecting all objects in an image and\nsegmenting each instance (semantic segmentation [97]). These\ntwo tasks are usually regarded as two independent processes.\nAnd the multi-task scheme will create spurious edge and\nexhibit systematic errors on overlapping instances [98]. To\nsolve this problem, parallel to the existing branches in Faster\nR-CNN for classi\ufb01cation and bounding box regression, the\nMask R-CNN [67] adds a branch to predict segmentation\nmasks in a pixel-to-pixel manner (Figure 8).\nDifferent from the other two branches which are inevitably\ncollapsed into short output vectors by FC layers, the segmen-\ntation mask branch encodes an m \u00d7 m mask to maintain the\nexplicit object spatial layout. This kind of fully convolutional\nrepresentation requires fewer parameters but is more accurate\nthan that of [97]. Formally, besides the two losses in (1) for\nclassi\ufb01cation and bounding box regression, an additional loss\nfor segmentation mask branch is de\ufb01ned to reach a multi-task\nloss. An this loss is only associated with ground-truth class\nand relies on the classi\ufb01cation branch to predict the category.\nBecause RoI pooling, the core operation in Faster R-CNN,\nperforms a coarse spatial quantization for feature extraction,\nmisalignment is introduced between the RoI and the features.\nIt affects classi\ufb01cation little because of its robustness to small\ntranslations. However, it has a large negative effect on pixel-\nto-pixel mask prediction. To solve this problem, Mask R-CNN\nadopts a simple and quantization-free layer, namely RoIAlign,\nto preserve the explicit per-pixel spatial correspondence faith-\nfully. RoIAlign is achieved by replacing the harsh quantization\nof RoI pooling with bilinear interpolation [99], computing the\nexact values of the input features at four regularly sampled\nlocations in each RoI bin. In spite of its simplicity, this\nseemingly minor change improves mask accuracy greatly,\nespecially under strict localization metrics.\nGiven the Faster R-CNN framework, the mask branch only\nadds a small computational burden and its cooperation with\nother tasks provides complementary information for object\ndetection. As a result, Mask R-CNN is simple to implement\nwith promising instance segmentation and object detection\nresults. In a word, Mask R-CNN is a \ufb02exible and ef\ufb01cient\nframework for instance-level recognition, which can be easily\ngeneralized to other tasks (e.g. human pose estimation [7][S4])\nwith minimal modi\ufb01cation.\n8) Multi-task Learning, Multi-scale Representation and\nContextual Modelling:\nAlthough the Faster R-CNN gets\npromising results with several hundred proposals, it still strug-\ngles in small-size object detection and localization, mainly due\nto the coarseness of its feature maps and limited information\nprovided in particular candidate boxes. The phenomenon is\nmore obvious on the Microsoft COCO dataset which consists\nof objects at a broad range of scales, less prototypical images,\nand requires more precise localization. To tackle these prob-\nlems, it is of necessity to accomplish object detection with\nmulti-task learning [100], multi-scale representation [95] and\ncontext modelling [101] to combine complementary informa-\ntion from multiple sources.\nMulti-task Learning learns a useful representation for\nmultiple correlated tasks from the same input [102], [103].\nBrahmbhatt et al. introduced conv features trained for ob-\nject segmentation and \u2018stuff\u2019 (amorphous categories such as\nground and water) to guide accurate object detection of small\nobjects (StuffNet) [100]. Dai et al. [97] presented Multitask\nNetwork Cascades of three networks, namely class-agnostic\nregion proposal generation, pixel-level instance segmentation\nand regional instance classi\ufb01cation. Li et al. incorporated the\nweakly-supervised object segmentation cues and region-based\nobject detection into a multi-stage architecture to fully exploit\nthe learned segmentation features [104].\nMulti-scale Representation combines activations from\nmultiple layers with skip-layer connections to provide seman-\ntic information of different spatial resolutions [66]. Cai et\nal. proposed the MS-CNN [105] to ease the inconsistency\nbetween the sizes of objects and receptive \ufb01elds with multiple\nscale-independent output layers. Yang et al. investigated two\nstrategies, namely scale-dependent pooling (SDP) and layer-\nwise cascaded rejection classi\ufb01ers (CRC), to exploit appropri-\nate scale-dependent conv features [33]. Kong et al. proposed\nthe HyperNet to calculate the shared features between RPN\nand object detection network by aggregating and compressing\nhierarchical feature maps from different resolutions into a\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n8\nuniform space [101].\nContextual Modelling improves detection performance by\nexploiting features from or around RoIs of different support\nregions and resolutions to deal with occlusions and local\nsimilarities [95]. Zhu et al. proposed the SegDeepM to exploit\nobject segmentation which reduces the dependency on initial\ncandidate boxes with Markov Random Field [106]. Moysset\net al. took advantage of 4 directional 2D-LSTMs [107] to\nconvey global context between different local regions and re-\nduced trainable parameters with local parameter-sharing [108].\nZeng et al. proposed a novel GBD-Net by introducing gated\nfunctions to control message transmission between different\nsupport regions [109].\nThe Combination incorporates different components above\ninto the same model to improve detection performance further.\nGidaris et al. proposed the Multi-Region CNN (MR-CNN)\nmodel [110] to capture different aspects of an object, the\ndistinct appearances of various object parts and semantic\nsegmentation-aware features. To obtain contextual and multi-\nscale representations, Bell et al. proposed the Inside-Outside\nNet (ION) by exploiting information both inside and outside\nthe RoI [95] with spatial recurrent neural networks [111] and\nskip pooling [101]. Zagoruyko et al. proposed the MultiPath\narchitecture by introducing three modi\ufb01cations to the Fast\nR-CNN [112], including multi-scale skip connections [95],\na modi\ufb01ed foveal structure [110] and a novel loss function\nsumming different IoU losses.\n9) Thinking in Deep Learning based Object Detection:\nApart from the above approaches, there are still many impor-\ntant factors for continued progress.\nThere is a large imbalance between the number of annotated\nobjects and background examples. To address this problem,\nShrivastava et al. proposed an effective online mining algo-\nrithm (OHEM) [113] for automatic selection of the hard ex-\namples, which leads to a more effective and ef\ufb01cient training.\nInstead of concentrating on feature extraction, Ren et al.\nmade a detailed analysis on object classi\ufb01ers [114], and\nfound that it is of particular importance for object detection\nto construct a deep and convolutional per-region classi\ufb01er\ncarefully, especially for ResNets [47] and GoogLeNets [45].\nTraditional CNN framework for object detection is not\nskilled in handling signi\ufb01cant scale variation, occlusion or\ntruncation, especially when only 2D object detection is in-\nvolved. To address this problem, Xiang et al. proposed a\nnovel subcategory-aware region proposal network [60], which\nguides the generation of region proposals with subcategory\ninformation related to object poses and jointly optimize object\ndetection and subcategory classi\ufb01cation.\nOuyang et al. found that the samples from different classes\nfollow a longtailed distribution [115], which indicates that dif-\nferent classes with distinct numbers of samples have different\ndegrees of impacts on feature learning. To this end, objects are\n\ufb01rstly clustered into visually similar class groups, and then a\nhierarchical feature learning scheme is adopted to learn deep\nrepresentations for each group separately.\nIn order to minimize computational cost and achieve the\nstate-of-the-art performance, with the \u2018deep and thin\u2019 design\nprinciple and following the pipeline of Fast R-CNN, Hong et\nal. proposed the architecture of PVANET [116], which adopts\nsome building blocks including concatenated ReLU [117],\nInception [45], and HyperNet [101] to reduce the expense on\nmulti-scale feature extraction and trains the network with batch\nnormalization [43], residual connections [47], and learning\nrate scheduling based on plateau detection [47]. The PVANET\nachieves the state-of-the-art performance and can be processed\nin real time on Titan X GPU (21 FPS).\nB. Regression/Classi\ufb01cation Based Framework\nRegion proposal based frameworks are composed of sev-\neral correlated stages, including region proposal generation,\nfeature extraction with CNN, classi\ufb01cation and bounding box\nregression, which are usually trained separately. Even in recent\nend-to-end module Faster R-CNN, an alternative training is\nstill required to obtain shared convolution parameters between\nRPN and detection network. As a result, the time spent in\nhandling different components becomes the bottleneck in real-\ntime application.\nOne-step\nframeworks\nbased\non\nglobal\nregres-\nsion/classi\ufb01cation, mapping straightly from image pixels\nto bounding box coordinates and class probabilities, can\nreduce time expense. We \ufb01rstly reviews some pioneer CNN\nmodels, and then focus on two signi\ufb01cant frameworks,\nnamely You only look once (YOLO) [17] and Single Shot\nMultiBox Detector (SSD) [71].\n1) Pioneer Works: Previous to YOLO and SSD, many\nresearchers have already tried to model object detection as\na regression or classi\ufb01cation task.\nSzegedy et al. formulated object detection task as a DNN-\nbased regression [118], generating a binary mask for the\ntest image and extracting detections with a simple bounding\nbox inference. However, the model has dif\ufb01culty in handling\noverlapping objects, and bounding boxes generated by direct\nupsampling is far from perfect.\nPinheiro et al. proposed a CNN model with two branches:\none generates class agnostic segmentation masks and the\nother predicts the likelihood of a given patch centered on\nan object [119]. Inference is ef\ufb01cient since class scores and\nsegmentation can be obtained in a single model with most of\nthe CNN operations shared.\nErhan et al. proposed regression based MultiBox to produce\nscored class-agnostic region proposals [68], [120]. A uni\ufb01ed\nloss was introduced to bias both localization and con\ufb01dences\nof multiple components to predict the coordinates of class-\nagnostic bounding boxes. However, a large quantity of addi-\ntional parameters are introduced to the \ufb01nal layer.\nYoo et al. adopted an iterative classi\ufb01cation approach to\nhandle object detection and proposed an impressive end-to-\nend CNN architecture named AttentionNet [69]. Starting from\nthe top-left (TL) and bottom-right (BR) corner of an image,\nAttentionNet points to a target object by generating quantized\nweak directions and converges to an accurate object bound-\nary box with an ensemble of iterative predictions. However,\nthe model becomes quite inef\ufb01cient when handling multiple\ncategories with a progressive two-step procedure.\nNajibi et al. proposed a proposal-free iterative grid based\nobject detector (G-CNN), which models object detection as\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n9\nFig. 9. Main idea of YOLO [17].\n\ufb01nding a path from a \ufb01xed grid to boxes tightly surrounding\nthe objects [70]. Starting with a \ufb01xed multi-scale bounding box\ngrid, G-CNN trains a regressor to move and scale elements of\nthe grid towards objects iteratively. However, G-CNN has a\ndif\ufb01culty in dealing with small or highly overlapping objects.\n2) YOLO: Redmon et al. [17] proposed a novel framework\ncalled YOLO, which makes use of the whole topmost feature\nmap to predict both con\ufb01dences for multiple categories and\nbounding boxes. The basic idea of YOLO is exhibited in\nFigure 9. YOLO divides the input image into an S \u00d7 S grid and\neach grid cell is responsible for predicting the object centered\nin that grid cell. Each grid cell predicts B bounding boxes\nand their corresponding con\ufb01dence scores. Formally, con\ufb01-\ndence scores are de\ufb01ned as Pr(Object) \u2217IOU truth\npred , which\nindicates how likely there exist objects (Pr(Object) \u22650) and\nshows con\ufb01dences of its prediction (IOU truth\npred ). At the same\ntime, regardless of the number of boxes, C conditional class\nprobabilities (Pr(Classi|Object)) should also be predicted in\neach grid cell. It should be noticed that only the contribution\nfrom the grid cell containing an object is calculated.\nAt test time, class-speci\ufb01c con\ufb01dence scores for each box\nare achieved by multiplying the individual box con\ufb01dence\npredictions with the conditional class probabilities as follows.\nPr(Object) \u2217IOU truth\npred \u2217Pr(Classi|Object)\n= Pr(Classi) \u2217IOU truth\npred\n(5)\nwhere the existing probability of class-speci\ufb01c objects in the\nbox and the \ufb01tness between the predicted box and the object\nare both taken into consideration.\nDuring training, the following loss function is optimized,\n\u03bbcoord\nS2\nX\ni=0\nB\nX\nj=0\n1obj\nij\n\u0002\n(xi \u2212\u02c6xi)2 + (yi \u2212\u02c6yi)2\u0003\n+\u03bbcoord\nS2\nX\ni=0\nB\nX\nj=0\n1obj\nij\n\"\u0012\u221awi \u2212\np\n\u02c6wi)2 + (\np\nhi \u2212\nq\n\u02c6hi\n\u00132#\n+\nS2\nX\ni=0\nB\nX\nj=0\n1obj\nij\n\u0010\nCi \u2212\u02c6Ci\n\u00112\n+\u03bbnoobj\nS2\nX\ni=0\nB\nX\nj=0\n1noobj\nij\n\u0010\nCi \u2212\u02c6Ci\n\u00112\n+\nS2\nX\ni=0\n1obj\ni\nX\nc\u2208classes\n(pi(c) \u2212\u02c6pi(c))2\n(6)\nIn a certain cell i, (xi, yi) denote the center of the box relative\nto the bounds of the grid cell, (wi, hi) are the normalized width\nand height relative to the image size, Ci represents con\ufb01dence\nscores, 1obj\ni\nindicates the existence of objects and 1obj\nij\ndenotes\nthat the prediction is conducted by the jth bounding box\npredictor. Note that only when an object is present in that grid\ncell, the loss function penalizes classi\ufb01cation errors. Similarly,\nwhen the predictor is \u2018responsible\u2019 for the ground truth box\n(i.e. the highest IoU of any predictor in that grid cell is\nachieved), bounding box coordinate errors are penalized.\nThe YOLO consists of 24 conv layers and 2 FC layers,\nof which some conv layers construct ensembles of inception\nmodules with 1 \u00d7 1 reduction layers followed by 3 \u00d7 3 conv\nlayers. The network can process images in real-time at 45\nFPS and a simpli\ufb01ed version Fast YOLO can reach 155 FPS\nwith better results than other real-time detectors. Furthermore,\nYOLO produces fewer false positives on background, which\nmakes the cooperation with Fast R-CNN become possible. An\nimproved version, YOLOv2, was later proposed in [72], which\nadopts several impressive strategies, such as BN, anchor boxes,\ndimension cluster and multi-scale training.\n3) SSD: YOLO has a dif\ufb01culty in dealing with small\nobjects in groups, which is caused by strong spatial constraints\nimposed on bounding box predictions [17]. Meanwhile, YOLO\nstruggles to generalize to objects in new/unusual aspect ratios/\ncon\ufb01gurations and produces relatively coarse features due to\nmultiple downsampling operations.\nAiming at these problems, Liu et al. proposed a Single Shot\nMultiBox Detector (SSD) [71], which was inspired by the\nanchors adopted in MultiBox [68], RPN [18] and multi-scale\nrepresentation [95]. Given a speci\ufb01c feature map, instead of\n\ufb01xed grids adopted in YOLO, the SSD takes advantage of a set\nof default anchor boxes with different aspect ratios and scales\nto discretize the output space of bounding boxes. To handle\nobjects with various sizes, the network fuses predictions from\nmultiple feature maps with different resolutions .\nThe architecture of SSD is demonstrated in Figure 10. Given\nthe VGG16 backbone architecture, SSD adds several feature\nlayers to the end of the network, which are responsible for\npredicting the offsets to default boxes with different scales and\naspect ratios and their associated con\ufb01dences. The network is\ntrained with a weighted sum of localization loss (e.g. Smooth\nL1) and con\ufb01dence loss (e.g. Softmax), which is similar to\n(1). Final detection results are obtained by conducting NMS\non multi-scale re\ufb01ned bounding boxes.\nIntegrating with hard negative mining, data augmentation\nand a larger number of carefully chosen default anchors,\nSSD signi\ufb01cantly outperforms the Faster R-CNN in terms of\naccuracy on PASCAL VOC and COCO, while being three\ntimes faster. The SSD300 (input image size is 300\u00d7300) runs\nat 59 FPS, which is more accurate and ef\ufb01cient than YOLO.\nHowever, SSD is not skilled at dealing with small objects,\nwhich can be relieved by adopting better feature extractor\nbackbone (e.g. ResNet101), adding deconvolution layers with\nskip connections to introduce additional large-scale context\n[73] and designing better network structure (e.g. Stem Block\nand Dense Block) [74].\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n10\nFig. 10. The architecture of SSD 300 [71]. SSD adds several feature layers to the end of VGG16 backbone network to predict the offsets to default anchor\nboxes and their associated con\ufb01dences. Final detection results are obtained by conducting NMS on multi-scale re\ufb01ned bounding boxes.\nC. Experimental Evaluation\nWe compare various object detection methods on three\nbenchmark datasets, including PASCAL VOC 2007 [25],\nPASCAL VOC 2012 [121] and Microsoft COCO [94]. The\nevaluated approaches include R-CNN [15], SPP-net [64], Fast\nR-CNN [16], NOC [114], Bayes [85], MR-CNN&S-CNN\n[105], Faster R-CNN [18], HyperNet [101], ION [95], MS-\nGR [104], StuffNet [100], SSD300 [71], SSD512 [71], OHEM\n[113], SDP+CRC [33], GCNN [70], SubCNN [60], GBD-Net\n[109], PVANET [116], YOLO [17], YOLOv2 [72], R-FCN\n[65], FPN [66], Mask R-CNN [67], DSSD [73] and DSOD\n[74]. If no speci\ufb01c instructions for the adopted framework\nare provided, the utilized model is a VGG16 [46] pretrained\non 1000-way ImageNet classi\ufb01cation task [39]. Due to the\nlimitation of paper length, we only provide an overview, in-\ncluding proposal, learning method, loss function, programming\nlanguage and platform, of the prominent architectures in Table\nI. Detailed experimental settings, which can be found in the\noriginal papers, are missed. In addition to the comparisons of\ndetection accuracy, another comparison is provided to evaluate\ntheir test consumption on PASCAL VOC 2007.\n1) PASCAL VOC 2007/2012: PASCAL VOC 2007 and\n2012 datasets consist of 20 categories. The evaluation terms\nare Average Precision (AP) in each single category and mean\nAverage Precision (mAP) across all the 20 categories. Com-\nparative results are exhibited in Table II and III, from which\nthe following remarks can be obtained.\n\u2022 If incorporated with a proper way, more powerful back-\nbone CNN models can de\ufb01nitely improve object detection\nperformance (the comparison among R-CNN with AlexNet,\nR-CNN with VGG16 and SPP-net with ZF-Net [122]).\n\u2022 With the introduction of SPP layer (SPP-net), end-to-\nend multi-task architecture (FRCN) and RPN (Faster R-\nCNN), object detection performance is improved gradually\nand apparently.\n\u2022 Due to large quantities of trainable parameters, in order to\nobtain multi-level robust features, data augmentation is very\nimportant for deep learning based models (Faster R-CNN\nwith \u201807\u2019 ,\u201807+12\u2019 and \u201807+12+coco\u2019).\n\u2022 Apart from basic models, there are still many other factors\naffecting object detection performance, such as multi-scale\nand multi-region feature extraction (e.g. MR-CNN), modi-\n\ufb01ed classi\ufb01cation networks (e.g. NOC), additional informa-\ntion from other correlated tasks (e.g. StuffNet, HyperNet),\nmulti-scale representation (e.g. ION) and mining of hard\nnegative samples (e.g. OHEM).\n\u2022 As YOLO is not skilled in producing object localizations\nof high IoU, it obtains a very poor result on VOC 2012.\nHowever, with the complementary information from Fast\nR-CNN (YOLO+FRCN) and the aid of other strategies,\nsuch as anchor boxes, BN and \ufb01ne grained features, the\nlocalization errors are corrected (YOLOv2).\n\u2022 By combining many recent tricks and modelling the whole\nnetwork as a fully convolutional one, R-FCN achieves a\nmore obvious improvement of detection performance over\nother approaches.\n2) Microsoft COCO: Microsoft COCO is composed of\n300,000 fully segmented images, in which each image has\nan average of 7 object instances from a total of 80 categories.\nAs there are a lot of less iconic objects with a broad range\nof scales and a stricter requirement on object localization,\nthis dataset is more challenging than PASCAL 2012. Object\ndetection performance is evaluated by AP computed under\ndifferent degrees of IoUs and on different object sizes. The\nresults are shown in Table IV.\nBesides similar remarks to those of PASCAL VOC, some\nother conclusions can be drawn as follows from Table IV.\n\u2022 Multi-scale training and test are bene\ufb01cial in improv-\ning object detection performance, which provide additional\ninformation in different resolutions (R-FCN). FPN and\nDSSD provide some better ways to build feature pyramids\nto achieve multi-scale representation. The complementary\ninformation from other related tasks is also helpful for\naccurate object localization (Mask R-CNN with instance\nsegmentation task).\n\u2022\nOverall,\nregion\nproposal\nbased\nmethods,\nsuch\nas\nFaster R-CNN and R-FCN, perform better than regres-\nsion/class\ufb01cation based approaches, namely YOLO and\nSSD, due to the fact that quite a lot of localization errors\nare produced by regression/class\ufb01cation based approaches.\n\u2022 Context modelling is helpful to locate small objects,\nwhich provides additional information by consulting nearby\nobjects and surroundings (GBD-Net and multi-path).\n\u2022 Due to the existence of a large number of nonstandard\nsmall objects, the results on this dataset are much worse\nthan those of VOC 2007/2012. With the introduction of\nother powerful frameworks (e.g. ResNeXt [123]) and useful\nstrategies (e.g. multi-task learning [67], [124]), the perfor-\nmance can be improved.\n\u2022 The success of DSOD in training from scratch stresses the\nimportance of network design to release the requirements\nfor perfect pre-trained classi\ufb01ers on relevant tasks and large\nnumbers of annotated samples.\n3) Timing Analysis: Timing analysis (Table V) is conducted\non Intel i7-6700K CPU with a single core and NVIDIA Titan\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n11\nTABLE I\nAN OVERVIEW OF PROMINENT GENERIC OBJECT DETECTION ARCHITECTURES.\nFramework\nProposal\nMulti-scale Input\nLearning Method\nLoss Function\nSoftmax Layer\nEnd-to-end Train\nPlatform\nLanguage\nR-CNN [15]\nSelective Search\n-\nSGD,BP\nHinge loss (classi\ufb01cation),Bounding box regression\n+\n-\nCaffe\nMatlab\nSPP-net [64]\nEdgeBoxes\n+\nSGD\nHinge loss (classi\ufb01cation),Bounding box regression\n+\n-\nCaffe\nMatlab\nFast RCNN [16]\nSelective Search\n+\nSGD\nClass Log loss+bounding box regression\n+\n-\nCaffe\nPython\nFaster R-CNN [18]\nRPN\n+\nSGD\nClass Log loss+bounding box regression\n+\n+\nCaffe\nPython/Matlab\nR-FCN [65]\nRPN\n+\nSGD\nClass Log loss+bounding box regression\n-\n+\nCaffe\nMatlab\nMask R-CNN [67]\nRPN\n+\nSGD\nClass Log loss+bounding box regression\n+\n+\nTensorFlow/Keras\nPython\n+Semantic sigmoid loss\nFPN [66]\nRPN\n+\nSynchronized SGD\nClass Log loss+bounding box regression\n+\n+\nTensorFlow\nPython\nYOLO [17]\n-\n-\nSGD\nClass sum-squared error loss+bounding box regression\n+\n+\nDarknet\nC\n+object con\ufb01dence+background con\ufb01dence\nSSD [71]\n-\n-\nSGD\nClass softmax loss+bounding box regression\n-\n+\nCaffe\nC++\nYOLOv2 [72]\n-\n-\nSGD\nClass sum-squared error loss+bounding box regression\n+\n+\nDarknet\nC\n+object con\ufb01dence+background con\ufb01dence\n* \u2018+\u2019 denotes that corresponding techniques are employed while \u2018-\u2019 denotes that this technique is not considered. It should be noticed that R-CNN and SPP-net can not be trained end-to-end with a multi-task loss while the\nother architectures are based on multi-task joint training. As most of these architectures are re-implemented on different platforms with various programming languages, we only list the information associated with the versions\nby the referenced authors.\nTABLE II\nCOMPARATIVE RESULTS ON VOC 2007 TEST SET (%).\nMethods\nTrained on\nareo\nbike\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse\nmbike\nperson\nplant\nsheep\nsofa\ntrain\ntv\nmAP\nR-CNN (Alex) [15]\n07\n68.1\n72.8\n56.8\n43.0\n36.8\n66.3\n74.2\n67.6\n34.4\n63.5\n54.5\n61.2\n69.1\n68.6\n58.7\n33.4\n62.9\n51.1\n62.5\n68.6\n58.5\nR-CNN(VGG16) [15]\n07\n73.4\n77.0\n63.4\n45.4\n44.6\n75.1\n78.1\n79.8\n40.5\n73.7\n62.2\n79.4\n78.1\n73.1\n64.2\n35.6\n66.8\n67.2\n70.4\n71.1\n66.0\nSPP-net(ZF) [64]\n07\n68.5\n71.7\n58.7\n41.9\n42.5\n67.7\n72.1\n73.8\n34.7\n67.0\n63.4\n66.0\n72.5\n71.3\n58.9\n32.8\n60.9\n56.1\n67.9\n68.8\n60.9\nGCNN [70]\n07\n68.3\n77.3\n68.5\n52.4\n38.6\n78.5\n79.5\n81.0\n47.1\n73.6\n64.5\n77.2\n80.5\n75.8\n66.6\n34.3\n65.2\n64.4\n75.6\n66.4\n66.8\nBayes [85]\n07\n74.1\n83.2\n67.0\n50.8\n51.6\n76.2\n81.4\n77.2\n48.1\n78.9\n65.6\n77.3\n78.4\n75.1\n70.1\n41.4\n69.6\n60.8\n70.2\n73.7\n68.5\nFast R-CNN [16]\n07+12\n77.0\n78.1\n69.3\n59.4\n38.3\n81.6\n78.6\n86.7\n42.8\n78.8\n68.9\n84.7\n82.0\n76.6\n69.9\n31.8\n70.1\n74.8\n80.4\n70.4\n70.0\nSDP+CRC [33]\n07\n76.1\n79.4\n68.2\n52.6\n46.0\n78.4\n78.4\n81.0\n46.7\n73.5\n65.3\n78.6\n81.0\n76.7\n77.3\n39.0\n65.1\n67.2\n77.5\n70.3\n68.9\nSubCNN [60]\n07\n70.2\n80.5\n69.5\n60.3\n47.9\n79.0\n78.7\n84.2\n48.5\n73.9\n63.0\n82.7\n80.6\n76.0\n70.2\n38.2\n62.4\n67.7\n77.7\n60.5\n68.5\nStuffNet30 [100]\n07\n72.6\n81.7\n70.6\n60.5\n53.0\n81.5\n83.7\n83.9\n52.2\n78.9\n70.7\n85.0\n85.7\n77.0\n78.7\n42.2\n73.6\n69.2\n79.2\n73.8\n72.7\nNOC [114]\n07+12\n76.3\n81.4\n74.4\n61.7\n60.8\n84.7\n78.2\n82.9\n53.0\n79.2\n69.2\n83.2\n83.2\n78.5\n68.0\n45.0\n71.6\n76.7\n82.2\n75.7\n73.3\nMR-CNN&S-CNN [110]\n07+12\n80.3\n84.1\n78.5\n70.8\n68.5\n88.0\n85.9\n87.8\n60.3\n85.2\n73.7\n87.2\n86.5\n85.0\n76.4\n48.5\n76.3\n75.5\n85.0\n81.0\n78.2\nHyperNet [101]\n07+12\n77.4\n83.3\n75.0\n69.1\n62.4\n83.1\n87.4\n87.4\n57.1\n79.8\n71.4\n85.1\n85.1\n80.0\n79.1\n51.2\n79.1\n75.7\n80.9\n76.5\n76.3\nMS-GR [104]\n07+12\n80.0\n81.0\n77.4\n72.1\n64.3\n88.2\n88.1\n88.4\n64.4\n85.4\n73.1\n87.3\n87.4\n85.1\n79.6\n50.1\n78.4\n79.5\n86.9\n75.5\n78.6\nOHEM+Fast R-CNN [113]\n07+12\n80.6\n85.7\n79.8\n69.9\n60.8\n88.3\n87.9\n89.6\n59.7\n85.1\n76.5\n87.1\n87.3\n82.4\n78.8\n53.7\n80.5\n78.7\n84.5\n80.7\n78.9\nION [95]\n07+12+S\n80.2\n85.2\n78.8\n70.9\n62.6\n86.6\n86.9\n89.8\n61.7\n86.9\n76.5\n88.4\n87.5\n83.4\n80.5\n52.4\n78.1\n77.2\n86.9\n83.5\n79.2\nFaster R-CNN [18]\n07\n70.0\n80.6\n70.1\n57.3\n49.9\n78.2\n80.4\n82.0\n52.2\n75.3\n67.2\n80.3\n79.8\n75.0\n76.3\n39.1\n68.3\n67.3\n81.1\n67.6\n69.9\nFaster R-CNN [18]\n07+12\n76.5\n79.0\n70.9\n65.5\n52.1\n83.1\n84.7\n86.4\n52.0\n81.9\n65.7\n84.8\n84.6\n77.5\n76.7\n38.8\n73.6\n73.9\n83.0\n72.6\n73.2\nFaster R-CNN [18]\n07+12+COCO\n84.3\n82.0\n77.7\n68.9\n65.7\n88.1\n88.4\n88.9\n63.6\n86.3\n70.8\n85.9\n87.6\n80.1\n82.3\n53.6\n80.4\n75.8\n86.6\n78.9\n78.8\nSSD300 [71]\n07+12+COCO\n80.9\n86.3\n79.0\n76.2\n57.6\n87.3\n88.2\n88.6\n60.5\n85.4\n76.7\n87.5\n89.2\n84.5\n81.4\n55.0\n81.9\n81.5\n85.9\n78.9\n79.6\nSSD512 [71]\n07+12+COCO\n86.6\n88.3\n82.4\n76.0\n66.3\n88.6\n88.9\n89.1\n65.1\n88.4\n73.6\n86.5\n88.9\n85.3\n84.6\n59.1\n85.0\n80.4\n87.4\n81.2\n81.6\n* \u201807\u2019: VOC2007 trainval, \u201807+12\u2019: union of VOC2007 and VOC2012 trainval, \u201807+12+COCO\u2019: trained on COCO trainval35k at \ufb01rst and then \ufb01ne-tuned on 07+12. The S in ION \u201807+12+S\u2019 denotes SBD segmentation labels.\nTABLE III\nCOMPARATIVE RESULTS ON VOC 2012 TEST SET (%).\nMethods\nTrained on\nareo\nbike\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse\nmbike\nperson\nplant\nsheep\nsofa\ntrain\ntv\nmAP\nR-CNN(Alex) [15]\n12\n71.8\n65.8\n52.0\n34.1\n32.6\n59.6\n60.0\n69.8\n27.6\n52.0\n41.7\n69.6\n61.3\n68.3\n57.8\n29.6\n57.8\n40.9\n59.3\n54.1\n53.3\nR-CNN(VGG16) [15]\n12\n79.6\n72.7\n61.9\n41.2\n41.9\n65.9\n66.4\n84.6\n38.5\n67.2\n46.7\n82.0\n74.8\n76.0\n65.2\n35.6\n65.4\n54.2\n67.4\n60.3\n62.4\nBayes [85]\n12\n82.9\n76.1\n64.1\n44.6\n49.4\n70.3\n71.2\n84.6\n42.7\n68.6\n55.8\n82.7\n77.1\n79.9\n68.7\n41.4\n69.0\n60.0\n72.0\n66.2\n66.4\nFast R-CNN [16]\n07++12\n82.3\n78.4\n70.8\n52.3\n38.7\n77.8\n71.6\n89.3\n44.2\n73.0\n55.0\n87.5\n80.5\n80.8\n72.0\n35.1\n68.3\n65.7\n80.4\n64.2\n68.4\nSutffNet30 [100]\n12\n83.0\n76.9\n71.2\n51.6\n50.1\n76.4\n75.7\n87.8\n48.3\n74.8\n55.7\n85.7\n81.2\n80.3\n79.5\n44.2\n71.8\n61.0\n78.5\n65.4\n70.0\nNOC [114]\n07+12\n82.8\n79.0\n71.6\n52.3\n53.7\n74.1\n69.0\n84.9\n46.9\n74.3\n53.1\n85.0\n81.3\n79.5\n72.2\n38.9\n72.4\n59.5\n76.7\n68.1\n68.8\nMR-CNN&S-CNN [110]\n07++12\n85.5\n82.9\n76.6\n57.8\n62.7\n79.4\n77.2\n86.6\n55.0\n79.1\n62.2\n87.0\n83.4\n84.7\n78.9\n45.3\n73.4\n65.8\n80.3\n74.0\n73.9\nHyperNet [101]\n07++12\n84.2\n78.5\n73.6\n55.6\n53.7\n78.7\n79.8\n87.7\n49.6\n74.9\n52.1\n86.0\n81.7\n83.3\n81.8\n48.6\n73.5\n59.4\n79.9\n65.7\n71.4\nOHEM+Fast R-CNN [113]\n07++12+coco\n90.1\n87.4\n79.9\n65.8\n66.3\n86.1\n85.0\n92.9\n62.4\n83.4\n69.5\n90.6\n88.9\n88.9\n83.6\n59.0\n82.0\n74.7\n88.2\n77.3\n80.1\nION [95]\n07+12+S\n87.5\n84.7\n76.8\n63.8\n58.3\n82.6\n79.0\n90.9\n57.8\n82.0\n64.7\n88.9\n86.5\n84.7\n82.3\n51.4\n78.2\n69.2\n85.2\n73.5\n76.4\nFaster R-CNN [18]\n07++12\n84.9\n79.8\n74.3\n53.9\n49.8\n77.5\n75.9\n88.5\n45.6\n77.1\n55.3\n86.9\n81.7\n80.9\n79.6\n40.1\n72.6\n60.9\n81.2\n61.5\n70.4\nFaster R-CNN [18]\n07++12+coco\n87.4\n83.6\n76.8\n62.9\n59.6\n81.9\n82.0\n91.3\n54.9\n82.6\n59.0\n89.0\n85.5\n84.7\n84.1\n52.2\n78.9\n65.5\n85.4\n70.2\n75.9\nYOLO [17]\n07++12\n77.0\n67.2\n57.7\n38.3\n22.7\n68.3\n55.9\n81.4\n36.2\n60.8\n48.5\n77.2\n72.3\n71.3\n63.5\n28.9\n52.2\n54.8\n73.9\n50.8\n57.9\nYOLO+Fast R-CNN [17]\n07++12\n83.4\n78.5\n73.5\n55.8\n43.4\n79.1\n73.1\n89.4\n49.4\n75.5\n57.0\n87.5\n80.9\n81.0\n74.7\n41.8\n71.5\n68.5\n82.1\n67.2\n70.7\nYOLOv2 [72]\n07++12+coco\n88.8\n87.0\n77.8\n64.9\n51.8\n85.2\n79.3\n93.1\n64.4\n81.4\n70.2\n91.3\n88.1\n87.2\n81.0\n57.7\n78.1\n71.0\n88.5\n76.8\n78.2\nSSD300 [71]\n07++12+coco\n91.0\n86.0\n78.1\n65.0\n55.4\n84.9\n84.0\n93.4\n62.1\n83.6\n67.3\n91.3\n88.9\n88.6\n85.6\n54.7\n83.8\n77.3\n88.3\n76.5\n79.3\nSSD512 [71]\n07++12+coco\n91.4\n88.6\n82.6\n71.4\n63.1\n87.4\n88.1\n93.9\n66.9\n86.6\n66.3\n92.0\n91.7\n90.8\n88.5\n60.9\n87.0\n75.4\n90.2\n80.4\n82.2\nR-FCN (ResNet101) [16]\n07++12+coco\n92.3\n89.9\n86.7\n74.7\n75.2\n86.7\n89.0\n95.8\n70.2\n90.4\n66.5\n95.0\n93.2\n92.1\n91.1\n71.0\n89.7\n76.0\n92.0\n83.4\n85.0\n* \u201807++12\u2019: union of VOC2007 trainval and test and VOC2012 trainval. \u201807++12+COCO\u2019: trained on COCO trainval35k at \ufb01rst then \ufb01ne-tuned on 07++12.\nTABLE IV\nCOMPARATIVE RESULTS ON MICROSOFT COCO TEST DEV SET (%).\nMethods\nTrained on 0.5:0.95 0.5 0.75\nS\nM\nL\n1\n10\n100\nS\nM\nL\nFast R-CNN [16]\ntrain\n20.5\n39.9 19.4 4.1 20.0 35.8 21.3 29.4 30.1 7.3 32.1 52.0\nION [95]\ntrain\n23.6\n43.2 23.6 6.4 24.1 38.3 23.2 32.7 33.5 10.1 37.7 53.6\nNOC+FRCN(VGG16) [114]\ntrain\n21.2\n41.5 19.7\n-\n-\n-\n-\n-\n-\n-\n-\n-\nNOC+FRCN(Google) [114]\ntrain\n24.8\n44.4 25.2\n-\n-\n-\n-\n-\n-\n-\n-\n-\nNOC+FRCN (ResNet101) [114]\ntrain\n27.2\n48.4 27.6\n-\n-\n-\n-\n-\n-\n-\n-\n-\nGBD-Net [109]\ntrain\n27.0\n45.8\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nOHEM+FRCN [113]\ntrain\n22.6\n42.5 22.2 5.0 23.7 34.6\n-\n-\n-\n-\n-\n-\nOHEM+FRCN* [113]\ntrain\n24.4\n44.4 24.8 7.1 26.4 37.9\n-\n-\n-\n-\n-\n-\nOHEM+FRCN* [113]\ntrainval\n25.5\n45.9 26.1 7.4 27.7 38.5\n-\n-\n-\n-\n-\n-\nFaster R-CNN [18]\ntrainval\n24.2\n45.3 23.5 7.7 26.4 37.1 23.8 34.0 34.6 12.0 38.5 54.4\nYOLOv2 [72]\ntrainval35k\n21.6\n44.0 19.2 5.0 22.4 35.5 20.7 31.6 33.3 9.8 36.5 54.4\nSSD300 [71]\ntrainval35k\n23.2\n41.2 23.4 5.3 23.2 39.6 22.5 33.2 35.3 9.6 37.6 56.5\nSSD512 [71]\ntrainval35k\n26.8\n46.5 27.8 9.0 28.9 41.9 24.8 37.5 39.8 14.0 43.5 59.0\nR-FCN (ResNet101) [65]\ntrainval\n29.2\n51.5\n-\n10.8 32.8 45.0\n-\n-\n-\n-\n-\n-\nR-FCN*(ResNet101) [65]\ntrainval\n29.9\n51.9\n-\n10.4 32.4 43.3\n-\n-\n-\n-\n-\n-\nR-FCN**(ResNet101) [65]\ntrainval\n31.5\n53.2\n-\n14.3 35.5 44.2\n-\n-\n-\n-\n-\n-\nMulti-path [112]\ntrainval\n33.2\n51.9 36.3 13.6 37.2 47.8 29.9 46.0 48.3 23.4 56.0 66.4\nFPN (ResNet101) [66]\ntrainval35k\n36.2\n59.1 39.0 18.2 39.0 48.2\n-\n-\n-\n-\n-\n-\nMask (ResNet101+FPN) [67]\ntrainval35k\n38.2\n60.3 41.7 20.1 41.1 50.2\n-\n-\n-\n-\n-\n-\nMask (ResNeXt101+FPN) [67] trainval35k\n39.8\n62.3 43.4 22.1 43.2 51.2\n-\n-\n-\n-\n-\n-\nDSSD513 (ResNet101) [73]\ntrainval35k\n33.2\n53.3 35.2 13.0 35.4 51.1 28.9 43.5 46.2 21.8 49.1 66.4\nDSOD300 [74]\ntrainval\n29.3\n47.3 30.6 9.4 31.5 47.0 27.3 40.7 43.0 16.7 47.1 65.0\n* FRCN*: Fast R-CNN with multi-scale training, R-FCN*: R-FCN with multi-scale training, R-FCN**: R-FCN\nwith multi-scale training and testing, Mask: Mask R-CNN.\nX GPU. Except for \u2018SS\u2019 which is processed with CPU, the\nother procedures related to CNN are all evaluated on GPU.\nFrom Table V, we can draw some conclusions as follows.\n\u2022 By computing CNN features on shared feature maps\n(SPP-net), test consumption is reduced largely. Test time is\nfurther reduced with the uni\ufb01ed multi-task learning (FRCN)\nand removal of additional region proposal generation stage\n(Faster R-CNN). It\u2019s also helpful to compress the parameters\nof FC layers with SVD [91] (PAVNET and FRCN).\nTABLE V\nCOMPARISON OF TESTING CONSUMPTION ON VOC 07 TEST SET.\nMethods\nTrained on\nmAP(%)\nTest time(sec/img)\nRate(FPS)\nSS+R-CNN [15]\n07\n66.0\n32.84\n0.03\nSS+SPP-net [64]\n07\n63.1\n2.3\n0.44\nSS+FRCN [16]\n07+12\n66.9\n1.72\n0.6\nSDP+CRC [33]\n07\n68.9\n0.47\n2.1\nSS+HyperNet* [101]\n07+12\n76.3\n0.20\n5\nMR-CNN&S-CNN [110]\n07+12\n78.2\n30\n0.03\nION [95]\n07+12+S\n79.2\n1.92\n0.5\nFaster R-CNN(VGG16) [18]\n07+12\n73.2\n0.11\n9.1\nFaster R-CNN(ResNet101) [18]\n07+12\n83.8\n2.24\n0.4\nYOLO [17]\n07+12\n63.4\n0.02\n45\nSSD300 [71]\n07+12\n74.3\n0.02\n46\nSSD512 [71]\n07+12\n76.8\n0.05\n19\nR-FCN(ResNet101) [65]\n07+12+coco\n83.6\n0.17\n5.9\nYOLOv2(544*544) [72]\n07+12\n78.6\n0.03\n40\nDSSD321(ResNet101) [73]\n07+12\n78.6\n0.07\n13.6\nDSOD300 [74]\n07+12+coco\n81.7\n0.06\n17.4\nPVANET+ [116]\n07+12+coco\n83.8\n0.05\n21.7\nPVANET+(compress) [116]\n07+12+coco\n82.9\n0.03\n31.3\n* SS: Selective Search [15], SS*: \u2018fast mode\u2019 Selective Search [16], HyperNet*: the speed up version of\nHyperNet and PAVNET+ (compresss): PAVNET with additional bounding box voting and compressed fully\nconvolutional layers.\n\u2022 It takes additional test time to extract multi-scale fea-\ntures and contextual information (ION and MR-RCNN&S-\nRCNN).\n\u2022 It takes more time to train a more complex and deeper\nnetwork (ResNet101 against VGG16) and this time con-\nsumption can be reduced by adding as many layers into\nshared fully convolutional layers as possible (FRCN).\n\u2022 Regression based models can usually be processed in real-\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n12\ntime at the cost of a drop in accuracy compared with region\nproposal based models. Also, region proposal based models\ncan be modi\ufb01ed into real-time systems with the introduction\nof other tricks [116] (PVANET), such as BN [43], residual\nconnections [123].\nIV. SALIENT OBJECT DETECTION\nVisual saliency detection, one of the most important and\nchallenging tasks in computer vision, aims to highlight the\nmost dominant object regions in an image. Numerous ap-\nplications incorporate the visual saliency to improve their\nperformance, such as image cropping [125] and segmentation\n[126], image retrieval [57] and object detection [66].\nBroadly, there are two branches of approaches in salient\nobject detection, namely bottom-up (BU) [127] and top-down\n(TD) [128]. Local feature contrast plays the central role in BU\nsalient object detection, regardless of the semantic contents of\nthe scene. To learn local feature contrast, various local and\nglobal features are extracted from pixels, e.g. edges [129],\nspatial information [130]. However, high-level and multi-scale\nsemantic information cannot be explored with these low-level\nfeatures. As a result, low contrast salient maps instead of\nsalient objects are obtained. TD salient object detection is task-\noriented and takes prior knowledge about object categories\nto guide the generation of salient maps. Taking semantic\nsegmentation as an example, a saliency map is generated in the\nsegmentation to assign pixels to particular object categories via\na TD approach [131]. In a word, TD saliency can be viewed\nas a focus-of-attention mechanism, which prunes BU salient\npoints that are unlikely to be parts of the object [132].\nA. Deep learning in Salient Object Detection\nDue to the signi\ufb01cance for providing high-level and multi-\nscale feature representation and the successful applications\nin many correlated computer vision tasks, such as semantic\nsegmentation [131], edge detection [133] and generic object\ndetection [16], it is feasible and necessary to extend CNN to\nsalient object detection.\nThe early work by Eleonora Vig et al. [28] follows a\ncompletely automatic data-driven approach to perform a large-\nscale search for optimal features, namely an ensemble of deep\nnetworks with different layers and parameters. To address the\nproblem of limited training data, Kummerer et al. proposed the\nDeep Gaze [134] by transferring from the AlexNet to generate\na high dimensional feature space and create a saliency map. A\nsimilar architecture was proposed by Huang et al. to integrate\nsaliency prediction into pre-trained object recognition DNNs\n[135]. The transfer is accomplished by \ufb01ne-tuning DNNs\u2019\nweights with an objective function based on the saliency\nevaluation metrics, such as Similarity, KL-Divergence and\nNormalized Scanpath Saliency.\nSome works combined local and global visual clues to\nimprove salient object detection performance. Wang et al.\ntrained two independent deep CNNs (DNN-L and DNN-G)\nto capture local information and global contrast and predicted\nsaliency maps by integrating both local estimation and global\nsearch [136]. Cholakkal et al. proposed a weakly supervised\nsaliency detection framework to combine visual saliency from\nbottom-up and top-down saliency maps, and re\ufb01ned the results\nwith a multi-scale superpixel-averaging [137]. Zhao et al.\nproposed a multi-context deep learning framework, which\nutilizes a uni\ufb01ed learning framework to model global and\nlocal context jointly with the aid of superpixel segmentation\n[138]. To predict saliency in videos, Bak et al. fused two\nstatic saliency models, namely spatial stream net and tem-\nporal stream net, into a two-stream framework with a novel\nempirically grounded data augmentation technique [139].\nComplementary information from semantic segmentation\nand context modeling is bene\ufb01cial. To learn internal represen-\ntations of saliency ef\ufb01ciently, He et al. proposed a novel su-\nperpixelwise CNN approach called SuperCNN [140], in which\nsalient object detection is formulated as a binary labeling\nproblem. Based on a fully convolutional neural network, Li\net al. proposed a multi-task deep saliency model, in which\nintrinsic correlations between saliency detection and semantic\nsegmentation are set up [141]. However, due to the conv layers\nwith large receptive \ufb01elds and pooling layers, blurry object\nboundaries and coarse saliency maps are produced. Tang et\nal. proposed a novel saliency detection framework (CRPSD)\n[142], which combines region-level saliency estimation and\npixel-level saliency prediction together with three closely\nrelated CNNs. Li et al. proposed a deep contrast network\nto combine segment-wise spatial pooling and pixel-level fully\nconvolutional streams [143].\nThe proper integration of multi-scale feature maps is also\nof signi\ufb01cance for improving detection performance. Based\non Fast R-CNN, Wang et al. proposed the RegionNet by\nperforming salient object detection with end-to-end edge pre-\nserving and multi-scale contextual modelling [144]. Liu et al.\n[27] proposed a multi-resolution convolutional neural network\n(Mr-CNN) to predict eye \ufb01xations, which is achieved by\nlearning both bottom-up visual saliency and top-down visual\nfactors from raw image data simultaneously. Cornia et al.\nproposed an architecture which combines features extracted at\ndifferent levels of the CNN [145]. Li et al. proposed a multi-\nscale deep CNN framework to extract three scales of deep\ncontrast features [146], namely the mean-subtracted region,\nthe bounding box of its immediate neighboring regions and\nthe masked entire image, from each candidate region.\nIt is ef\ufb01cient and accurate to train a direct pixel-wise\nCNN architecture to predict salient objects with the aids of\nRNNs and deconvolution networks. Pan et al. formulated\nsaliency prediction as a minimization optimization on the\nEuclidean distance between the predicted saliency map and\nthe ground truth and proposed two kinds of architectures\n[147]: a shallow one trained from scratch and a deeper one\nadapted from deconvoluted VGG network. As convolutional-\ndeconvolution networks are not expert in recognizing objects\nof multiple scales, Kuen et al. proposed a recurrent attentional\nconvolutional-deconvolution network (RACDNN) with several\nspatial transformer and recurrent network units to conquer\nthis problem [148]. To fuse local, global and contextual\ninformation of salient objects, Tang et al. developed a deeply-\nsupervised recurrent convolutional neural network (DSRCNN)\nto perform a full image-to-image saliency detection [149].\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n13\nB. Experimental Evaluation\nFour representative datasets, including ECSSD [156], HKU-\nIS [146], PASCALS [157], and SOD [158], are used to\nevaluate several state-of-the-art methods. ECSSD consists of\n1000 structurally complex but semantically meaningful natural\nimages. HKU-IS is a large-scale dataset containing over 4000\nchallenging images. Most of these images have more than\none salient object and own low contrast. PASCALS is a\nsubset chosen from the validation set of PASCAL VOC 2010\nsegmentation dataset and is composed of 850 natural images.\nThe SOD dataset possesses 300 images containing multiple\nsalient objects. The training and validation sets for different\ndatasets are kept the same as those in [152].\nTwo standard metrics, namely F-measure and the mean\nabsolute error (MAE), are utilized to evaluate the quality of a\nsaliency map. Given precision and recall values pre-computed\non the union of generated binary mask B and ground truth Z,\nF-measure is de\ufb01ned as below\nF\u03b2 = (1 + \u03b22)Presion \u00d7 Recall\n\u03b22Presion + Recall\n(7)\nwhere \u03b22 is set to 0.3 in order to stress the importance of the\nprecision value.\nThe MAE score is computed with the following equation\nMAE =\n1\nH \u00d7 W\nH\nX\ni=1\nW\nX\nj=1\n\f\f\f \u02c6S(i, j) = \u02c6Z(i, j)\n\f\f\f\n(8)\nwhere \u02c6Z and \u02c6S represent the ground truth and the continuous\nsaliency map, respectively. W and H are the width and\nheight of the salient area, respectively. This score stresses\nthe importance of successfully detected salient objects over\ndetected non-salient pixels [159].\nThe following approaches are evaluated: CHM [150], RC\n[151], DRFI [152], MC [138], MDF [146], LEGS [136], DSR\n[149], MTDNN [141], CRPSD [142], DCL [143], ELD [153],\nNLDF [154] and DSSC [155]. Among these methods, CHM,\nRC and DRFI are classical ones with the best performance\n[159], while the other methods are all associated with CNN.\nF-measure and MAE scores are shown in Table VI.\nFrom Table VI, we can \ufb01nd that CNN based methods\nperform better than classic methods. MC and MDF combine\nthe information from local and global context to reach a\nmore accurate saliency. ELD refers to low-level handcrafted\nfeatures for complementary information. LEGS adopts generic\nregion proposals to provide initial salient regions, which may\nbe insuf\ufb01cient for salient detection. DSR and MT act in\ndifferent ways by introducing recurrent network and semantic\nsegmentation, which provide insights for future improvements.\nCPRSD, DCL, NLDF and DSSC are all based on multi-scale\nrepresentations and superpixel segmentation, which provide\nrobust salient regions and smooth boundaries. DCL, NLDF\nand DSSC perform the best on these four datasets. DSSC\nearns the best performance by modelling scale-to-scale short-\nconnections.\nOverall, as CNN mainly provides salient information in\nlocal regions, most of CNN based methods need to model\nvisual saliency along region boundaries with the aid of su-\nperpixel segmentation. Meanwhile, the extraction of multi-\nscale deep CNN features is of signi\ufb01cance for measuring local\nconspicuity. Finally, it\u2019s necessary to strengthen local con-\nnections between different CNN layers and as well to utilize\ncomplementary information from local and global context.\nV. FACE DETECTION\nFace detection is essential to many face applications and acts\nas an important pre-processing procedure to face recognition\n[160]\u2013[162], face synthesis [163], [164] and facial expression\nanalysis [165]. Different from generic object detection, this\ntask is to recognize and locate face regions covering a very\nlarge range of scales (30-300 pts vs. 10-1000 pts). At the same\ntime, faces have their unique object structural con\ufb01gurations\n(e.g. the distribution of different face parts) and characteristics\n(e.g. skin color). All these differences lead to special attention\nto this task. However, large visual variations of faces, such as\nocclusions, pose variations and illumination changes, impose\ngreat challenges for this task in real applications.\nThe most famous face detector proposed by Viola and\nJones [166] trains cascaded classi\ufb01ers with Haar-Like features\nand AdaBoost, achieving good performance with real-time\nef\ufb01ciency. However, this detector may degrade signi\ufb01cantly\nin real-world applications due to larger visual variations of\nhuman faces. Different from this cascade structure, Felzen-\nszwalb et al. proposed a deformable part model (DPM) for face\ndetection [24]. However, for these traditional face detection\nmethods, high computational expenses and large quantities\nof annotations are required to achieve a reasonable result.\nBesides, their performance is greatly restricted by manually\ndesigned features and shallow architecture.\nA. Deep learning in Face Detection\nRecently, some CNN based face detection approaches have\nbeen proposed [167]\u2013[169].As less accurate localization re-\nsults from independent regressions of object coordinates, Yu\net al. [167] proposed a novel IoU loss function for predicting\nthe four bounds of box jointly. Farfade et al. [168] proposed a\nDeep Dense Face Detector (DDFD) to conduct multi-view face\ndetection, which is able to detect faces in a wide range of ori-\nentations without requirement of pose/landmark annotations.\nYang et al. proposed a novel deep learning based face detection\nframework [169], which collects the responses from local fa-\ncial parts (e.g. eyes, nose and mouths) to address face detection\nunder severe occlusions and unconstrained pose variations.\nYang et al. [170] proposed a scale-friendly detection network\nnamed ScaleFace, which splits a large range of target scales\ninto smaller sub-ranges. Different specialized sub-networks are\nconstructed on these sub-scales and combined into a single\none to conduct end-to-end optimization. Hao et al. designed an\nef\ufb01cient CNN to predict the scale distribution histogram of the\nfaces and took this histogram to guide the zoom-in and zoom-\nout of the image [171]. Since the faces are approximately\nin uniform scale after zoom, compared with other state-of-\nthe-art baselines, better performance is achieved with less\ncomputation cost. Besides, some generic detection frameworks\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n14\nTABLE VI\nCOMPARISON BETWEEN STATE OF THE ART METHODS.\nDataset\nMetrics\nCHM [150]\nRC [151]\nDRFI [152]\nMC [138]\nMDF [146]\nLEGS [136]\nDSR [149]\nMTDNN [141]\nCRPSD [142]\nDCL [143]\nELD [153]\nNLDF [154]\nDSSC [155]\nPASCAL-S\nwF\u03b2\n0.631\n0.640\n0.679\n0.721\n0.764\n0.756\n0.697\n0.818\n0.776\n0.822\n0.767\n0.831\n0.830\nMAE\n0.222\n0.225\n0.221\n0.147\n0.145\n0.157\n0.128\n0.170\n0.063\n0.108\n0.121\n0.099\n0.080\nECSSD\nwF\u03b2\n0.722\n0.741\n0.787\n0.822\n0.833\n0.827\n0.872\n0.810\n0.849\n0.898\n0.865\n0.905\n0.915\nMAE\n0.195\n0.187\n0.166\n0.107\n0.108\n0.118\n0.037\n0.160\n0.046\n0.071\n0.098\n0.063\n0.052\nHKU-IS\nwF\u03b2\n0.728\n0.726\n0.783\n0.781\n0.860\n0.770\n0.833\n-\n0.821\n0.907\n0.844\n0.902\n0.913\nMAE\n0.158\n0.165\n0.143\n0.098\n0.129\n0.118\n0.040\n-\n0.043\n0.048\n0.071\n0.048\n0.039\nSOD\nwF\u03b2\n0.655\n0.657\n0.712\n0.708\n0.785\n0.707\n-\n0.781\n-\n0.832\n0.760\n0.810\n0.842\nMAE\n0.249\n0.242\n0.215\n0.184\n0.155\n0.205\n-\n0.150\n-\n0.126\n0.154\n0.143\n0.118\n* The bigger wF\u03b2 is or the smaller MAE is, the better the performance is.\nare extended to face detection with different modi\ufb01cations, e.g.\nFaster R-CNN [29], [172], [173].\nSome authors trained CNNs with other complementary\ntasks, such as 3D modelling and face landmarks, in a multi-\ntask learning manner. Huang et al. proposed a uni\ufb01ed end-\nto-end FCN framework called DenseBox to jointly conduct\nface detection and landmark localization [174]. Li et al.\n[175] proposed a multi-task discriminative learning framework\nwhich integrates a ConvNet with a \ufb01xed 3D mean face model\nin an end-to-end manner. In the framework, two issues are\naddressed to transfer from generic object detection to face\ndetection, namely eliminating prede\ufb01ned anchor boxes by a\n3D mean face model and replacing RoI pooling layer with\na con\ufb01guration pooling layer. Zhang et al. [176] proposed a\ndeep cascaded multi-task framework named MTCNN which\nexploits the inherent correlations between face detection and\nalignment in unconstrained environment to boost up detection\nperformance in a coarse-to-\ufb01ne manner.\nReducing computational expenses is of necessity in real ap-\nplications. To achieve real-time detection on mobile platform,\nKalinovskii and Spitsyn proposed a new solution of frontal\nface detection based on compact CNN cascades [177]. This\nmethod takes a cascade of three simple CNNs to generate,\nclassify and re\ufb01ne candidate object positions progressively.\nTo reduce the effects of large pose variations, Chen et al.\nproposed a cascaded CNN denoted by Supervised Transformer\nNetwork [31]. This network takes a multi-task RPN to predict\ncandidate face regions along with associated facial landmarks\nsimultaneously, and adopts a generic R-CNN to verify the\nexistence of valid faces. Yang et al. proposed a three-stage\ncascade structure based on FCNs [8], while in each stage, a\nmulti-scale FCN is utilized to re\ufb01ne the positions of possible\nfaces. Qin et al. proposed a uni\ufb01ed framework which achieves\nbetter results with the complementary information from dif-\nferent jointly trained CNNs [178].\nB. Experimental Evaluation\nThe FDDB [179] dataset has a total of 2,845 pictures in\nwhich 5,171 faces are annotated with elliptical shape. Two\ntypes of evaluations are used: the discrete score and continuous\nscore. By varying the threshold of the decision rule, the ROC\ncurve for the discrete scores can re\ufb02ect the dependence of\nthe detected face fractions on the number of false alarms.\nCompared with annotations, any detection with an IoU ratio\nexceeding 0.5 is treated as positive. Each annotation is only\nassociated with one detection. The ROC curve for the contin-\nuous scores is the re\ufb02ection of face localization quality.\nThe evaluated models cover DDFD [168], CascadeCNN\n[180], ACF-multiscale [181], Pico [182], HeadHunter [183],\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0\n 500\n 1000\n 1500\n 2000\nTrue positive rate\nFalse positive\nDDFD\nCascadeCNN\nACF-multiscale\nPico\nHeadHunter\nJoint Cascade\nSURF-multiview\nViola-Jones\nNPDFace\nFaceness\nCCF\nMTCNN\nConv3D\nHyperface\nUnitBox\nLDCF+\nDeepIR\nHR-ER\nFace-R-CNN\nScaleFace\n(a) Discrete ROC curves\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0\n 500\n 1000\n 1500\n 2000\nTrue positive rate\nFalse positive\nDDFD\nCascadeCNN\nACF-multiscale\nPico\nHeadHunter\nJoint Cascade\nSURF-multiview\nViola-Jones\nNPDFace\nFaceness\nCCF\nMTCNN\nConv3D\nHyperface\nUnitBox\nLDCF+\nDeepIR\nHR-ER\nFace-R-CNN\nScaleFace\n(b) Continuous ROC curves\nFig. 11. The ROC curves of state-of-the-art methods on FDDB.\nJoint Cascade [30], SURF-multiview [184], Viola-Jones [166],\nNPDFace [185], Faceness [169], CCF [186], MTCNN [176],\nConv3D [175], Hyperface [187], UnitBox [167], LDCF+ [S2],\nDeepIR [173], HR-ER [188], Face-R-CNN [172] and Scale-\nFace [170]. ACF-multiscale, Pico, HeadHunter, Joint Cascade,\nSURF-multiview, Viola-Jones, NPDFace and LDCF+ are built\non classic hand-crafted features while the rest methods are\nbased on deep CNN features. The ROC curves are shown in\nFigure 11.\nFrom Figure 11(a), in spite of relatively competitive results\nproduced by LDCF+, it can be observed that most of classic\nmethods perform with similar results and are outperformed\nby CNN based methods by a signi\ufb01cant margin. From Figure\n11(b), it can be observed that most of CNN based methods\nearn similar true positive rates between 60% and 70% while\nDeepIR and HR-ER perform much better than them. Among\nclassic methods, Joint Cascade is still competitive. As earlier\nworks, DDFD and CCF directly make use of generated feature\nmaps and obtain relatively poor results. CascadeCNN builds\ncascaded CNNs to locate face regions, which is ef\ufb01cient but in-\naccurate. Faceness combines the decisions from different part\ndetectors, resulting in precise face localizations while being\ntime-consuming. The outstanding performance of MTCNN,\nConv3D and Hyperface proves the effectiveness of multi-task\nlearning. HR-ER and ScaleFace adaptively detect faces of\ndifferent scales, and make a balance between accuracy and\nef\ufb01ciency. DeepIR and Face-R-CNN are two extensions of the\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n15\nFaster R-CNN architecture to face detection, which validate\nthe signi\ufb01cance and effectiveness of Faster R-CNN. Unitbox\nprovides an alternative choice for performance improvements\nby carefully designing optimization loss.\nFrom these results, we can draw the conclusion that\nCNN based methods are in the leading position. The perfor-\nmance can be improved by the following strategies: designing\nnovel optimization loss, modifying generic detection pipelines,\nbuilding meaningful network cascades, adapting scale-aware\ndetection and learning multi-task shared CNN features.\nVI. PEDESTRIAN DETECTION\nRecently, pedestrian detection has been intensively studied,\nwhich has a close relationship to pedestrian tracking [189],\n[190], person re-identi\ufb01cation [191], [192] and robot naviga-\ntion [193], [194]. Prior to the recent progress in DCNN based\nmethods [195], [196], some researchers combined boosted\ndecision forests with hand-crafted features to obtain pedestrian\ndetectors [197]\u2013[199]. At the same time, to explicitly model\nthe deformation and occlusion, part-based models [200] and\nexplicit occlusion handling [201], [202] are of concern.\nAs there are many pedestrian instances of small sizes\nin typical scenarios of pedestrian detection (e.g. automatic\ndriving and intelligent surveillance), the application of RoI\npooling layer in generic object detection pipeline may result\nin \u2018plain\u2019 features due to collapsing bins. In the meantime, the\nmain source of false predictions in pedestrian detection is the\nconfusion of hard background instances, which is in contrast\nto the interference from multiple categories in generic object\ndetection. As a result, different con\ufb01gurations and components\nare required to accomplish accurate pedestrian detection.\nA. Deep learning in Pedestrian Detection\nAlthough DCNNs have obtained excellent performance on\ngeneric object detection [16], [72], none of these approaches\nhave achieved better results than the best hand-crafted feature\nbased method [198] for a long time, even when part-based\ninformation and occlusion handling are incorporated [202].\nThereby, some researches have been conducted to analyze the\nreasons. Zhang et al. attempted to adapt generic Faster R-CNN\n[18] to pedestrian detection [203]. They modi\ufb01ed the down-\nstream classi\ufb01er by adding boosted forests to shared, high-\nresolution conv feature maps and taking a RPN to handle small\ninstances and hard negative examples. To deal with complex\nocclusions existing in pedestrian images, inspired by DPM\n[24], Tian et al. proposed a deep learning framework called\nDeepParts [204], which makes decisions based an ensemble of\nextensive part detectors. DeepParts has advantages in dealing\nwith weakly labeled data, low IoU positive proposals and\npartial occlusion.\nOther researchers also tried to combine complementary in-\nformation from multiple data sources. CompACT-Deep adopts\na complexity-aware cascade to combine hand-crafted features\nand \ufb01ne-tuned DCNNs [195]. Based on Faster R-CNN, Liu et\nal. proposed multi-spectral deep neural networks for pedestrian\ndetection to combine complementary information from color\nand thermal images [205]. Tian et al. [206] proposed a task-\nassistant CNN (TA-CNN) to jointly learn multiple tasks with\nTABLE VII\nDETAILED BREAKDOWN PERFORMANCE COMPARISONS OF\nSTATE-OF-THE-ART MODELS ON CALTECH PEDESTRIAN DATASET. ALL\nNUMBERS ARE REPORTED IN L-AMR.\nMethod\nReasonable\nAll\nFar\nMedium\nNear\nnone\npartial\nheavy\nCheckerboards+ [198]\n17.1\n68.4\n100\n58.3\n5.1\n15.6\n31.4\n78.4\nLDCF++[S2]\n15.2\n67.1\n100\n58.4\n5.4\n13.3\n33.3\n76.2\nSCF+AlexNet [210]\n23.3\n70.3\n100\n62.3\n10.2\n20.0\n48.5\n74.7\nSA-FastRCNN [211]\n9.7\n62.6\n100\n51.8\n0\n7.7\n24.8\n64.3\nMS-CNN [105]\n10.0\n61.0\n97.2\n49.1\n2.6\n8.2\n19.2\n60.0\nDeepParts [204]\n11.9\n64.8\n100\n56.4\n4.8\n10.6\n19.9\n60.4\nCompACT-Deep [195]\n11.8\n64.4\n100\n53.2\n4.0\n9.6\n25.1\n65.8\nRPN+BF [203]\n9.6\n64.7\n100\n53.9\n2.3\n7.7\n24.2\n74.2\nF-DNN+SS [207]\n8.2\n50.3\n77.5\n33.2\n2.8\n6.7\n15.1\n53.4\nmultiple data sources and to combine pedestrian attributes\nwith semantic scene attributes together. Du et al. proposed\na deep neural network fusion architecture for fast and robust\npedestrian detection [207]. Based on the candidate bounding\nboxes generated with SSD detectors [71], multiple binary\nclassi\ufb01ers are processed parallelly to conduct soft-rejection\nbased network fusion (SNF) by consulting their aggregated\ndegree of con\ufb01dences.\nHowever, most of these approaches are much more sophisti-\ncated than the standard R-CNN framework. CompACT-Deep\nconsists of a variety of hand-crafted features, a small CNN\nmodel and a large VGG16 model [195]. DeepParts contains\n45 \ufb01ne-tuned DCNN models, and a set of strategies, including\nbounding box shifting handling and part selection, are required\nto arrive at the reported results [204]. So the modi\ufb01cation and\nsimpli\ufb01cation is of signi\ufb01cance to reduce the burden on both\nsoftware and hardware to satisfy real-time detection demand.\nTome et al. proposed a novel solution to adapt generic object\ndetection pipeline to pedestrian detection by optimizing most\nof its stages [59]. Hu et al. [208] trained an ensemble of\nboosted decision models by reusing the conv feature maps, and\na further improvement was gained with simple pixel labelling\nand additional complementary hand-crafted features. Tome\net al. [209] proposed a reduced memory region based deep\nCNN architecture, which fuses regional responses from both\nACF detectors and SVM classi\ufb01ers into R-CNN. Ribeiro et\nal. addressed the problem of Human-Aware Navigation [32]\nand proposed a vision-based person tracking system guided\nby multiple camera sensors.\nB. Experimental Evaluation\nThe evaluation is conducted on the most popular Caltech\nPedestrian dataset [3]. The dataset was collected from the\nvideos of a vehicle driving through an urban environment\nand consists of 250,000 frames with about 2300 unique\npedestrians and 350,000 annotated bounding boxes (BBs).\nThree kinds of labels, namely \u2018Person (clear identi\ufb01cations)\u2019,\n\u2018Person? (unclear identi\ufb01cations)\u2019 and \u2018People (large group of\nindividuals)\u2019, are assigned to different BBs. The performance\nis measured with the log-average miss rate (L-AMR) which\nis computed evenly spaced in log-space in the range 10\u22122 to\n1 by averaging miss rate at the rate of nine false positives\nper image (FPPI) [3]. According to the differences in the\nheight and visible part of the BBs, a total of 9 popular settings\nare adopted to evaluate different properties of these models.\nDetails of these settings are as [3].\nEvaluated methods include Checkerboards+ [198], LDCF++\n[S2], SCF+AlexNet [210], SA-FastRCNN [211], MS-CNN\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n16\n[105], DeepParts [204], CompACT-Deep [195], RPN+BF\n[203] and F-DNN+SS [207]. The \ufb01rst two methods are based\non hand-crafted features while the rest ones rely on deep CNN\nfeatures. All results are exhibited in Table VII. From this table,\nwe observe that different from other tasks, classic handcrafted\nfeatures can still earn competitive results with boosted decision\nforests [203], ACF [197] and HOG+LUV channels [S2]. As\nan early attempt to adapt CNN to pedestrian detection, the\nfeatures generated by SCF+AlexNet are not so discriminant\nand produce relatively poor results. Based on multiple CNNs,\nDeepParts and CompACT-Deep accomplish detection tasks via\ndifferent strategies, namely local part integration and cascade\nnetwork. The responses from different local part detectors\nmake DeepParts robust to partial occlusions. However, due to\ncomplexity, it is too time-consuming to achieve real-time de-\ntection. The multi-scale representation of MS-CNN improves\naccuracy of pedestrian locations. SA-FastRCNN extends Fast\nR-CNN to automatically detecting pedestrians according to\ntheir different scales, which has trouble when there are partial\nocclusions. RPN+BF combines the detectors produced by\nFaster R-CNN with boosting decision forest to accurately\nlocate different pedestrians. F-DNN+SS, which is composed\nof multiple parallel classi\ufb01ers with soft rejections, performs\nthe best followed by RPN+BF, SA-FastRCNN and MS-CNN.\nIn short, CNN based methods can provide more accurate\ncandidate boxes and multi-level semantic information for\nidentifying and locating pedestrians. Meanwhile, handcrafted\nfeatures are complementary and can be combined with CNN\nto achieve better results. The improvements over existing CNN\nmethods can be obtained by carefully designing the framework\nand classi\ufb01ers, extracting multi-scale and part based semantic\ninformation and searching for complementary information\nfrom other related tasks, such as segmentation.\nVII. PROMISING FUTURE DIRECTIONS AND TASKS\nIn spite of rapid development and achieved promising\nprogress of object detection, there are still many open issues\nfor future work.\nThe \ufb01rst one is small object detection such as occurring\nin COCO dataset and in face detection task. To improve\nlocalization accuracy on small objects under partial occlusions,\nit is necessary to modify network architectures from the\nfollowing aspects.\n\u2022 Multi-task joint optimization and multi-modal infor-\nmation fusion. Due to the correlations between different\ntasks within and outside object detection, multi-task joint\noptimization has already been studied by many researchers\n[16] [18]. However, apart from the tasks mentioned in\nSubs. III-A8, it is desirable to think over the characteristics\nof different sub-tasks of object detection (e.g. superpixel\nsemantic segmentation in salient object detection) and ex-\ntend multi-task optimization to other applications such as\ninstance segmentation [66], multi-object tracking [202] and\nmulti-person pose estimation [S4]. Besides, given a speci\ufb01c\napplication, the information from different modalities, such\nas text [212], thermal data [205] and images [65], can be\nfused together to achieve a more discriminant network.\n\u2022 Scale adaption. Objects usually exist in different scales,\nwhich is more apparent in face detection and pedestrian\ndetection. To increase the robustness to scale changes, it\nis demanded to train scale-invariant, multi-scale or scale-\nadaptive detectors. For scale-invariant detectors, more pow-\nerful backbone architectures (e.g. ResNext [123]), negative\nsample mining [113], reverse connection [213] and sub-\ncategory modelling [60] are all bene\ufb01cial. For multi-scale\ndetectors, both the FPN [66] which produces multi-scale\nfeature maps and Generative Adversarial Network [214]\nwhich narrows representation differences between small ob-\njects and the large ones with a low-cost architecture provide\ninsights into generating meaningful feature pyramid. For\nscale-adaptive detectors, it is useful to combine knowledge\ngraph [215], attentional mechanism [216], cascade network\n[180] and scale distribution estimation [171] to detect ob-\njects adaptively.\n\u2022 Spatial correlations and contextual modelling. Spatial\ndistribution plays an important role in object detection. So\nregion proposal generation and grid regression are taken\nto obtain probable object locations. However, the corre-\nlations between multiple proposals and object categories\nare ignored. Besides, the global structure information is\nabandoned by the position-sensitive score maps in R-FCN.\nTo solve these problems, we can refer to diverse subset\nselection [217] and sequential reasoning tasks [218] for\npossible solutions. It is also meaningful to mask salient parts\nand couple them with the global structure in a joint-learning\nmanner [219].\nThe second one is to release the burden on manual labor and\naccomplish real-time object detection, with the emergence of\nlarge-scale image and video data. The following three aspects\ncan be taken into account.\n\u2022 Cascade network. In a cascade network, a cascade of\ndetectors are built in different stages or layers [180], [220].\nAnd easily distinguishable examples are rejected at shallow\nlayers so that features and classi\ufb01ers at latter stages can\nhandle more dif\ufb01cult samples with the aid of the decisions\nfrom previous stages. However, current cascades are built in\na greedy manner, where previous stages in cascade are \ufb01xed\nwhen training a new stage. So the optimizations of different\nCNNs are isolated, which stresses the necessity of end-to-\nend optimization for CNN cascade. At the same time, it\nis also a matter of concern to build contextual associated\ncascade networks with existing layers.\n\u2022 Unsupervised and weakly supervised learning. It\u2019s\nvery time consuming to manually draw large quantities\nof bounding boxes. To release this burden, semantic prior\n[55], unsupervised object discovery [221], multiple instance\nlearning [222] and deep neural network prediction [47] can\nbe integrated to make best use of image-level supervision to\nassign object category tags to corresponding object regions\nand re\ufb01ne object boundaries. Furthermore, weakly annota-\ntions (e.g. center-click annotations [223]) are also helpful\nfor achieving high-quality detectors with modest annotation\nefforts, especially aided by the mobile platform.\n\u2022 Network optimization. Given speci\ufb01c applications and\nplatforms, it is signi\ufb01cant to make a balance among speed,\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n17\nmemory and accuracy by selecting an optimal detection\narchitecture [116], [224]. However, despite that detection\naccuracy is reduced, it is more meaningful to learn compact\nmodels with fewer number of parameters [209]. And this\nsituation can be relieved by introducing better pre-training\nschemes [225], knowledge distillation [226] and hint learn-\ning [227]. DSOD also provides a promising guideline to\ntrain from scratch to bridge the gap between different image\nsources and tasks [74].\nThe third one is to extend typical methods for 2D object de-\ntection to adapt 3D object detection and video object detection,\nwith the requirements from autonomous driving, intelligent\ntransportation and intelligent surveillance.\n\u2022 3D object detection. With the applications of 3D sensors\n(e.g. LIDAR and camera), additional depth information can\nbe utilized to better understand the images in 2D and extend\nthe image-level knowledge to the real world. However,\nseldom of these 3D-aware techniques aim to place correct\n3D bounding boxes around detected objects. To achieve\nbetter bounding results, multi-view representation [181] and\n3D proposal network [228] may provide some guidelines to\nencode depth information with the aid of inertial sensors\n(accelerometer and gyrometer) [229].\n\u2022 Video object detection. Temporal information across\ndifferent frames play an important role in understanding\nthe behaviors of different objects. However, the accuracy\nsuffers from degenerated object appearances (e.g., motion\nblur and video defocus) in videos and the network is\nusually not trained end-to-end. To this end, spatiotemporal\ntubelets [230], optical \ufb02ow [199] and LSTM [107] should\nbe considered to fundamentally model object associations\nbetween consecutive frames.\nVIII. CONCLUSION\nDue to its powerful learning ability and advantages in\ndealing with occlusion, scale transformation and background\nswitches, deep learning based object detection has been a\nresearch hotspot in recent years. This paper provides a detailed\nreview on deep learning based object detection frameworks\nwhich handle different sub-problems, such as occlusion, clutter\nand low resolution, with different degrees of modi\ufb01cations\non R-CNN. The review starts on generic object detection\npipelines which provide base architectures for other related\ntasks. Then, three other common tasks, namely salient object\ndetection, face detection and pedestrian detection, are also\nbrie\ufb02y reviewed. Finally, we propose several promising future\ndirections to gain a thorough understanding of the object\ndetection landscape. This review is also meaningful for the\ndevelopments in neural networks and related learning systems,\nwhich provides valuable insights and guidelines for future\nprogress.\nACKNOWLEDGMENTS\nThis research was supported by the National Natural Sci-\nence Foundation of China (No.61672203 & 61375047 &\n91746209), the National Key Research and Development Pro-\ngram of China (2016YFB1000901), and Anhui Natural Sci-\nence Funds for Distinguished Young Scholar (No.170808J08).\nREFERENCES\n[1] P. F. Felzenszwalb, R. B. Girshick, D. Mcallester, and D. Ramanan,\n\u201cObject detection with discriminatively trained part-based models,\u201d\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 9, p. 1627, 2010.\n[2] K. K. Sung and T. Poggio, \u201cExample-based learning for view-based\nhuman face detection,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 20,\nno. 1, pp. 39\u201351, 2002.\n[3] C. Wojek, P. Dollar, B. Schiele, and P. Perona, \u201cPedestrian detection:\nAn evaluation of the state of the art,\u201d IEEE Trans. Pattern Anal. Mach.\nIntell., vol. 34, no. 4, p. 743, 2012.\n[4] H. Kobatake and Y. Yoshinaga, \u201cDetection of spicules on mammogram\nbased on skeleton analysis.\u201d IEEE Trans. Med. Imag., vol. 15, no. 3,\npp. 235\u2013245, 1996.\n[5] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell, \u201cCaffe: Convolutional architecture for\nfast feature embedding,\u201d in ACM MM, 2014.\n[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImagenet classi\ufb01cation\nwith deep convolutional neural networks,\u201d in NIPS, 2012.\n[7] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, \u201cRealtime multi-person\n2d pose estimation using part af\ufb01nity \ufb01elds,\u201d in CVPR, 2017.\n[8] Z. Yang and R. Nevatia, \u201cA multi-scale cascade fully convolutional\nnetwork face detector,\u201d in ICPR, 2016.\n[9] C. Chen, A. Seff, A. L. Kornhauser, and J. Xiao, \u201cDeepdriving:\nLearning affordance for direct perception in autonomous driving,\u201d in\nICCV, 2015.\n[10] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, \u201cMulti-view 3d object\ndetection network for autonomous driving,\u201d in CVPR, 2017.\n[11] A. Dundar, J. Jin, B. Martini, and E. Culurciello, \u201cEmbedded streaming\ndeep neural networks accelerator with applications,\u201d IEEE Trans.\nNeural Netw. & Learning Syst., vol. 28, no. 7, pp. 1572\u20131583, 2017.\n[12] R. J. Cintra, S. Duffner, C. Garcia, and A. Leite, \u201cLow-complexity\napproximate convolutional neural networks,\u201d IEEE Trans. Neural Netw.\n& Learning Syst., vol. PP, no. 99, pp. 1\u201312, 2018.\n[13] S. H. Khan, M. Hayat, M. Bennamoun, F. A. Sohel, and R. Togneri,\n\u201cCost-sensitive learning of deep feature representations from imbal-\nanced data.\u201d IEEE Trans. Neural Netw. & Learning Syst., vol. PP,\nno. 99, pp. 1\u201315, 2017.\n[14] A. Stuhlsatz, J. Lippel, and T. Zielke, \u201cFeature extraction with deep\nneural networks by a generalized discriminant analysis.\u201d IEEE Trans.\nNeural Netw. & Learning Syst., vol. 23, no. 4, pp. 596\u2013608, 2012.\n[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik, \u201cRich feature\nhierarchies for accurate object detection and semantic segmentation,\u201d\nin CVPR, 2014.\n[16] R. Girshick, \u201cFast r-cnn,\u201d in ICCV, 2015.\n[17] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \u201cYou only look\nonce: Uni\ufb01ed, real-time object detection,\u201d in CVPR, 2016.\n[18] S. Ren, K. He, R. Girshick, and J. Sun, \u201cFaster r-cnn: Towards real-\ntime object detection with region proposal networks,\u201d in NIPS, 2015,\npp. 91\u201399.\n[19] D. G. Lowe, \u201cDistinctive image features from scale-invariant key-\npoints,\u201d Int. J. of Comput. Vision, vol. 60, no. 2, pp. 91\u2013110, 2004.\n[20] N. Dalal and B. Triggs, \u201cHistograms of oriented gradients for human\ndetection,\u201d in CVPR, 2005.\n[21] R. Lienhart and J. Maydt, \u201cAn extended set of haar-like features for\nrapid object detection,\u201d in ICIP, 2002.\n[22] C. Cortes and V. Vapnik, \u201cSupport vector machine,\u201d Machine Learning,\nvol. 20, no. 3, pp. 273\u2013297, 1995.\n[23] Y. Freund and R. E. Schapire, \u201cA desicion-theoretic generalization of\non-line learning and an application to boosting,\u201d J. of Comput. & Sys.\nSci., vol. 13, no. 5, pp. 663\u2013671, 1997.\n[24] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan,\n\u201cObject detection with discriminatively trained part-based models,\u201d\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 32, pp. 1627\u20131645, 2010.\n[25] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\nserman, \u201cThe pascal visual object classes challenge 2007 (voc 2007)\nresults (2007),\u201d 2008.\n[26] Y. LeCun, Y. Bengio, and G. Hinton, \u201cDeep learning,\u201d Nature, vol.\n521, no. 7553, pp. 436\u2013444, 2015.\n[27] N. Liu, J. Han, D. Zhang, S. Wen, and T. Liu, \u201cPredicting eye \ufb01xations\nusing convolutional neural networks,\u201d in CVPR, 2015.\n[28] E. Vig, M. Dorr, and D. Cox, \u201cLarge-scale optimization of hierarchical\nfeatures for saliency prediction in natural images,\u201d in CVPR, 2014.\n[29] H. Jiang and E. Learned-Miller, \u201cFace detection with the faster r-cnn,\u201d\nin FG, 2017.\n[30] D. Chen, S. Ren, Y. Wei, X. Cao, and J. Sun, \u201cJoint cascade face\ndetection and alignment,\u201d in ECCV, 2014.\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n18\n[31] D. Chen, G. Hua, F. Wen, and J. Sun, \u201cSupervised transformer network\nfor ef\ufb01cient face detection,\u201d in ECCV, 2016.\n[32] D. Ribeiro, A. Mateus, J. C. Nascimento, and P. Miraldo, \u201cA real-time\npedestrian detector using deep learning for human-aware navigation,\u201d\narXiv:1607.04441, 2016.\n[33] F. Yang, W. Choi, and Y. Lin, \u201cExploit all the layers: Fast and accurate\ncnn object detector with scale dependent pooling and cascaded rejection\nclassi\ufb01ers,\u201d in CVPR, 2016.\n[34] P. Druzhkov and V. Kustikova, \u201cA survey of deep learning methods and\nsoftware tools for image classi\ufb01cation and object detection,\u201d Pattern\nRecognition and Image Anal., vol. 26, no. 1, p. 9, 2016.\n[35] W. Pitts and W. S. McCulloch, \u201cHow we know universals the perception\nof auditory and visual forms,\u201d The Bulletin of Mathematical Biophysics,\nvol. 9, no. 3, pp. 127\u2013147, 1947.\n[36] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, \u201cLearning internal\nrepresentation by back-propagation of errors,\u201d Nature, vol. 323, no.\n323, pp. 533\u2013536, 1986.\n[37] G. E. Hinton and R. R. Salakhutdinov, \u201cReducing the dimensionality\nof data with neural networks,\u201d Sci., vol. 313, pp. 504\u2013507, 2006.\n[38] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,\nA. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., \u201cDeep neural\nnetworks for acoustic modeling in speech recognition: The shared\nviews of four research groups,\u201d IEEE Signal Process. Mag., vol. 29,\nno. 6, pp. 82\u201397, 2012.\n[39] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet:\nA large-scale hierarchical image database,\u201d in CVPR, 2009.\n[40] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and\nG. Hinton, \u201cBinary coding of speech spectrograms using a deep auto-\nencoder,\u201d in INTERSPEECH, 2010.\n[41] G. Dahl, A.-r. Mohamed, G. E. Hinton et al., \u201cPhone recognition with\nthe mean-covariance restricted boltzmann machine,\u201d in NIPS, 2010.\n[42] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov, \u201cImproving neural networks by preventing co-\nadaptation of feature detectors,\u201d arXiv:1207.0580, 2012.\n[43] S. Ioffe and C. Szegedy, \u201cBatch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,\u201d in ICML, 2015.\n[44] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun,\n\u201cOverfeat: Integrated recognition, localization and detection using\nconvolutional networks,\u201d arXiv:1312.6229, 2013.\n[45] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,\nD. Erhan, V. Vanhoucke, and A. Rabinovich, \u201cGoing deeper with\nconvolutions,\u201d in CVPR, 2015.\n[46] K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks\nfor large-scale image recognition,\u201d arXiv:1409.1556, 2014.\n[47] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image\nrecognition,\u201d in CVPR, 2016.\n[48] V. Nair and G. E. Hinton, \u201cRecti\ufb01ed linear units improve restricted\nboltzmann machines,\u201d in ICML, 2010.\n[49] M. Oquab, L. Bottou, I. Laptev, J. Sivic et al., \u201cWeakly supervised\nobject recognition with convolutional neural networks,\u201d in NIPS, 2014.\n[50] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, \u201cLearning and transferring\nmid-level image representations using convolutional neural networks,\u201d\nin CVPR, 2014.\n[51] F. M. Wadley, \u201cProbit analysis: a statistical treatment of the sigmoid\nresponse curve,\u201d Annals of the Entomological Soc. of America, vol. 67,\nno. 4, pp. 549\u2013553, 1947.\n[52] K. Kavukcuoglu, R. Fergus, Y. LeCun et al., \u201cLearning invariant\nfeatures through topographic \ufb01lter maps,\u201d in CVPR, 2009.\n[53] K. Kavukcuoglu, P. Sermanet, Y.-L. Boureau, K. Gregor, M. Mathieu,\nand Y. LeCun, \u201cLearning convolutional feature hierarchies for visual\nrecognition,\u201d in NIPS, 2010.\n[54] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus, \u201cDeconvolu-\ntional networks,\u201d in CVPR, 2010.\n[55] H. Noh, S. Hong, and B. Han, \u201cLearning deconvolution network for\nsemantic segmentation,\u201d in ICCV, 2015.\n[56] Z.-Q. Zhao, B.-J. Xie, Y.-m. Cheung, and X. Wu, \u201cPlant leaf iden-\nti\ufb01cation via a growing convolution neural network with progressive\nsample learning,\u201d in ACCV, 2014.\n[57] A. Babenko, A. Slesarev, A. Chigorin, and V. Lempitsky, \u201cNeural codes\nfor image retrieval,\u201d in ECCV, 2014.\n[58] J. Wan, D. Wang, S. C. H. Hoi, P. Wu, J. Zhu, Y. Zhang, and J. Li,\n\u201cDeep learning for content-based image retrieval: A comprehensive\nstudy,\u201d in ACM MM, 2014.\n[59] D. Tom`e, F. Monti, L. Barof\ufb01o, L. Bondi, M. Tagliasacchi, and\nS. Tubaro, \u201cDeep convolutional neural networks for pedestrian detec-\ntion,\u201d Signal Process.: Image Commun., vol. 47, pp. 482\u2013489, 2016.\n[60] Y. Xiang, W. Choi, Y. Lin, and S. Savarese, \u201cSubcategory-aware\nconvolutional neural networks for object proposals and detection,\u201d in\nWACV, 2017.\n[61] Z.-Q. Zhao, H. Bian, D. Hu, W. Cheng, and H. Glotin, \u201cPedestrian\ndetection based on fast r-cnn and batch normalization,\u201d in ICIC, 2017.\n[62] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng,\n\u201cMultimodal deep learning,\u201d in ICML, 2011.\n[63] Z. Wu, X. Wang, Y.-G. Jiang, H. Ye, and X. Xue, \u201cModeling spatial-\ntemporal clues in a hybrid deep learning framework for video classi\ufb01-\ncation,\u201d in ACM MM, 2015.\n[64] K. He, X. Zhang, S. Ren, and J. Sun, \u201cSpatial pyramid pooling in deep\nconvolutional networks for visual recognition,\u201d IEEE Trans. Pattern\nAnal. Mach. Intell., vol. 37, no. 9, pp. 1904\u20131916, 2015.\n[65] Y. Li, K. He, J. Sun et al., \u201cR-fcn: Object detection via region-based\nfully convolutional networks,\u201d in NIPS, 2016, pp. 379\u2013387.\n[66] T.-Y. Lin, P. Doll\u00b4ar, R. B. Girshick, K. He, B. Hariharan, and S. J.\nBelongie, \u201cFeature pyramid networks for object detection,\u201d in CVPR,\n2017.\n[67] K. He, G. Gkioxari, P. Doll\u00b4ar, and R. B. Girshick, \u201cMask r-cnn,\u201d in\nICCV, 2017.\n[68] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, \u201cScalable object\ndetection using deep neural networks,\u201d in CVPR, 2014.\n[69] D. Yoo, S. Park, J.-Y. Lee, A. S. Paek, and I. So Kweon, \u201cAttentionnet:\nAggregating weak directions for accurate object detection,\u201d in CVPR,\n2015.\n[70] M. Najibi, M. Rastegari, and L. S. Davis, \u201cG-cnn: an iterative grid\nbased object detector,\u201d in CVPR, 2016.\n[71] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and\nA. C. Berg, \u201cSsd: Single shot multibox detector,\u201d in ECCV, 2016.\n[72] J. Redmon and A. Farhadi, \u201cYolo9000: better, faster, stronger,\u201d\narXiv:1612.08242, 2016.\n[73] C. Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg, \u201cDssd:\nDeconvolutional single shot detector,\u201d arXiv:1701.06659, 2017.\n[74] Z. Shen, Z. Liu, J. Li, Y. G. Jiang, Y. Chen, and X. Xue, \u201cDsod:\nLearning deeply supervised object detectors from scratch,\u201d in ICCV,\n2017.\n[75] G. E. Hinton, A. Krizhevsky, and S. D. Wang, \u201cTransforming auto-\nencoders,\u201d in ICANN, 2011.\n[76] G. W. Taylor, I. Spiro, C. Bregler, and R. Fergus, \u201cLearning invariance\nthrough imitation,\u201d in CVPR, 2011.\n[77] X. Ren and D. Ramanan, \u201cHistograms of sparse codes for object\ndetection,\u201d in CVPR, 2013.\n[78] J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders,\n\u201cSelective search for object recognition,\u201d Int. J. of Comput. Vision, vol.\n104, no. 2, pp. 154\u2013171, 2013.\n[79] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun, \u201cPedestrian\ndetection with unsupervised multi-stage feature learning,\u201d in CVPR,\n2013.\n[80] P. Kr\u00a8ahenb\u00a8uhl and V. Koltun, \u201cGeodesic object proposals,\u201d in ECCV,\n2014.\n[81] P. Arbel\u00b4aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik,\n\u201cMultiscale combinatorial grouping,\u201d in CVPR, 2014.\n[82] C. L. Zitnick and P. Doll\u00b4ar, \u201cEdge boxes: Locating object proposals\nfrom edges,\u201d in ECCV, 2014.\n[83] W. Kuo, B. Hariharan, and J. Malik, \u201cDeepbox: Learning objectness\nwith convolutional networks,\u201d in ICCV, 2015.\n[84] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Doll\u00b4ar, \u201cLearning to\nre\ufb01ne object segments,\u201d in ECCV, 2016.\n[85] Y. Zhang, K. Sohn, R. Villegas, G. Pan, and H. Lee, \u201cImproving object\ndetection with deep convolutional networks via bayesian optimization\nand structured prediction,\u201d in CVPR, 2015.\n[86] S. Gupta, R. Girshick, P. Arbel\u00b4aez, and J. Malik, \u201cLearning rich features\nfrom rgb-d images for object detection and segmentation,\u201d in ECCV,\n2014.\n[87] W. Ouyang, X. Wang, X. Zeng, S. Qiu, P. Luo, Y. Tian, H. Li, S. Yang,\nZ. Wang, C.-C. Loy et al., \u201cDeepid-net: Deformable deep convolutional\nneural networks for object detection,\u201d in CVPR, 2015.\n[88] K. Lenc and A. Vedaldi, \u201cR-cnn minus r,\u201d arXiv:1506.06981, 2015.\n[89] S. Lazebnik, C. Schmid, and J. Ponce, \u201cBeyond bags of features:\nSpatial pyramid matching for recognizing natural scene categories,\u201d\nin CVPR, 2006.\n[90] F. Perronnin, J. S\u00b4anchez, and T. Mensink, \u201cImproving the \ufb01sher kernel\nfor large-scale image classi\ufb01cation,\u201d in ECCV, 2010.\n[91] J. Xue, J. Li, and Y. Gong, \u201cRestructuring of deep neural network\nacoustic models with singular value decomposition.\u201d in Interspeech,\n2013.\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n19\n[92] S. Ren, K. He, R. Girshick, and J. Sun, \u201cFaster r-cnn: Towards real-time\nobject detection with region proposal networks,\u201d IEEE Trans. Pattern\nAnal. Mach. Intell., vol. 39, no. 6, pp. 1137\u20131149, 2017.\n[93] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRethink-\ning the inception architecture for computer vision,\u201d in CVPR, 2016.\n[94] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll\u00b4ar, and C. L. Zitnick, \u201cMicrosoft coco: Common objects in\ncontext,\u201d in ECCV, 2014.\n[95] S. Bell, C. Lawrence Zitnick, K. Bala, and R. Girshick, \u201cInside-outside\nnet: Detecting objects in context with skip pooling and recurrent neural\nnetworks,\u201d in CVPR, 2016.\n[96] A. Arnab and P. H. S. Torr, \u201cPixelwise instance segmentation with a\ndynamically instantiated network,\u201d in CVPR, 2017.\n[97] J. Dai, K. He, and J. Sun, \u201cInstance-aware semantic segmentation via\nmulti-task network cascades,\u201d in CVPR, 2016.\n[98] Y. Li, H. Qi, J. Dai, X. Ji, and Y. Wei, \u201cFully convolutional instance-\naware semantic segmentation,\u201d in CVPR, 2017.\n[99] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,\n\u201cSpatial transformer networks,\u201d in CVPR, 2015.\n[100] S. Brahmbhatt, H. I. Christensen, and J. Hays, \u201cStuffnet: Using stuffto\nimprove object detection,\u201d in WACV, 2017.\n[101] T. Kong, A. Yao, Y. Chen, and F. Sun, \u201cHypernet: Towards accurate\nregion proposal generation and joint object detection,\u201d in CVPR, 2016.\n[102] A. Pentina, V. Sharmanska, and C. H. Lampert, \u201cCurriculum learning\nof multiple tasks,\u201d in CVPR, 2015.\n[103] J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim, \u201cRotating your\nface using multi-task deep neural network,\u201d in CVPR, 2015.\n[104] J. Li, X. Liang, J. Li, T. Xu, J. Feng, and S. Yan, \u201cMulti-stage object\ndetection with group recursive learning,\u201d arXiv:1608.05159, 2016.\n[105] Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos, \u201cA uni\ufb01ed multi-scale\ndeep convolutional neural network for fast object detection,\u201d in ECCV,\n2016.\n[106] Y. Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler, \u201csegdeepm:\nExploiting segmentation and context in deep neural networks for object\ndetection,\u201d in CVPR, 2015.\n[107] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki, \u201cScene labeling\nwith lstm recurrent neural networks,\u201d in CVPR, 2015.\n[108] B. Moysset, C. Kermorvant, and C. Wolf, \u201cLearning to detect and\nlocalize many objects from few examples,\u201d arXiv:1611.05664, 2016.\n[109] X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang, \u201cGated bi-\ndirectional cnn for object detection,\u201d in ECCV, 2016.\n[110] S. Gidaris and N. Komodakis, \u201cObject detection via a multi-region and\nsemantic segmentation-aware cnn model,\u201d in CVPR, 2015.\n[111] M. Schuster and K. K. Paliwal, \u201cBidirectional recurrent neural net-\nworks,\u201d IEEE Trans. Signal Process., vol. 45, pp. 2673\u20132681, 1997.\n[112] S. Zagoruyko, A. Lerer, T.-Y. Lin, P. O. Pinheiro, S. Gross, S. Chin-\ntala, and P. Doll\u00b4ar, \u201cA multipath network for object detection,\u201d\narXiv:1604.02135, 2016.\n[113] A. Shrivastava, A. Gupta, and R. Girshick, \u201cTraining region-based\nobject detectors with online hard example mining,\u201d in CVPR, 2016.\n[114] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, \u201cObject detection\nnetworks on convolutional feature maps,\u201d IEEE Trans. Pattern Anal.\nMach. Intell., vol. 39, no. 7, pp. 1476\u20131481, 2017.\n[115] W. Ouyang, X. Wang, C. Zhang, and X. Yang, \u201cFactors in \ufb01netuning\ndeep model for object detection with long-tail distribution,\u201d in CVPR,\n2016.\n[116] S. Hong, B. Roh, K.-H. Kim, Y. Cheon, and M. Park, \u201cPvanet:\nLightweight deep neural networks for real-time object detection,\u201d\narXiv:1611.08588, 2016.\n[117] W. Shang, K. Sohn, D. Almeida, and H. Lee, \u201cUnderstanding and\nimproving convolutional neural networks via concatenated recti\ufb01ed\nlinear units,\u201d in ICML, 2016.\n[118] C. Szegedy, A. Toshev, and D. Erhan, \u201cDeep neural networks for object\ndetection,\u201d in NIPS, 2013.\n[119] P. O. Pinheiro, R. Collobert, and P. Doll\u00b4ar, \u201cLearning to segment object\ncandidates,\u201d in NIPS, 2015.\n[120] C. Szegedy, S. Reed, D. Erhan, D. Anguelov, and S. Ioffe, \u201cScalable,\nhigh-quality object detection,\u201d arXiv:1412.1441, 2014.\n[121] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman,\n\u201cThe pascal visual object classes challenge 2012 (voc2012) results\n(2012),\u201d in http://www.pascal-network.org/challenges/VOC/voc2011/\nworkshop/index.html, 2011.\n[122] M. D. Zeiler and R. Fergus, \u201cVisualizing and understanding convolu-\ntional networks,\u201d in ECCV, 2014.\n[123] S. Xie, R. B. Girshick, P. Doll\u00b4ar, Z. Tu, and K. He, \u201cAggregated residual\ntransformations for deep neural networks,\u201d in CVPR, 2017.\n[124] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei,\n\u201cDeformable convolutional networks,\u201d arXiv:1703.06211, 2017.\n[125] C. Rother, L. Bordeaux, Y. Hamadi, and A. Blake, \u201cAutocollage,\u201d ACM\nTrans. on Graphics, vol. 25, no. 3, pp. 847\u2013852, 2006.\n[126] C. Jung and C. Kim, \u201cA uni\ufb01ed spectral-domain approach for saliency\ndetection and its application to automatic object segmentation,\u201d IEEE\nTrans. Image Process., vol. 21, no. 3, pp. 1272\u20131283, 2012.\n[127] W.-C. Tu, S. He, Q. Yang, and S.-Y. Chien, \u201cReal-time salient object\ndetection with a minimum spanning tree,\u201d in CVPR, 2016.\n[128] J. Yang and M.-H. Yang, \u201cTop-down visual saliency via joint crf and\ndictionary learning,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 39,\nno. 3, pp. 576\u2013588, 2017.\n[129] P. L. Rosin, \u201cA simple method for detecting salient regions,\u201d Pattern\nRecognition, vol. 42, no. 11, pp. 2363\u20132371, 2009.\n[130] T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang, and H.-Y. Shum,\n\u201cLearning to detect a salient object,\u201d IEEE Trans. Pattern Anal. Mach.\nIntell., vol. 33, no. 2, pp. 353\u2013367, 2011.\n[131] J. Long, E. Shelhamer, and T. Darrell, \u201cFully convolutional networks\nfor semantic segmentation,\u201d in CVPR, 2015.\n[132] D. Gao, S. Han, and N. Vasconcelos, \u201cDiscriminant saliency, the detec-\ntion of suspicious coincidences, and applications to visual recognition,\u201d\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 31, pp. 989\u20131005, 2009.\n[133] S. Xie and Z. Tu, \u201cHolistically-nested edge detection,\u201d in ICCV, 2015.\n[134] M. K\u00a8ummerer, L. Theis, and M. Bethge, \u201cDeep gaze i: Boost-\ning saliency prediction with feature maps trained on imagenet,\u201d\narXiv:1411.1045, 2014.\n[135] X. Huang, C. Shen, X. Boix, and Q. Zhao, \u201cSalicon: Reducing the\nsemantic gap in saliency prediction by adapting deep neural networks,\u201d\nin ICCV, 2015.\n[136] L. Wang, H. Lu, X. Ruan, and M.-H. Yang, \u201cDeep networks for saliency\ndetection via local estimation and global search,\u201d in CVPR, 2015.\n[137] H. Cholakkal, J. Johnson, and D. Rajan, \u201cWeakly supervised top-down\nsalient object detection,\u201d arXiv:1611.05345, 2016.\n[138] R. Zhao, W. Ouyang, H. Li, and X. Wang, \u201cSaliency detection by\nmulti-context deep learning,\u201d in CVPR, 2015.\n[139] C\u00b8 . Bak, A. Erdem, and E. Erdem, \u201cTwo-stream convolutional networks\nfor dynamic saliency prediction,\u201d arXiv:1607.04730, 2016.\n[140] S. He, R. W. Lau, W. Liu, Z. Huang, and Q. Yang, \u201cSupercnn: A su-\nperpixelwise convolutional neural network for salient object detection,\u201d\nInt. J. of Comput. Vision, vol. 115, no. 3, pp. 330\u2013344, 2015.\n[141] X. Li, L. Zhao, L. Wei, M.-H. Yang, F. Wu, Y. Zhuang, H. Ling, and\nJ. Wang, \u201cDeepsaliency: Multi-task deep neural network model for\nsalient object detection,\u201d IEEE Trans. Image Process., vol. 25, no. 8,\npp. 3919\u20133930, 2016.\n[142] Y. Tang and X. Wu, \u201cSaliency detection via combining region-level\nand pixel-level predictions with cnns,\u201d in ECCV, 2016.\n[143] G. Li and Y. Yu, \u201cDeep contrast learning for salient object detection,\u201d\nin CVPR, 2016.\n[144] X. Wang, H. Ma, S. You, and X. Chen, \u201cEdge preserving and\nmulti-scale contextual neural network for salient object detection,\u201d\narXiv:1608.08029, 2016.\n[145] M. Cornia, L. Baraldi, G. Serra, and R. Cucchiara, \u201cA deep multi-level\nnetwork for saliency prediction,\u201d in ICPR, 2016.\n[146] G. Li and Y. Yu, \u201cVisual saliency detection based on multiscale deep\ncnn features,\u201d IEEE Trans. Image Process., vol. 25, no. 11, pp. 5012\u2013\n5024, 2016.\n[147] J. Pan, E. Sayrol, X. Giro-i Nieto, K. McGuinness, and N. E. O\u2019Connor,\n\u201cShallow and deep convolutional networks for saliency prediction,\u201d in\nCVPR, 2016.\n[148] J. Kuen, Z. Wang, and G. Wang, \u201cRecurrent attentional networks for\nsaliency detection,\u201d in CVPR, 2016.\n[149] Y. Tang, X. Wu, and W. Bu, \u201cDeeply-supervised recurrent convolutional\nneural network for saliency detection,\u201d in ACM MM, 2016.\n[150] X. Li, Y. Li, C. Shen, A. Dick, and A. Van Den Hengel, \u201cContextual\nhypergraph modeling for salient object detection,\u201d in ICCV, 2013.\n[151] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M. Hu, \u201cGlobal\ncontrast based salient region detection,\u201d IEEE Trans. Pattern Anal.\nMach. Intell., vol. 37, no. 3, pp. 569\u2013582, 2015.\n[152] H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li, \u201cSalient object\ndetection: A discriminative regional feature integration approach,\u201d in\nCVPR, 2013.\n[153] G. Lee, Y.-W. Tai, and J. Kim, \u201cDeep saliency with encoded low level\ndistance map and high level features,\u201d in CVPR, 2016.\n[154] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M. Jodoin,\n\u201cNon-local deep features for salient object detection,\u201d in CVPR, 2017.\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n20\n[155] Q. Hou, M.-M. Cheng, X.-W. Hu, A. Borji, Z. Tu, and P. Torr,\n\u201cDeeply supervised salient object detection with short connections,\u201d\narXiv:1611.04849, 2016.\n[156] Q. Yan, L. Xu, J. Shi, and J. Jia, \u201cHierarchical saliency detection,\u201d in\nCVPR, 2013.\n[157] Y. Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille, \u201cThe secrets of\nsalient object segmentation,\u201d in CVPR, 2014.\n[158] V. Movahedi and J. H. Elder, \u201cDesign and perceptual validation of\nperformance measures for salient object segmentation,\u201d in CVPRW,\n2010.\n[159] A. Borji, M.-M. Cheng, H. Jiang, and J. Li, \u201cSalient object detection:\nA benchmark,\u201d IEEE Trans. Image Process., vol. 24, no. 12, pp. 5706\u2013\n5722, 2015.\n[160] C. Peng, X. Gao, N. Wang, and J. Li, \u201cGraphical representation for\nheterogeneous face recognition,\u201d IEEE Trans. Pattern Anal. Mach.\nIntell., vol. 39, no. 2, pp. 301\u2013312, 2015.\n[161] C. Peng, N. Wang, X. Gao, and J. Li, \u201cFace recognition from multiple\nstylistic sketches: Scenarios, datasets, and evaluation,\u201d in ECCV, 2016.\n[162] X. Gao, N. Wang, D. Tao, and X. Li, \u201cFace sketchcphoto synthesis\nand retrieval using sparse representation,\u201d IEEE Trans. Circuits Syst.\nVideo Technol., vol. 22, no. 8, pp. 1213\u20131226, 2012.\n[163] N. Wang, D. Tao, X. Gao, X. Li, and J. Li, \u201cA comprehensive survey\nto face hallucination,\u201d Int. J. of Comput. Vision, vol. 106, no. 1, pp.\n9\u201330, 2014.\n[164] C. Peng, X. Gao, N. Wang, D. Tao, X. Li, and J. Li, \u201cMultiple\nrepresentations-based face sketch-photo synthesis.\u201d IEEE Trans. Neural\nNetw. & Learning Syst., vol. 27, no. 11, pp. 2201\u20132215, 2016.\n[165] A. Majumder, L. Behera, and V. K. Subramanian, \u201cAutomatic facial\nexpression recognition system using deep network-based data fusion,\u201d\nIEEE Trans. Cybern., vol. 48, pp. 103\u2013114, 2018.\n[166] P. Viola and M. Jones, \u201cRobust real-time face detection,\u201d Int. J. of\nComput. Vision, vol. 57, no. 2, pp. 137\u2013154, 2004.\n[167] J. Yu, Y. Jiang, Z. Wang, Z. Cao, and T. Huang, \u201cUnitbox: An advanced\nobject detection network,\u201d in ACM MM, 2016.\n[168] S. S. Farfade, M. J. Saberian, and L.-J. Li, \u201cMulti-view face detection\nusing deep convolutional neural networks,\u201d in ICMR, 2015.\n[169] S. Yang, P. Luo, C.-C. Loy, and X. Tang, \u201cFrom facial parts responses\nto face detection: A deep learning approach,\u201d in ICCV, 2015.\n[170] S. Yang, Y. Xiong, C. C. Loy, and X. Tang, \u201cFace detection through\nscale-friendly deep convolutional networks,\u201d in CVPR, 2017.\n[171] Z. Hao, Y. Liu, H. Qin, J. Yan, X. Li, and X. Hu, \u201cScale-aware face\ndetection,\u201d in CVPR, 2017.\n[172] H. Wang, Z. Li, X. Ji, and Y. Wang, \u201cFace r-cnn,\u201d arXiv:1706.01061,\n2017.\n[173] X. Sun, P. Wu, and S. C. Hoi, \u201cFace detection using deep learning: An\nimproved faster rcnn approach,\u201d arXiv:1701.08289, 2017.\n[174] L. Huang, Y. Yang, Y. Deng, and Y. Yu, \u201cDensebox: Unifying landmark\nlocalization with end to end object detection,\u201d arXiv:1509.04874, 2015.\n[175] Y. Li, B. Sun, T. Wu, and Y. Wang, \u201cface detection with end-to-end\nintegration of a convnet and a 3d model,\u201d in ECCV, 2016.\n[176] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, \u201cJoint face detection and\nalignment using multitask cascaded convolutional networks,\u201d IEEE\nSignal Process. Lett., vol. 23, no. 10, pp. 1499\u20131503, 2016.\n[177] I. A. Kalinovsky and V. G. Spitsyn, \u201cCompact convolutional neural\nnetwork cascadefor face detection,\u201d in CEUR Workshop, 2016.\n[178] H. Qin, J. Yan, X. Li, and X. Hu, \u201cJoint training of cascaded cnn for\nface detection,\u201d in CVPR, 2016.\n[179] V. Jain and E. Learned-Miller, \u201cFddb: A benchmark for face detection\nin unconstrained settings,\u201d Tech. Rep., 2010.\n[180] H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua, \u201cA convolutional neural\nnetwork cascade for face detection,\u201d in CVPR, 2015.\n[181] B. Yang, J. Yan, Z. Lei, and S. Z. Li, \u201cAggregate channel features for\nmulti-view face detection,\u201d in IJCB, 2014.\n[182] N. Marku\u02c7s, M. Frljak, I. S. Pand\u02c7zi\u00b4c, J. Ahlberg, and R. Forchheimer,\n\u201cObject detection with pixel intensity comparisons organized in deci-\nsion trees,\u201d arXiv:1305.4537, 2013.\n[183] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool, \u201cFace\ndetection without bells and whistles,\u201d in ECCV, 2014.\n[184] J. Li and Y. Zhang, \u201cLearning surf cascade for fast and accurate object\ndetection,\u201d in CVPR, 2013.\n[185] S. Liao, A. K. Jain, and S. Z. Li, \u201cA fast and accurate unconstrained\nface detector,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, no. 2,\npp. 211\u2013223, 2016.\n[186] B. Yang, J. Yan, Z. Lei, and S. Z. Li, \u201cConvolutional channel features,\u201d\nin ICCV, 2015.\n[187] R. Ranjan, V. M. Patel, and R. Chellappa, \u201cHyperface: A deep multi-\ntask learning framework for face detection, landmark localization, pose\nestimation, and gender recognition,\u201d arXiv:1603.01249, 2016.\n[188] P. Hu and D. Ramanan, \u201cFinding tiny faces,\u201d in CVPR, 2017.\n[189] Z. Jiang and D. Q. Huynh, \u201cMultiple pedestrian tracking from monoc-\nular videos in an interacting multiple model framework,\u201d IEEE Trans.\nImage Process., vol. 27, pp. 1361\u20131375, 2018.\n[190] D. Gavrila and S. Munder, \u201cMulti-cue pedestrian detection and tracking\nfrom a moving vehicle,\u201d Int. J. of Comput. Vision, vol. 73, pp. 41\u201359,\n2006.\n[191] S. Xu, Y. Cheng, K. Gu, Y. Yang, S. Chang, and P. Zhou, \u201cJointly\nattentive spatial-temporal pooling networks for video-based person re-\nidenti\ufb01cation,\u201d in ICCV, 2017.\n[192] Z. Liu, D. Wang, and H. Lu, \u201cStepwise metric promotion for unsuper-\nvised video person re-identi\ufb01cation,\u201d in ICCV, 2017.\n[193] A. Khan, B. Rinner, and A. Cavallaro, \u201cCooperative robots to observe\nmoving targets: Review,\u201d IEEE Trans. Cybern., vol. 48, pp. 187\u2013198,\n2018.\n[194] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, \u201cVision meets robotics:\nThe kitti dataset,\u201d Int. J. of Robotics Res., vol. 32, pp. 1231\u20131237,\n2013.\n[195] Z. Cai, M. Saberian, and N. Vasconcelos, \u201cLearning complexity-aware\ncascades for deep pedestrian detection,\u201d in ICCV, 2015.\n[196] Y. Tian, P. Luo, X. Wang, and X. Tang, \u201cDeep learning strong parts\nfor pedestrian detection,\u201d in CVPR, 2015.\n[197] P. Doll\u00b4ar, R. Appel, S. Belongie, and P. Perona, \u201cFast feature pyramids\nfor object detection,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 36,\nno. 8, pp. 1532\u20131545, 2014.\n[198] S. Zhang, R. Benenson, and B. Schiele, \u201cFiltered channel features for\npedestrian detection,\u201d in CVPR, 2015.\n[199] S. Paisitkriangkrai, C. Shen, and A. van den Hengel, \u201cPedestrian detec-\ntion with spatially pooled features and structured ensemble learning,\u201d\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 38, pp. 1243\u20131257, 2016.\n[200] L. Lin, X. Wang, W. Yang, and J.-H. Lai, \u201cDiscriminatively trained\nand-or graph models for object shape detection,\u201d IEEE Trans. Pattern\nAnal. Mach. Intell., vol. 37, no. 5, pp. 959\u2013972, 2015.\n[201] M. Mathias, R. Benenson, R. Timofte, and L. Van Gool, \u201cHandling\nocclusions with franken-classi\ufb01ers,\u201d in ICCV, 2013.\n[202] S. Tang, M. Andriluka, and B. Schiele, \u201cDetection and tracking of\noccluded people,\u201d Int. J. of Comput. Vision, vol. 110, pp. 58\u201369, 2014.\n[203] L. Zhang, L. Lin, X. Liang, and K. He, \u201cIs faster r-cnn doing well for\npedestrian detection?\u201d in ECCV, 2016.\n[204] Y. Tian, P. Luo, X. Wang, and X. Tang, \u201cDeep learning strong parts\nfor pedestrian detection,\u201d in ICCV, 2015.\n[205] J. Liu, S. Zhang, S. Wang, and D. N. Metaxas, \u201cMultispectral deep\nneural networks for pedestrian detection,\u201d arXiv:1611.02644, 2016.\n[206] Y. Tian, P. Luo, X. Wang, and X. Tang, \u201cPedestrian detection aided by\ndeep learning semantic tasks,\u201d in CVPR, 2015.\n[207] X. Du, M. El-Khamy, J. Lee, and L. Davis, \u201cFused dnn: A deep neural\nnetwork fusion approach to fast and robust pedestrian detection,\u201d in\nWACV, 2017.\n[208] Q. Hu, P. Wang, C. Shen, A. van den Hengel, and F. Porikli, \u201cPushing\nthe limits of deep cnns for pedestrian detection,\u201d IEEE Trans. Circuits\nSyst. Video Technol., 2017.\n[209] D. Tom\u00b4e, L. Bondi, L. Barof\ufb01o, S. Tubaro, E. Plebani, and D. Pau,\n\u201cReduced memory region based deep convolutional neural network\ndetection,\u201d in ICCE-Berlin, 2016.\n[210] J. Hosang, M. Omran, R. Benenson, and B. Schiele, \u201cTaking a deeper\nlook at pedestrians,\u201d in CVPR, 2015.\n[211] J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, \u201cScale-aware fast\nr-cnn for pedestrian detection,\u201d arXiv:1510.08160, 2015.\n[212] Y. Gao, M. Wang, Z.-J. Zha, J. Shen, X. Li, and X. Wu, \u201cVisual-textual\njoint relevance learning for tag-based social image search,\u201d IEEE Trans.\nImage Process., vol. 22, no. 1, pp. 363\u2013376, 2013.\n[213] T. Kong, F. Sun, A. Yao, H. Liu, M. Lv, and Y. Chen, \u201cRon: Reverse\nconnection with objectness prior networks for object detection,\u201d in\nCVPR, 2017.\n[214] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. C. Courville, and Y. Bengio, \u201cGenerative adversarial nets,\u201d\nin NIPS, 2014.\n[215] Y. Fang, K. Kuan, J. Lin, C. Tan, and V. Chandrasekhar, \u201cObject\ndetection meets knowledge graphs,\u201d in IJCAI, 2017.\n[216] S. Welleck, J. Mao, K. Cho, and Z. Zhang, \u201cSaliency-based sequential\nimage attention with multiset prediction,\u201d in NIPS, 2017.\n[217] S. Azadi, J. Feng, and T. Darrell, \u201cLearning detection with diverse\nproposals,\u201d in CVPR, 2017.\nTHIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION\n21\n[218] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus, \u201cEnd-to-end\nmemory networks,\u201d in NIPS, 2015.\n[219] P. Dabkowski and Y. Gal, \u201cReal time image saliency for black box\nclassi\ufb01ers,\u201d in NIPS, 2017.\n[220] B. Yang, J. Yan, Z. Lei, and S. Z. Li, \u201cCraft objects from images,\u201d in\nCVPR, 2016.\n[221] I. Croitoru, S.-V. Bogolin, and M. Leordeanu, \u201cUnsupervised learning\nfrom video to detect foreground objects in single images,\u201d in ICCV,\n2017.\n[222] C. Wang, W. Ren, K. Huang, and T. Tan, \u201cWeakly supervised object\nlocalization with latent category learning,\u201d in ECCV, 2014.\n[223] D. P. Papadopoulos, J. R. R. Uijlings, F. Keller, and V. Ferrari,\n\u201cTraining object class detectors with click supervision,\u201d in CVPR, 2017.\n[224] J. Huang, V. Rathod, C. Sun, M. Zhu, A. K. Balan, A. Fathi, I. Fischer,\nZ. Wojna, Y. S. Song, S. Guadarrama, and K. Murphy, \u201cSpeed/accuracy\ntrade-offs for modern convolutional object detectors,\u201d in CVPR, 2017.\n[225] Q. Li, S. Jin, and J. Yan, \u201cMimicking very ef\ufb01cient network for object\ndetection,\u201d in CVPR, 2017.\n[226] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a\nneural network,\u201d Comput. Sci., vol. 14, no. 7, pp. 38\u201339, 2015.\n[227] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\nY. Bengio, \u201cFitnets: Hints for thin deep nets,\u201d Comput. Sci., 2014.\n[228] X. Chen, K. Kundu, Y. Zhu, A. G. Berneshawi, H. Ma, S. Fidler, and\nR. Urtasun, \u201c3d object proposals for accurate object class detection,\u201d\nin NIPS, 2015.\n[229] J. Dong, X. Fei, and S. Soatto, \u201cVisual-inertial-semantic scene repre-\nsentation for 3d object detection,\u201d in CVPR, 2017.\n[230] K. Kang, H. Li, T. Xiao, W. Ouyang, J. Yan, X. Liu, and X. Wang,\n\u201cObject detection in videos with tubelet proposal networks,\u201d in CVPR,\n2017.\nZhong-Qiu Zhao is a professor at Hefei Univer-\nsity of Technology, China. He obtained the Ph.D.\ndegree in Pattern Recognition & Intelligent System\nat University of Science and Technology, China, in\n2007. From April 2008 to November 2009, he held a\npostdoctoral position in image processing in CNRS\nUMR6168 Lab Sciences de lInformation et des\nSyst`emes, France. From January 2013 to December\n2014, he held a research fellow position in image\nprocessing at the Department of Computer Science\nof Hongkong Baptist University, Hongkong, China.\nHis research is about pattern recognition, image processing, and computer\nvision.\nPeng Zheng is a Ph.D. candidate at Hefei Uni-\nversity of Technology since 2010. He received his\nBachelor\u2019s degree in 2010 from Hefei University of\nTechnology. His interests cover pattern recognition,\nimage processing and computer vision.\nShou-tao Xu is a Master student at Hefei University\nof Technology. His research interests cover pattern\nrecognition, image processing, deep learning and\ncomputer vision.\nXindong Wu is an Alfred and Helen Lamson En-\ndowed Professor in Computer Science, University\nof Louisiana at Lafayette (USA), and a Fellow of\nthe IEEE and the AAAS. He received his Ph.D.\ndegree in Arti\ufb01cial Intelligence from the University\nof Edinburgh, Britain. His research interests include\ndata mining, knowledge-based systems, and Web in-\nformation exploration. He is the Steering Committee\nChair of the IEEE International Conference on Data\nMining (ICDM), the Editor-in-Chief of Knowledge\nand Information Systems (KAIS, by Springer), and\na Series Editor of the Springer Book Series on Advanced Information and\nKnowledge Processing (AI&KP). He was the Editor-in-Chief of the IEEE\nTransactions on Knowledge and Data Engineering (TKDE, by the IEEE\nComputer Society) between 2005 and 2008.\n"
    },
    {
        "pdf_file": "paper7.pdf",
        "text": "Enable JavaScript and\ncookies to continue\n"
    }
]