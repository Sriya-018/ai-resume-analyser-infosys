[
    {
        "pdf_file": "paper1.pdf",
        "sections": {
            "abstract": "The impressive capability and versatility of large language mod-\nels (LLMs) have aroused increasing attention in automatic speech\nrecognition (ASR), with several pioneering studies attempting to\nbuild integrated ASR models by connecting a speech encoder with\nan LLM. This paper presents a comparative study of three com-\nmonly used structures as connectors, including fully connected\nlayers, multi-head cross-attention, and Q-Former. Speech encoders\nfrom the Whisper model series as well as LLMs from the Vicuna\nmodel series with different model sizes were studied. Experiments\nwere performed on the commonly used LibriSpeech, Common\nVoice, and GigaSpeech datasets, where the LLMs with Q-Formers\ndemonstrated consistent and considerable word error rate (WER)\nreductions over LLMs with other connector structures. Q-Former-\nbased LLMs can generalise well to out-of-domain datasets, where\n12% relative WER reductions over the Whisper baseline ASR model\nwere achieved on the Eval2000 test set without using any in-domain\ntraining data from Switchboard. Moreover, a novel segment-level\nQ-Former is proposed to enable LLMs to recognise speech seg-\nments with a duration exceeding the limitation of the encoders,\nwhich results in 17% relative WER reductions over other connector\nstructures on 90-second-long speech data.\nIndex Terms\u2014 Large language model, automatic speech recog-\nnition, Q-Former, long-form speech\n1.",
            "introduction": "Large language models (LLMs) [1\u20136] with rich knowledge and the\nability to solve novel and complex tasks have revolutionised the field\nof natural language processing. More recently, significant attention\nhas been drawn to enable LLMs to handle speech inputs [7\u201317]. In\naddition to pipeline-based",
            "methodology": "s in which the LLM serve as a con-\ntroller to manage a set of functional models [7,8], two categories of\napproaches have been developed. The first category of approaches\ndiscretises speech inputs and embeds the derived speech tokens into\na vector space shared with the text tokens, then the LLMs are fine-\ntuned to fit into this new token space [9, 10]. The other category of\napproaches directly connects a speech encoder with an LLM using\na connector that aligns the speech encoders with the LLMs [11\u201315].\nThis paper focuses on the second category of approaches.\nWhen aligning the speech encoder output and LLM input spaces,\nthe choice of the connector is of vital importance, and should meet\nthe following requirements. First, the connector should be able to\nretain as much information from the speech inputs as possible, since\nit determines the amount of information that LLMs can receive from\nthe speech. Second, as the computation and storage costs of LLMs\n\u2217Corresponding author\nincrease considerably when processing long input sequences, the\nconnector should be able to achieve efficient and effective informa-\ntion compression to reduce the lengths of the LLM input sequences.\nFocusing on automatic speech recognition (ASR) to show the\nability of LLMs to recognise speech, this paper studies different\nstructures of connectors that integrate LLMs with speech encoders.\nSpecifically, an end-to-end ASR system was constructed by con-\nnecting a Whisper model encoder [18] with a Vicuna LLM [5].\nThree types of connectors were compared in this paper, including\nthe fully connected layers, multi-head cross-attention [19] and Q-\nFormer [20]. To bypass the input length limitation of the pre-trained\nspeech encoders and enable the LLM to process long speech inputs, a\nnovel segment-level Q-Former connector is proposed. Experiments\nwere conducted based on a training set with \u223c4,000 hours data, and\nthe LLMs with Q-Former connectors consistently outperform strong\nWhisper baseline ASR systems on the in-domain datasets, and can\nachieve over 12% relative word error rate (WER) reductions on\nthe out-of-domain Eval2000 test set from the Switchboard corpus.\nThe influence of the number of connector output tokens and model\nsize are studied. Moreover, the proposed segment-level Q-Former\nstructure achieved obvious WER reductions on long speech inputs,\nwhen compared with other connectors.\nThe rest paper is organised as follows. Sec. 2 summarises the\nwork related to multimodal LLMs. Secs. 3 and 4 introduce the three\nconnectors to compare and the proposed segment-level Q-Former.\nThe experimental setup and",
            "conclusion": "s.\n2. RELATED WORK\nTo enable LLMs to perform both speech perception and generation,\nSpeechGPT [9] and AudioPaLM [10] augment the vocabularies of\nLLaMA [4] and PaLM [3] LLMs with discrete speech tokens ex-\ntracted by HuBERT [21] and W2v-BERT [22] or USM [23] speech\nencoders respectively. Regarding the approaches to connect multi-\nmodal encoders to LLMs, X-LLM [11] interfaced ChatGLM with\naudio and visual encoders. [14] and [13] connect speech encoders\nwith reduced frame rates to LLMs, and achieve integrated multilin-\ngual ASR and speech translation respectively. Moreover, LLMs can\nalso be prompt for domain adaptation [15] and uncertainty estima-\ntion [17] of the ASR results.\nSeveral works studied visual LLMs [19, 20, 24\u201330]. Following\nBLIP-2 [20], InstructBLIP [26] and Video-LLaMA [27] introduced\nQ-Former as the module connector. Alternative connectors were also\ninvestigated in [19,24,28,29]. Regarding audio and music LLMs, the\nreasoning ability based on audio is studied [31]. MU-LLaMA [32]\nshowed outstanding music understanding abilities.\narXiv:2309.13963v2  [eess.AS]  26 Sep 2023\n3. MODULE CONNECTOR\nAs shown in Fig. 1, the proposed ASR model consists of three mod-\nules: a frozen speech encoder, a trainable module connector and\na frozen LLM. This section introduces three connectors, including\nfully connected layers, multi-head cross-attention and Q-Former.\n\u2744 Speech Encoder\nLinear\nMHCA\n\u2744 Large Language Model\n(a)\n(b)\n(c)\nTranscription\nQ-Former\nQ-Former query\nVicuna embedding\nConv kernel\nLinear\nFig. 1. Illustration of integrating a speech encoder and an LLM into\nan ASR system with a module connector of: (a) fully connected\nlayers, (b) multi-head cross-attention, and (c) Q-Former.\nFor clarity, some basic notations are defined as follows: X \u2208\nRnx\u00d7dx denotes the speech features obtained from the speech en-\ncoder, and the module connector compresses X into Tspeech \u2208\nRnt\u00d7dt which are input to the LLM to produce ASR transcriptions.\nH \u2208Rnh\u00d7dh denotes the hidden states in connectors while n and d\nare the numbers of vectors and hidden dimensions respectively.\n3.1. Fully connected layers\nTo compress the length of speech features, m adjacent frames xi,\nxi+1, ..., xi+m\u22121 are stacked into hi \u2208Rm\u00d7dx. Then two Linear(\u00b7)\nlayers with ReLU(\u00b7) in between are introduced as follows:\nTspeech = Linear(ReLU(Linear(H))),\n(1)\nwhere H consists of hi of a batch of samples. Actually, the vector\nstacking operation together with the first linear layer works the same\nas a 1-dimensional (-d) convolutional layer, Conv1d(\u00b7).\n3.2. Multi-head cross-attention\nTo bridge the gap between the multi-modal encoder output features\nX and LLM input textual features Tspeech, a multi-head attention\nlayer [33] denoted as MultiHead(Query, Key, Value) is used in the\nmulti-head cross-attention approach to align the two feature spaces\n[19]. First, a Conv1d(\u00b7) layer reduces the length of the speech input\nby a rate of s. Then the hidden states H are converted to Tspeech\nbased on the textual embeddings E using MultiHead(\u00b7). That is,\nH = Linear(Conv1d(X))\n(2)\nTspeech = MultiHead(H, E, E).\n(3)\n3.3. Q-Former\nQ-Former [20] is a Transformer-based module converting variable-\nlength input sequences into fixed-length output query representa-\ntions. It was initially proposed for visual-text modality alignment,\nand here is applied to audio-text alignment. In each Q-Former block,\ntrainable query embeddings Q \u2208Rnq\u00d7dq interact with the input fea-\ntures X through multi-head self-attention and cross-attention layers,\nSpecifically, Q-Former, denoted as QF(Q, X), in this work consists\nof two Transformer decoder blocks [33] with the causal attention\nmasks removed. Here Q is used as the decoder inputs and X as the\nencoder outputs in the standard Transformer.\n4. SEGMENT-LEVEL Q-FORMER\nTransformer-based speech encoders can have limitations on the in-\nput sequence duration [18]. To enable LLMs to process with longer\nspeech inputs, the whole sequence can be split into several shorter\nsegments to transform by the speech encoder separately. Such seg-\nments can be concatenated to reform a single sequence at either\nthe input or output end of Q-Former. In this paper, the structure\nof segment-level Q-Former (seg-QF) shown in Fig. 2 is proposed,\nwhich uses a Q-Former to transform each encoder output segment\nsimultaneously and concatenates their fixed-length output token se-\nquences before feeding into the LLM. Compared to performing the\nconcatenation at the Q-Former input end and producing a fixed num-\nber of nq output tokens, seg-QF allows varying the number of out-\nput tokens N \u00d7 nq according to the number of segments N, which\nis more suitable for speech inputs with variable lengths in a wide\nrange. Note the trainable query embeddings Q and Q-Former lay-\ners are shared among all the segments, and seg-QF can be initialised\nwith a pre-trained standard Q-Former.\n\u2744 Speech Encoder\n1\n2\nSeg-QF\n\u2744 Large Language Model\nTranscription\n\u2026\n\u2026\n\u2026\npos:\npos:\nFig. 2. The model structure of segment-level Q-Former (seg-QF).\nThe integers in rectangles are segment-level positional encodings.\nDespite that relative positions of the frames are provided by the\nspeech encoder within each segment Si \u2208Rnx\u00d7dx, Seg-QF is not\naware of their absolute positions in the whole input sequence. To\ninform Seg-QF with such information, segment-level position em-\nbeddings pi \u2208Rdx are added to X, as shown in Fig. 2. Specifically,\nTspeech = [QF(Q, Si \u2295pi)]N\ni=1,\n(4)\nwhere Si \u2295qi means adding qi to each row of Si.\n5. EXPERIMENTAL SETUP\n5.1. Data specifications\nExperiments were conducted on three setups. Specifically, models\nin Sections 6.1-6.4 were trained on LibriSpeech [34] 960h dataset.\nModels in Section 6.5 were trained on around 4,000 hours of data\nincluding LibriSpeech 960h, Common Voice 7.0 (English) [35] and\nGigaSpeech [36] subset M. Models in Section 6.6 were finetuned on\nLibriSpeech train-clean-100 subset. While the test sets of the afore-\nmentioned data were used for in-domain evaluation, the Eval2000\nset was used for out-of-domain evaluation.\n5.2. Model specifications\nThe Vicuna LLMs [5] and the Speech encoders from the Whisper\nmodels were used as the decoders and encoders [18], and were both\nfrozen in training. In Section 6.4, speech encoders with different\nsizes were compared, including those from Whisper base, medium\nand large-v2. LLMs including Vicuna 7B and Vicuna 13B were\ncompared. Based on Table 2, the best-performing setup with Whis-\nper large-v2 encoder and Vicuna 13B were used in other sections.\nAll the models in Sections 6.1\u20136.5 were trained for 90k steps\nwith a batch size of 24 with NVidia A100 80GB GPUs. Models\nin Section 6.6 were initialised with pre-trained Q-Former models or\nfully connected layers and trained for 10k steps with a batch size of\n8. Sinusoid encoding matrix [33] was used as the segment-level po-\nsition embeddings. Checkpoints with the highest validation set accu-\nracy obtained with teacher forcing were selected as the final models.\n6. EXPERIMENTAL RESULTS\n6.1. Random audio concatenation in training\nThe Whisper encoder is built to have a fixed input window of 30\nseconds, and zero vectors are padded to the input to increase its\nsequence length to match the window size. When connecting the\nWhisper encoder to LLMs using Q-Former and this padding strat-\negy, high deletion errors are often produced for long inputs since the\nattention mechanisms of Q-Former are trained to ignore the ending\nelements in the sequence, which are often padded zeros in the train-\ning samples. To resolve this issue, a random concatenation strategy\nis applied, which is similar to that used in [37]. Each input utterance\nin a training mini-batch is concatenated with a number of utterances\nrandomly selected from the whole training set. The random concate-\nnation continues as long as the total length of the concatenated utter-\nance does not exceed a pre-set upper limit of T seconds. The values\nof T for the training samples are set to follow a uniform distribution\nof U[0, 30] seconds. From the results in Table 1, it is demonstrated\nthe strategy can considerably reduce deletion errors and improve the\nWERs, and therefore it is always used in the rest of the experiments.\n6.2. Comparisons with different connectors\nA desirable connector should be able to extract all useful informa-\ntion from the input without causing obvious increases in computa-\ntion and storage. Regarding ASR, WERs can be used as an indicator\nto reflect the quality of the information extracted by the connector.\nBesides having a reasonable amount of model parameters, the con-\nnector is also expected to produce a reduced number of output tokens\nwhen required since the number of LLM input tokens is a key factor\ninfluencing both LLM computation cost and memory usage.\nRandom\nConcat.\nLibriSpeech\ntest-clean\ntest-other\n%WER\n%Del\n%WER\n%Del\n%\n3.72\n1.38\n6.47\n1.32\n!\n2.28\n0.30\n5.20\n0.50\nTable 1. Results with or without using the random concatenation\ntraining strategy. Both %WERs and deletion error rates (%Del) are\nshown. The ASR is a Vicuna 13B LLM with a Q-Former connector.\nModel\n#Tokens\n#Params\nLibriSpeech\ntest-clean\ntest-other\nFC\n75\n24.6M\n3.00\n6.70\nFC\n300\n23.6M\n2.29\n5.44\nCA\n75\n133.4M\n3.22\n7.54\nQF\n60\n24.5M\n2.33\n5.43\nQF\n80\n24.5M\n2.28\n5.20\nTable 2. %WERs of LLMs with different connectors. FC, CA, and\nQF refer to fully connected layers, multi-head cross-attention, and\nQ-Former connectors respectively. #Tokens and #Params are the\nnumbers of output tokens and model parameters of the connectors.\nIn Table 2, three connectors, including fully connected layers,\nmulti-head cross-attention and Q-Former, are compared. Q-Former\nresults in the lowest WERs on both test sets by producing only 80\noutput tokens. Although fully connected layers with 300 output to-\nkens (with m = 5 in Sec. 3.1) can achieve similar WERs to Q-\nFormer, it requires much more calculations and memory usage. The\nWERs produced by the fully connected layers connector with 75\noutput tokens (with m = 20) are obviously worse. The multi-head\ncross-attention connector [19] has \u223c133.4 million (M) model param-\neters, which are \u223c6 times more than the others, but still produced the\nworst WERs. As a result, Q-Former is used in the rest of the study.\n6.3. Trainable queries of Q-Former\nRecapping Section 3.3, the number of trainable queries used in the\nQ-Former determines the number of its output tokens. This sec-\ntion compares Q-Formers with different numbers of queries. Table\n3 shows that increasing the number of queries to 80 can consider-\nably reduce WERs, which implies that using \u223c80 tokens can retain\nsufficient information for ASR for inputs with less than 30 seconds\n(similar WER changes are found when only considering the utter-\nances whose durations are close to 30 seconds), which reveals the\nstrong information compression ability of Q-Former.\n#Queries\nLibriSpeech\ntest-clean\ntest-other\n40\n2.43\n5.72\n60\n2.33\n5.43\n80\n2.28\n5.20\n160\n2.26\n5.29\n300\n2.19\n5.25\nTable 3. Changes in %WERs by varying the number of trainable\nqueries (equals to the number of output tokens) in Q-Former.\n6.4. The sizes of LLMs and speech encoders\nIn this section, models with LLMs and speech encoders of different\nsizes are compared in this section. Q-Former connectors with 80\ntrainable queries are used for all models. From the results Table 4,\nincreasing the sizes of both speech encoder and LLM can result in\nlower WERs, which is in line with expectations. Doubling the size\nof the speech encoder (Whisper medium to Whisper large) reduces\nthe WERs more obviously than doubling the size of LLMs, which\nindicates that a stronger speech encoder is more important for ASR,\nwhich does not require much content understanding ability.\nModel scale\nLibriSpeech\nEncoder\nLLM\ntest-clean\ntest-other\nWhisper base\nVicuna 13B\n3.48\n9.83\nWhisper medium\nVicuna 13B\n2.35\n5.66\nWhisper large-v2\nVicuna 13B\n2.28\n5.20\nWhisper large-v2\nVicuna 7B\n2.30\n5.48\nTable 4. %WERs with different speech encoders and LLMs.\n6.5. Results with large-scale training set\nThis section verifies the performance of the best-performing model\nconfiguration by training on a large-scale dataset with 4,000 hours of\nspeech. A Vicuna 13B LLM is connected with a Whisper large-v2\nencoder using a Q-Former connector with 80 trainable queries. The\nresults in Table 5 show that the proposed model outperformed Whis-\nper large-v2 by an average of \u223c11% relative WER reduction on the\nthree in-domain test sets. It also generalises well to out-of-domain\ndata by achieving a \u223c12% relative WER reduction over Whisper\nlarge-v2 on the Eval2000 test sets. These results verify the superior\nperformance of the proposed model in ASR.\nTest set\nModel\nWhisper large-v2\nours\nLibriSpeech test-clean\n2.5 (2.7)\n2.1\nLibriSpeech test-other\n5.2 (5.2)\n5.0\nCommon Voice 7.0 dev\n7.9 (-)\n7.1\nCommon Voice 7.0 test\n9.8 (-)\n8.2\nGigaSpeech dev\n10.3 (-)\n9.0\nGigaSpeech test\n10.0 (-)\n9.2\nCallHome test\n18.9 (17.6)\n15.5\nSwitchboard test\n15.8 (13.8)\n12.1\nTable 5. %WERs of models trained on the large-scale dataset. Num-\nbers in brackets are the official Whisper %WERs reported in [18].\n6.6. Results of segment-level Q-Former\nTo evaluate the models when recognising speech exceeding the du-\nration limitation of the speech encoder, concatenated long-form test\nsets were built based on LibriSpeech test-clean and test-other test\nsets respectively. Utterances of the same chapter were concatenated\nsequentially to form sequences with a duration limit of 60, 90, or\n120 seconds. The models were trained on LibriSpeech 100h subset\nusing random concatenation with T sampled from U[0, 90].\nDifferent models were compared in Table 6. Without finetun-\ning on data longer than 30 seconds, QF and FC cannot generalise\nto recognise longer speech. After finetuning, seg-QF outperformed\nboth QF and FC considerably and showed some generalisation to\nConcat.\nTest Set\nModel\nFC\u2020\nQF\u2020\nFC\nQF\nSeg-QF\nSeg-QF*\ntest-clean\n3.00\n2.28\n2.89\n2.32\n2.35\n2.19\nt \u2a7d60\n58.18\n61.99\n3.43\n3.83\n3.11\n2.68\nt \u2a7d90\n70.64\n75.45\n4.98\n5.40\n3.81\n3.24\nt \u2a7d120\n75.48\n76.81\n19.14\n26.91\n17.95\n13.60\ntest-other\n6.70\n5.20\n6.68\n5.40\n5.37\n5.20\nt \u2a7d60\n62.19\n63.49\n7.58\n7.81\n6.43\n5.13\nt \u2a7d90\n74.27\n76.03\n9.03\n11.78\n8.07\n6.17\nt \u2a7d120\n78.75\n76.14\n24.55\n32.24\n22.25\n19.63\nTable 6. %WERs on the concatenated test sets. Results based on\nthe Librispeech test clean and test other test sets are shown in Row\n1-4 and Row 5-8 respectively, and Row 1 and Row 5 are the results\nwith the original segmentation. Each t \u2a7dTtest test set is obtained by\nconcatenating either test clean or test other utterances based on their\norders that appeared in the original audiobooks to up to Ttest seconds.\nModels with \u2020 were not fine-tuned on concatenated training data.\nPre-trained checkpoints for Seg-QF* were QF in Table 5, and were\nFC in Row 1 and QF in Row 5 of Table 2 for other models.\noriginal\n 60\n 90 /s\nduration\n4\n5\n6\n7\n8\n%WER\nall data\nsuccessfully\ndecoded data\noriginal\n 60\n 90 /s\nduration\n5\n6\n%WER\nall data\nsuccessfully\ndecoded data\nFig. 3. %WERs of Seg-QF (left) and Seg-QF* (right) in Tab. 6\ncalculated on all data and successfully decoded data in the test-other\nset. The x-axis denotes the duration of the concatenated audio.\n120-second-long speech inputs. From Table 6, WERs degrade with\nlonger speech inputs. More extreme cases, such as repeated out-\nputs and large chunks of deletions, were observed in case studies\nwhen feeding long speech inputs. The WERs for all utterances and\nthose successfully decoded in Librispeech test-other are plotted in\nFig. 3 for Seg-QF and Seg-QF*. It shows that the contexts in long\nspeech inputs help ASR to reduce WERs if being decoded success-\nfully. However, the overall WERs on the full test-other set increased\ndue to more frequent extreme cases.\n7. CONCLUSION\nThis paper studies to enable LLMs to recognise speech inputs by\ninterfacing with a speech encoder. Three commonly used connectors\nincluding fully-connected layers, multi-head cross-attention and Q-\nFormer were compared. The LLMs with Q-Formers demonstrated\nsuperior performance over LLMs with other connectors and the\nWhisper baseline ASR system on all of the in-domain and out-of-\ndomain test sets. Moreover, a novel segment-level Q-Former was\nproposed to improve the performance with long-form speech inputs\nwhose duration exceeds the limitations of the pre-trained speech\nencoder. Analyses show that although the rich context information\nin long-form speech inputs can improve ASR accuracy, overly long\ninputs can also aggravate the hallucination problem of LLMs.\n8. REFERENCES\n[1] OpenAI, \u201cGPT-4 technical report,\u201d arXiv:2308.11276, 2023.\n[2] T. Brown, B. Mann, N. Ryder, et al., \u201cLanguage models are\nfew-shot learners,\u201d in Proc. NeurIPS, 2020.\n[3] R. Anil, A.M. Dai, O. Firat, et al., \u201cPaLM 2 technical report,\u201d\narXiv:2305.10403, 2023.\n[4] H. Touvron, T. Lavril, et al.,\n\u201cLLaMA: Open and efficient\nfoundation language models,\u201d arXiv:2302.13971, 2023.\n[5] W. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng,\nS. Zhuang, Y. Zhuang, et al., \u201cVicuna: An open-source chatbot\nimpressing GPT-4 with 90%* ChatGPT quality,\u201d https://\nvicuna.lmsys.org (accessed 14 April 2023), 2023.\n[6] W. Zeng, X. Ren, T. Su, et al.,\n\u201cPangu-\u03b1: Large-scale au-\ntoregressive pretrained Chinese language models with auto-\nparallel computation,\u201d arXiv:2104.12369, 2021.\n[7] R. Huang, M. Li, D. Yang, et al., \u201cAudioGPT: Understand-\ning and generating speech, music, sound, and talking head,\u201d\narXiv:2304.12995, 2023.\n[8] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, \u201cHug-\ngingGPT: Solving ai tasks with chatGPT and its friends in hug-\ngingface,\u201d arXiv:2303.17580, 2023.\n[9] D. Zhang, S. Li, X. Zhang, J. Zhan, P. Wang, Y. Zhou,\nand X. Qiu,\n\u201cSpeechGPT: Empowering large language\nmodels with intrinsic cross-modal conversational abilities,\u201d\narXiv:2305.11000, 2023.\n[10] P.K. Rubenstein, C. Asawaroengchai, D.D. Nguyen, et al.,\n\u201cAudioPaLM: A large language model that can speak and lis-\nten,\u201d arXiv:2306.12925, 2023.\n[11] F. Chen, M. Han, H. Zhao, Q. Zhang, J. Shi, S. Xu, and\nB. Xu,\n\u201cX-LLM: Bootstrapping advanced large language\nmodels by treating multi-modalities as foreign languages,\u201d\narXiv:2305.04160, 2023.\n[12] Y. Shu, S. Dong, G. Chen, et al., \u201cLLaSM: Large language and\nspeech model,\u201d arXiv:2308.15930, 2023.\n[13] J. Wu, Y. Gaur, Z. Chen, et al., \u201cOn decoder-only architec-\nture for speech-to-text and large language model integration,\u201d\narXiv:2307.03917, 2023.\n[14] Y. Fathullah, C. Wu, E. Lakomkin, J. Jia, Y. Shangguan, K. Li,\nJ. Guo, W. Xiong, J. Mahadeokar, O. Kalinli, et al., \u201cPrompt-\ning large language models with speech recognition abilities,\u201d\narXiv:2307.11795, 2023.\n[15] Y. Li, Y. Wu, J. Li, and S. Liu, \u201cPrompting large language\nmodels for zero-shot domain adaptation in speech recognition,\u201d\narXiv:2306.16007, 2023.\n[16] R. Ma, M. Qian, P. Manakul, M. Gales, and K. Knill, \u201cCan gen-\nerative large language models perform asr error correction?,\u201d\narXiv:2307.04172, 2023.\n[17] P. Dighe, Y. Su, S. Zheng, et al., \u201cLeveraging large language\nmodels for exploiting asr uncertainty,\u201d\narXiv:2309.04842,\n2023.\n[18] A. Radford, J. Kim, T. Xu, G. Brockman, C. McLeavey, and\nI. Sutskever, \u201cRobust speech recognition via large-scale weak\nsupervision,\u201d in Proc. ICML, Honolulu, 2023.\n[19] C. Lyu, M. Wu, L. Wang, et al., \u201cMacaw-LLM: Multi-modal\nlanguage modeling with image, audio, video, and text integra-\ntion,\u201d arXiv:2306.09093, 2023.\n[20] J. Li, D. Li, S. Savarese, and S. Hoi, \u201cBLIP-2: Bootstrapping\nlanguage-image pre-training with frozen image encoders and\nlarge language models,\u201d in Proc. ICML, Vienna, 2023.\n[21] W. Hsu, B. Bolte, Y. H. Tsai, K. Lakhotia, R. Salakhutdinov,\nand A. Mohamed,\n\u201cHuBERT: Self-supervised speech rep-\nresentation learning by masked prediction of hidden units,\u201d\nIEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, vol. 29, pp. 3451\u20133460, 2021.\n[22] Y. Chung, Y. Zhang, W. Han, C. Chiu, J. Qin, R. Pang, and\nY. Wu,\n\u201cW2v-BERT: Combining contrastive learning and\nmasked language modeling for self-supervised speech pre-\ntraining,\u201d in Proc. ASRU, Cartagena, 2021.\n[23] Y. Zhang, W. Han, et al., \u201cGoogle USM: Scaling automatic\nspeech recognition beyond 100 languages,\u201d arXiv:2303.01037,\n2023.\n[24] J.B. Alayrac, J. Donahue, P. Luc, et al., \u201cFlamingo: A visual\nlanguage model for few-shot learning,\u201d in Proc. NeurIPS, New\nOrleans, 2022.\n[25] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, \u201cMiniGPT-\n4: Enhancing vision-language understanding with advanced\nlarge language models,\u201d arXiv:2304.10592, 2023.\n[26] W. Dai, J. Li, D. Li, A.M.H. Tiong, J. Zhao, W. Wang,\nB. Li, P. Fung, and S. Hoi, \u201cInstructBLIP: Towards general-\npurpose vision-language models with instruction tuning,\u201d\narXiv:2305.06500, 2023.\n[27] H. Zhang, X. Li, and L. Bing, \u201cVideo-LLaMA: An instruction-\ntuned audio-visual language model for video understanding,\u201d\narXiv:2306.02858, 2023.\n[28] Y. Su, T. Lan, H. Li, J. Xu, Y. Wang, and D. Cai, \u201cPandaGPT:\nOne model to instruction-follow them all,\u201d arXiv:2305.16355,\n2023.\n[29] G. Chen, Y.D. Zheng, et al., \u201cVideoLLM: Modeling video se-\nquence with large language models,\u201d arXiv:2305.13292, 2023.\n[30] M. Maaz, H. Rasheed, S. Khan, and F.S. Khan,\n\u201cVideo-\nChatGPT: Towards detailed video understanding via large vi-\nsion and language models,\u201d arXiv:2306.05424, 2023.\n[31] Y. Gong, H. Luo, A.H. Liu, L. Karlinsky, and J. Glass, \u201cListen,\nthink, and understand,\u201d arXiv:2305.10790, 2023.\n[32] S. Liu, A. Hussain, C. Sun, and Y. Shan, \u201cMusic understand-\ning LLaMA: Advancing text-to-music generation with ques-\ntion answering and captioning,\u201d arXiv:2308.11276, 2023.\n[33] A. Vaswani, N. Shazeer, N. Parmar, et al., \u201cAttention is all you\nneed,\u201d in Proc. NeurIPS, Long Beach, 2017.\n[34] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLib-\nrispeech:\nAn ASR corpus based on public domain audio\nbooks,\u201d in Proc. ICASSP, South Brisbane, 2015.\n[35] R. Ardila, M. Branson, K. Davis, et al., \u201cCommon Voice: A\nmassively-multilingual speech corpus,\u201d in Proc. LREC, Mar-\nseille, 2020.\n[36] G. Chen, S. Chai, G. Wang, et al., \u201cGigaSpeech: An evolv-\ning, multi-domain asr corpus with 10,000 hours of transcribed\naudio,\u201d in Proc. Interspeech, Brno, 2021.\n[37] Y. Lin, T. Han, H. Xu, et al., \u201cRandom utterance concatena-\ntion based data augmentation for improving short-video speech\nrecognition,\u201d in Proc. Interspeech, Dublin, 2023."
        }
    },
    {
        "pdf_file": "paper2.pdf",
        "sections": {
            "abstract": "",
            "introduction": "",
            "methodology": "",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper3.pdf",
        "sections": {
            "abstract": "",
            "introduction": "",
            "methodology": "",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper4.pdf",
        "sections": {
            "abstract": "",
            "introduction": "",
            "methodology": "",
            "conclusion": ""
        }
    },
    {
        "pdf_file": "paper5.pdf",
        "sections": {
            "abstract": "",
            "introduction": "",
            "methodology": "",
            "conclusion": ""
        }
    }
]